Deck,Question,ModelAnswer
Day 1: Strategy Gaia Kinexys & AI,What is the 'Fortress Balance Sheet' in a technical architecture context?,It translates to Operational Resilience. It dictates that systems must prioritize survival and recovery over cost efficiency; necessitating active-active redundancy to withstand Black Swan events and ensure the bank can settle payments even during a catastrophe.
Day 1: Strategy Gaia Kinexys & AI,What is 'Gaia' in the JP Morgan ecosystem?,JP Morgan's internal Private Cloud (PaaS) built on Cloud Foundry/Kubernetes. It is used to host core ledgers; high-sensitivity data (MNPI); and applications requiring strict data sovereignty within the firm's firewall.
Day 1: Strategy Gaia Kinexys & AI,When should a Solutions Analyst choose AWS (Public Cloud) over Gaia?,For 'burst' compute needs (e.g. overnight risk simulations); highly elastic workloads; or customer-facing applications that benefit from global edge caching; provided the data is not restricted MNPI.
Day 1: Strategy Gaia Kinexys & AI,What is Kinexys?,The rebrand of JP Morgan's blockchain unit (formerly Onyx). It represents the strategic shift from experimental to industrial digital assets; focusing on tokenized deposits (JPM Coin) and programmable payments for institutional clients.
Day 1: Strategy Gaia Kinexys & AI,How does JPM Coin (JPMD) differ from a stablecoin (like USDC)?,JPM Coin is a Tokenized Deposit; meaning it is a direct liability of the bank and carries the full weight of JPMC's balance sheet. Stablecoins are asset-backed and carry counterparty risk of the issuer.
Day 1: Strategy Gaia Kinexys & AI,What is the 'Singleness of Money' principle?,The regulatory requirement that a digital token (like JPM Coin) must always be exchangeable at par ($1:$1) with commercial bank money; ensuring fungibility and no fluctuation in value compared to traditional deposits.
Day 1: Strategy Gaia Kinexys & AI,What is 'LLM Suite'?,JPMC's internal; secure gateway to Large Language Models (like GPT-4). It acts as a walled garden to prevent data leakage; allowing employees to use GenAI for productivity without exposing proprietary code or client data to the public internet.
Day 1: Strategy Gaia Kinexys & AI,Define 'Agentic AI' in the context of JPMC's 2025 strategy.,AI systems that can execute multi-step workflows (e.g. Investigate this alert; query the DB; and draft a SAR) rather than just generating text. It shifts AI from a passive tool to an autonomous digital worker with human supervision.
Day 1: Strategy Gaia Kinexys & AI,What is the 'Paved Road' concept in software delivery?,A set of standardized; pre-approved CI/CD pipelines and infrastructure patterns that automatically inherit the bank's security and compliance controls; allowing developers to deploy code without manual risk board approvals.
Day 1: Strategy Gaia Kinexys & AI,What are 'Programmable Payments'?,Using smart contracts to embed logic into a payment. For example 'Release funds ONLY when the digital Bill of Lading is verified.' This collapses reconciliation and payment execution into a single atomic step.
Day 1: Strategy Gaia Kinexys & AI,What is the 'Weekend Trap' that Kinexys helps solve?,Traditional RTGS systems (like Fedwire or Target2) often close on weekends/holidays; trapping liquidity. Kinexys allows institutional clients to move value 24/7/365 on the blockchain ledger.
Day 1: Strategy Gaia Kinexys & AI,What is MNPI?,Material Non-Public Information. Data that; if disclosed; could affect a company's stock price. Systems handling MNPI usually require hosting on Gaia (Private Cloud) with strict Chinese Wall access controls.
Day 1: Strategy Gaia Kinexys & AI,Why is JPMC extending JPM Coin to public blockchains like Base?,To enable Interoperability. It allows institutional clients to settle transactions on the networks where they are already trading (DeFi/Tokenized Assets) while maintaining the safety and regulatory compliance of Bank Money.
Day 1: Strategy Gaia Kinexys & AI,What is the 'Unified Ledger' concept?,A future state where tokenized deposits (money) and tokenized assets (bonds/equities) reside on the same programmable platform; enabling instant Delivery vs. Payment (DvP) and eliminating settlement risk.
Day 1: Strategy Gaia Kinexys & AI,What is 'Data Sovereignty' and how does it impact cloud strategy?,The legal requirement that data remains within a specific country's borders (e.g. Saudi Arabia; EU). This often mandates using Gaia local instances or specific AWS regions to ensure data does not physically leave the jurisdiction.
Day 1: Strategy Gaia Kinexys & AI,What is the 'Blast Radius' design principle?,Architecting systems so that a failure in one component (e.g. a payment gateway) is contained and does not cascade to bring down the entire platform. This is often achieved using Bulkhead or Circuit Breaker patterns.
Day 1: Strategy Gaia Kinexys & AI,Why is 'Human-in-the-Loop' (HITL) critical for AI in RegTech?,Regulators require accountability. While AI (like Agentic AI) can draft a Suspicious Activity Report (SAR); a human must review and sign off on it to accept the regulatory liability; as AI cannot be held legally responsible.
Day 1: Strategy Gaia Kinexys & AI,What is 'Atomic Settlement'?,The simultaneous exchange of assets and payment in a single transaction block. If one side fails; the entire transaction reverts. This eliminates counterparty settlement risk.
Day 1: Strategy Gaia Kinexys & AI,What is the approximate annual technology spend of JP Morgan?,~$17-18 Billion. This massive budget allows JPMC to build proprietary advantages (like Kinexys and Gaia) rather than relying solely on third-party vendors.
Day 1: Strategy Gaia Kinexys & AI,What is the 'Hybrid Cloud' strategy at JPMC?,The strategic use of both Gaia (Private Cloud) for core/sensitive workloads and AWS (Public Cloud) for elastic/customer-facing workloads; optimizing for both security (Gaia) and speed/scale (AWS).
"Day - 1 JPM Interview Prep","Describe your career aspirations at JPMorgan Chase.","Growth, innovation, and leadership opportunities at a leading global financial institution. Aspire to develop expertise in banking operations, contribute to digital transformation initiatives, and build a career aligned with JPM's mission of supporting economic growth. Interested in continuous learning, mentorship, and taking on progressively challenging roles."
"Day - 1 JPM Interview Prep","Why do you want to work at JPMorgan Chase?","JPMorgan Chase's reputation as a leading global financial institution appeals to me. The firm's commitment to innovation, strong technology infrastructure, and stability make it an ideal place to build a career. Drawn to JPM's diverse business lines, commitment to employee development, and impact on the financial industry."
"Day - 1 JPM Interview Prep","What do you know about JPMorgan Chase's business segments?","JPMorgan Chase operates through three main segments: Investment Banking (advisory, underwriting), Wealth Management (asset management, advisory services), and Commercial Banking (lending, payment solutions). Additionally, the firm has significant operations in Corporate & Investment Bank, Community Banking, and serves clients across over 100 countries with diverse financial solutions."
"Day - 1 JPM Interview Prep","How would you describe JPMorgan Chase's role in the financial industry?","JPMorgan Chase is a leader and innovator in the global financial industry. The firm plays a pivotal role in supporting economic growth through lending, capital markets, and financial advisory services. Known for its technological advancement, risk management expertise, and commitment to serving clients across consumer banking, investment banking, and wealth management sectors."
"Day - 1 JPM Interview Prep","What are JPMorgan Chase's core values?","JPMorgan Chase's core values include integrity (ethical conduct and honesty), excellence (commitment to high quality), teamwork (collaboration and mutual respect), diversity and inclusion (valuing different perspectives), and innovation (continuous improvement and forward-thinking). These values guide decision-making and shape the firm's culture."
"Day - 1 JPM Interview Prep","Describe your first 90 days at JPMorgan Chase.","In my first 90 days, I would focus on learning and relationship building. Key priorities: understand team structure, business processes, and key clients; establish productive relationships with colleagues and managers; complete onboarding training; deliver quality work on assigned projects; seek feedback and demonstrate commitment to the role; familiarize myself with JPM's systems and culture; and identify opportunities to contribute meaningfully to team objectives."
"Day - 1 JPM Interview Prep","Tell us about a time you overcame a challenge.","[Share a specific example demonstrating problem-solving ability, resilience, and teamwork. Example structure: Situation - identify the challenge, Action - steps taken to address it, Result - positive outcome achieved. Emphasize learning, collaboration with others, and how the experience shaped your professional growth. Choose an example relevant to the banking/finance domain if possible.]"
"Day - 1 JPM Interview Prep","How would you handle working in a fast-paced environment?","I thrive in fast-paced environments by staying organized, prioritizing effectively, and maintaining focus under pressure. My approach includes: maintaining clear communication with team members, breaking complex tasks into manageable steps, leveraging technology and tools for efficiency, seeking clarification when needed, and maintaining perspective during high-stress situations. I view challenges as opportunities for growth and remain adaptable to changing priorities."
"Day - 1 JPM Interview Prep","What is your approach to continuous learning?","Continuous learning is essential in banking and finance. My approach includes: seeking feedback regularly from managers and colleagues, pursuing relevant certifications (e.g., CFA, compliance certifications), staying updated on industry trends and regulations, mentoring junior colleagues, attending training programs, reading financial publications, and reflecting on successes and failures to extract lessons. I'm committed to developing both technical and soft skills throughout my career."
"Day - 1 JPM Interview Prep","How do you work with people from diverse backgrounds?","I value diversity as a strength. My approach includes: respecting different perspectives and experiences, actively listening to understand viewpoints different from my own, fostering inclusive communication, avoiding assumptions based on background, acknowledging unconscious bias, and advocating for inclusive practices. I believe diverse teams deliver better outcomes and I'm committed to creating an environment where everyone feels valued and can contribute their best work."
"Day - 1 JPM Interview Prep","Which JPMorgan Chase business line interests you most and why?","[Choose one and provide specific reasons] Investment Banking: advisory services, M&A experience, working with large corporations. Wealth Management: asset management, client relationships, portfolio management. Commercial Banking: enterprise clients, relationship management, financial solutions. Corporate & Investment Bank: trading, capital markets, financial engineering. The answer should demonstrate research, alignment with career goals, and understanding of the specific business line's role in JPM."
"Day - 1 JPM Interview Prep","Tell us about your understanding of JPMorgan Chase's technology initiatives.","JPMorgan Chase is a leader in financial technology innovation. Key initiatives include: JPMCoin (blockchain-based digital currency), blockchain implementation for settlement and payments, AI and machine learning for risk management and trading, cloud computing infrastructure migration, cybersecurity investments, and fintech partnerships. The firm invests heavily in technology to improve operational efficiency, customer experience, and competitive advantage in the digital financial services landscape."
"Day - 1 JPM Interview Prep","How do you stay informed about regulatory changes in banking?","Staying informed about regulatory changes is crucial in banking. I do this by: reading regulatory publications and updates from agencies like the SEC, Fed, and FDIC, following industry publications and banking news, attending compliance training and webinars, engaging with colleagues in compliance and risk functions, understanding how regulations impact JPM's business lines, and maintaining awareness of global regulatory developments affecting international operations. This knowledge helps me understand risk management and business strategy contexts."
"Day - 1 JPM Interview Prep","Describe your experience with financial analysis or data-driven decision making.","[Share specific examples of financial analysis, using data to drive decisions, working with financial models or reports. Examples could include: analyzing investment opportunities, preparing financial reports, working with budget forecasts, using data visualization tools, or presenting findings to leadership. Structure: context, analytical approach used, data sources, insights gained, and business impact. Emphasize analytical thinking and data literacy.]"
"Day - 1 JPM Interview Prep","How would you contribute to JPMorgan Chase's diversity and inclusion goals?","I would contribute to JPM's D&I goals by: advocating for inclusive hiring and promotion practices, mentoring colleagues from underrepresented backgrounds, serving on employee resource groups (ERGs) if appropriate, fostering an inclusive team environment through respectful communication, challenging biased behavior or language when observed, supporting diversity initiatives, and continuously learning about different perspectives and experiences. Diversity strengthens organizations and I'm committed to being an active participant in JPM's D&I efforts."
"Day - 1 JPM Interview Prep","What are the key risks JPMorgan Chase faces in its business?","Key risks for JPMorgan Chase include: credit risk (borrower defaults), market risk (interest rates, foreign exchange), operational risk (system failures, fraud), regulatory compliance risk (changing regulations, enforcement actions), cybersecurity risk (data breaches, hacking attempts), geopolitical risk (international operations), and liquidity risk. The firm has sophisticated risk management frameworks and committees to monitor and mitigate these risks. Understanding risk is central to banking strategy and decision-making."
"Day - 1 JPM Interview Prep","Describe your knowledge of JPMorgan Chase's ESG (Environmental, Social, Governance) initiatives.","JPMorgan Chase is committed to ESG principles: Environmental - supporting renewable energy projects, climate finance, and sustainable investing. Social - supporting community development, financial inclusion, education initiatives, and diversity programs. Governance - maintaining strong ethical standards, robust compliance frameworks, stakeholder engagement, and executive accountability. JPM recognizes that ESG factors are material to business performance and risk management, and integrates ESG considerations into investment decisions and business strategy."
"Day - 1 JPM Interview Prep","How would you handle a situation where you disagreed with a manager's decision?","I would handle disagreement professionally and constructively: first, understand the manager's rationale thoroughly; present my perspective respectfully using data and facts; listen to the manager's reasoning; seek to find common ground; ultimately respect the manager's decision if they proceed differently; maintain professionalism and commitment to the decision; and if serious ethical concerns exist, follow appropriate escalation procedures. Healthy debate and diverse perspectives strengthen organizations, but hierarchy and respect for decision-making authority are also important."
"Day - 1 JPM Interview Prep","What is your biggest professional weakness and how are you addressing it?","[Choose a real but not critical weakness relevant to the role and demonstrate growth mindset] Example: 'Initially, I wasn't strong in public speaking, but I've joined Toastmasters, taken presentation skills courses, and volunteered for speaking opportunities. I've made significant progress and continue practicing.' Show: self-awareness, genuine effort to improve, willingness to seek help, and progress made. Avoid making the weakness sound like a strength in disguise or claiming no weaknesses."
"Day - 1 JPM Interview Prep","Why should JPMorgan Chase hire you?","JPMorgan Chase should hire me because: I bring [relevant skills, experience, education] aligned with the role requirements; I'm genuinely interested in JPM's mission and culture; I've demonstrated [specific accomplishments] showing capabilities and work ethic; I'm committed to continuous learning and professional development; I work effectively in teams and value collaboration; I understand JPM's business and challenges; I'm detail-oriented and deliver quality work; I'm adaptable, resilient, and motivated by challenging work; and I'm excited about the opportunity to contribute to JPM's success and grow my career with a leading organization."
Day 1: JPMC Strategy & Tech,Explain J.P. Morgan's 'Fortress Balance Sheet' philosophy to a technical team.,"The Fortress Balance Sheet is our strategy of maintaining capital and liquidity buffers far above regulatory minimums. For us as engineers, this translates to prioritizing operational resilience, redundancy, and risk controls over pure speed or cost efficiency."
Day 1: JPMC Strategy & Tech,How would you decide whether to host a new application on Gaia vs. AWS?,"I would use the Data Classification Matrix. If the app handles Core Ledgers, MNPI, or requires strict Data Sovereignty (e.g., in KSA), it goes on Gaia (Private Cloud). If it requires elastic compute for analytics or is customer-facing, I'd deploy it to AWS using the 'Paved Road' patterns."
Day 1: JPMC Strategy & Tech,What is the strategic significance of rebranding 'Onyx' to 'Kinexys'?,"The rebrand signals a shift from 'experimental blockchain' to 'industrial-grade value movement.' It emphasizes that we are no longer just testing technology but are now scaling real-world asset tokenization and programmable payments for institutional clients."
Day 1: JPMC Strategy & Tech,Explain the concept of 'Singleness of Money' regarding JPM Coin.,"Singleness of Money means that a digital token must always be exchangeable 1:1 with commercial bank money. JPM Coin achieves this because it is a direct liability of the bank (a tokenized deposit), unlike stablecoins which carry counterparty risk."
Day 1: JPMC Strategy & Tech,How does 'Agentic AI' differ from the 'LLM Suite' tools we use today?,"Currently, the LLM Suite is a passive tool that generates text or code when prompted. Agentic AI (the 2025 focus) actively executes workflows—it can plan steps, query databases, and perform actions like investigating a payment failure autonomously."
Day 1: JPMC Strategy & Tech,Why is 'Idempotency' critical for our payment APIs?,"Idempotency ensures that if a client retries a request due to a timeout, we don't process the payment twice. It is a fundamental control for financial integrity in distributed systems to prevent double-spending."
Day 1: JPMC Strategy & Tech,How would you explain 'Programmable Payments' to a Corporate Treasurer?,"It allows you to embed logic directly into the money. Instead of manually reconciling invoices, you can set a rule: 'When the digital Bill of Lading is received, automatically release the payment,' enabling 24/7 atomic settlement."
Day 1: JPMC Strategy & Tech,What is the 'Strangler Fig' pattern and why do we use it?,"It's a modernization strategy where we don't rewrite a legacy mainframe system all at once. Instead, we build new microservices around the edges, gradually routing specific traffic to them until the old system is obsolete and can be safely decommissioned."
Day 1: JPMC Strategy & Tech,Describe the 'Blast Radius' principle in system design.,"It's the practice of designing components with isolation (bulkheads) so that if one service fails, the impact is contained locally and doesn't cascade to bring down the entire global platform. It is key to our 'Active-Active' resilience strategy."
Day 1: JPMC Strategy & Tech,Why is 'Data Sovereignty' a major constraint for our EMEA architecture?,"Many jurisdictions like Saudi Arabia and the UAE have laws requiring data to physically remain in-country. This forces us to deploy local Gaia nodes or specific 'Lockdown' cloud regions rather than using a generic central cloud region."
Day 1: JPMC Strategy & Tech,What is the 'Tokenized Collateral Network' (TCN) used for?,"TCN allows clients to tokenize assets like Money Market Fund shares and use them as collateral instantly. This releases 'trapped liquidity' that would otherwise be stuck in slow settlement cycles, allowing for intraday margin moves."
Day 1: JPMC Strategy & Tech,How does the 'Flywheel Effect' apply to JPMC's technology investment?,"Our massive scale allows us to invest $17B in tech, which builds better products. Better products attract more clients and data, which gives us more scale and capital to reinvest, creating a self-reinforcing cycle of dominance."
Day 1: JPMC Strategy & Tech,What is the 'Paved Road' approach to CI/CD?,"It is a set of pre-approved, standardized infrastructure patterns and pipelines. By using them, teams inherit security and compliance controls automatically, speeding up deployment by bypassing manual architecture review boards."
Day 1: JPMC Strategy & Tech,Why do we favor 'Active-Active' over 'Active-Passive' for core payments?,"Active-Passive disaster recovery is risky because the backup site is rarely tested under full load. Active-Active ensures both sites are always processing traffic, proving real-time resilience and doubling our processing capacity."
Day 1: JPMC Strategy & Tech,What is the primary risk of using public ChatGPT for coding?,"The risk is data leakage. Public models might train on the code or data we input, potentially exposing JPMC proprietary IP or MNPI. That's why we use the internal 'LLM Suite' gateway."
Day 1: JPMC Strategy & Tech,Explain 'Atomic Settlement' in the context of Kinexys.,"Atomic Settlement means the transfer of the asset and the payment happen simultaneously in a single transaction. If one fails, both fail. This eliminates 'Herstatt Risk'—the risk that you pay but don't receive the asset."
Day 1: JPMC Strategy & Tech,What is 'Material Non-Public Information' (MNPI) in a tech context?,"MNPI is sensitive data (like unreleased earnings or M&A info) that could be used for insider trading. In tech, this data requires the highest level of encryption, access control, and isolation, often mandating Gaia hosting."
Day 1: JPMC Strategy & Tech,How would you handle a requirement that conflicts with our 'Fortress' risk controls?,"I would reject the requirement in its current form. I would explain the risk to the stakeholder and propose an alternative technical solution that achieves their business goal without compromising our control framework."
Day 1: JPMC Strategy & Tech,What is the difference between 'Resilience' and 'Reliability'?,"Reliability means the system works correctly (no bugs). Resilience means the system continues to operate even when components fail (e.g., a server crash). The Fortress mindset prioritizes Resilience to survive unexpected shocks."
Day 1: JPMC Strategy & Tech,Why is 'Data Lineage' important for Regulatory Reporting?,"We must prove to regulators (like the BoE) exactly where the numbers in our report came from. Data Lineage tools allow us to trace a report value back through every transformation to the original transaction source."
"Day 2: ISO 20022 Deep Dive","Explain the 'Payment Return' workflow involving a pacs.004. specifically addressing how Foreign Exchange (FX) is handled if the original payment involved a currency conversion.","A pacs.004 is generated when the beneficiary bank cannot apply funds (e.g., account closed). It reverses the chain. If FX was involved (e.g., GBP sent -> converted to USD), the return must usually convert back (USD -> GBP). The pacs.004 contains blocks to specify the 'Original Interbank Settlement Amount' and the 'Returned Interbank Settlement Amount', allowing transparency on the FX loss/gain during the round trip, which is often charged to the customer or absorbed depending on the error reason."
"Day 2: ISO 20022 Deep Dive","Describe the difference between the 'Serial Method' and the 'Cover Method' in ISO 20022, highlighting which message types are used in each.","In the Serial Method, the payment message travel bank-to-bank along the same path as the settlement funds. A single pacs.008 is passed from Bank A -> Intermediary -> Bank B. In the Cover Method, the information flows directly (or nearly directly) via a pacs.008 to the beneficiary bank, while the funds settle via a separate correspondent chain using a pacs.009 COV. The COV message *must* contain a reference to the underlying pacs.008 to ensure AML transparency."
"Day 2: ISO 20022 Deep Dive","Explain the concept of 'POBO' (Payments On Behalf Of) and how ISO 20022 tags <InitgPty> and <Dbtr> facilitate this structure better than legacy formats.","POBO allows a treasury centre or third party to make payments for a subsidiary. Legacy formats often blurred who was the 'sender' vs the 'account owner'. ISO 20022 separates them: <Dbtr> is the entity whose account is debited (the Treasury), while <UltmtDbtr> or <InitgPty> can clearly identify the subsidiary/entity on whose behalf the payment is made. This ensures the beneficiary recognizes who paid them (reconciliation) while satisfying AML rules about the source of funds."
"Day 2: ISO 20022 Deep Dive","Walk through the 'Cancellation' process. If a client realizes they made a mistake, what message chain is triggered?","The client initiates a request (often via portal/API) which triggers a camt.056 (FIToFIPaymentCancellationRequest) sent from the Debtor Bank to the next bank in the chain. This message travels to the Beneficiary Bank. If funds are still there and the beneficiary agrees to return them, the Beneficiary Bank sends a pacs.004 (Return) and a camt.029 (Resolution) confirming positive action. If the request is denied (funds already withdrawn), a camt.029 with a negative status is sent back."
"Day 2: ISO 20022 Deep Dive","What are 'Virtual Accounts' (vIBANs) and how does ISO 20022 support their reconciliation?","Virtual accounts are dummy account numbers routed to a real physical master account. In ISO 20022, the <CdtrAcct> (Creditor Account) field can carry the vIBAN. Because ISO allows rich remittance data and end-to-end references (<EndToEndId>), the master account holder can easily segregate incoming funds based on which vIBAN was used, enabling automated reconciliation for thousands of sub-clients without opening physical accounts."
"Day 2: ISO 20022 Deep Dive","Discuss the conflict between 'Data Richness' in ISO 20022 and 'Data Privacy' (GDPR). How should a bank manage this?","ISO 20022 encourages including full names, addresses, and ID numbers. However, GDPR requires data minimization. Banks must ensure that PII (Personally Identifiable Information) in payment messages is: 1) Only sent if required by regulation (Wire Transfer Regs), 2) Protected/Encrypted during transit, and 3) Not retained longer than necessary in archives. Cross-border transfers to non-equivalent privacy jurisdictions (e.g., outside EU) require strict legal basis."
"Day 2: ISO 20022 Deep Dive","Explain 'Interoperability' challenges between SEPA (EPC) and SWIFT (CBPR+) implementations of ISO 20022.","Although both use ISO 20022, they use different 'flavors' or Usage Guidelines. For example, SEPA may forbid certain characters that SWIFT allows, or SEPA requires specific Service Level codes (<SvcLvl> = 'SEPA') that SWIFT does not. A bank acting as a gateway between SWIFT and SEPA must perform 'Translation' or 'Mapping' to ensure a valid incoming SWIFT message doesn't fail validation when forwarded into the SEPA scheme."
"Day 2: ISO 20022 Deep Dive","How does the <RgltryRptg> (Regulatory Reporting) block function, and why is it crucial for cross-border payments?","The <RgltryRptg> block allows banks to embed specific codes required by central banks (e.g., Balance of Payments codes in UAE or India) directly into the payment instruction. This replaces manual post-payment reporting. It enables the central bank to automatically track capital flows (e.g., 'Software Export Proceeds') without delaying the settlement or requiring the client to file separate paperwork."
"Day 2: ISO 20022 Deep Dive","Explain the difference between 'V-Shape' and 'Y-Shape' message flows in Real-Time Gross Settlement (RTGS) systems.","In V-Shape, the full message goes from Bank A -> Central Bank -> Bank B. The Central Bank sees all data. In Y-Shape, a 'Copy' of the header/settlement info is stripped and sent to the Central Bank for settlement, while the full business data goes directly from Bank A -> Bank B. Y-Shape is often privacy-preserving for the central bank but can complicate the synchronization of settlement confirmation."
"Day 2: ISO 20022 Deep Dive","How does ISO 20022 improve 'Liquidity Management' compared to legacy batch systems?","ISO 20022 supports real-time, item-by-item processing and richer reporting (camt.052/053). This allows Treasurers to see exactly *what* funds are coming in (via <EndToEndId> and <RmtInf>) intra-day, rather than waiting for an end-of-day MT940. They can forecast cash positions more accurately and release outbound payments earlier, optimizing their use of capital."
"Day 2: ISO 20022 Deep Dive","What is 'Version Management' in ISO 20022 (e.g., pain.001.001.03 vs pain.001.001.09), and how does it impact upgrades?","ISO standards evolve. A message definition (like pain.001) has a version number (.03, .09). New versions add tags or change structures. Banks must manage 'coexistence' of versions—supporting clients who send old versions while the interbank space moves to newer ones. This requires a transformation layer that can up-convert legacy versions to the current standard required by the clearing system."
"Day 2: ISO 20022 Deep Dive","Explain the role of 'Supplementary Data' <SplmtryData> and the risk of using it.","<SplmtryData> allows communities to add custom fields not in the core ISO schema. While useful for specific local needs (e.g., a specific tax ID format), the risk is that it breaks interoperability. If a payment moves cross-border, the next bank likely won't understand the custom data and may drop it or reject the message. It should be used sparingly and only within closed user groups."
"Day 2: ISO 20022 Deep Dive","How does ISO 20022 handling of 'Claims and Compensation' (camt.056 / camt.029) differ from the manual way?","Traditionally, interest claims (for delayed payments) were handled via email or free-format MT199s. ISO attempts to standardize this via structured Investigation (camt.027) and Resolution (camt.029) messages, allowing banks to automate the calculation and settlement of compensation claims for failed or delayed transactions."
"Day 2: ISO 20022 Deep Dive","Describe how 'Batch Booking' vs 'Detail Booking' is indicated in a pain.001 file.","The <BtchBookg> flag in the Group Header controls this. If set to 'true', the client wants one debit for the total sum of the file (simplifying their statement). If 'false', they want individual debits for each transaction. The bank must respect this flag when generating the camt.053 statement sent back to the client."
"Day 2: ISO 20022 Deep Dive","Why is the 'Creation Date Time' <CreDtTm> critical for Duplicate Checking?","Banks use the combination of <MsgId>, <InitgPty>, and <CreDtTm> to identify unique files. If a file is re-sent (e.g., after a timeout), the bank checks these fields. If the <CreDtTm> is identical, it's a technical retry (safe to ignore if already processed). If <CreDtTm> is different but <MsgId> is same, it's a business error (duplicate ID usage). Precise timestamps prevent double-processing of payments."
"Day 2: ISO 20022 Deep Dive","Explain the 'Instruction for Creditor Agent' (<InstrForCdtrAgt>) usage.","This field allows the sender to give specific instructions to the beneficiary's bank, such as 'Phone beneficiary upon receipt' or 'Hold for collection'. In ISO, these are standardized codes (e.g., PHOB, HOLD) rather than free text, enabling the receiving bank to automate these actions or trigger alerts to branch staff."
"Day 2: ISO 20022 Deep Dive","What is the operational impact of 'Trimming' vs 'Truncation' in data handling?","Truncation is the hard cutting of data (losing the end). Trimming might refer to removing whitespace. In the context of ISO->MT conversion, 'Truncation' is the risk. Banks must decide: Do we truncate and flag it (e.g., adding '+' to indicate more data exists)? Or do we reject the payment? Most operational policies choose truncation with a 'Golden Copy' fallback to avoid stopping global payment flows."
"Day 2: ISO 20022 Deep Dive","How does the 'Charge Bearer' <ChrgBr> field impact the <Settlement Amount> in the interbank space?","If <ChrgBr> is 'DEBT' (Debtor), the full amount is sent. If 'SHA' (Shared) or 'CRED' (Creditor), intermediary banks may deduct fees from the principal. This means the <IntrBkSttlmAmt> (the money moving between banks) decreases at each hop, while the <InstdAmt> (Original Amount) remains constant as a reference. ISO 20022 clearly distinguishes these two amount fields to allow reconciliation of the fee deductions."
"Day 2: ISO 20022 Deep Dive","Explain the significance of 'Time Zones' in ISO 20022 Date/Time fields.","ISO 20022 uses UTC offsets (e.g., 2023-10-01T14:30:00+01:00). This is critical for global payments (e.g., Tokyo to New York). A 'Same Day' payment must respect the cut-off time in the receiver's time zone. Unambiguous time zone data prevents disputes about 'Late Payments' where value date was met in sender's zone but missed in receiver's."
"Day 2: ISO 20022 Deep Dive","What is a 'Payment Factory' and how does ISO 20022 enable it?","A Payment Factory is a centralized hub within a multinational corporation that gathers payments from all subsidiaries and processes them in bulk. ISO 20022 is the standard language for this. It allows the Factory to receive varied formats from subs, standardize them into pain.001, and send bulk files to banks. The 'On-Behalf-Of' tags in ISO are essential for this structure to work compliantly."
Day2_Payments_Fundamentals_ISO20022,Design a 'Duplicate Check' mechanism for a high-volume payment engine (1000 TPS).,"Architecture: Use a tiered approach. 1) Time-Window Cache (Redis): Store SHA-256 hash of critical fields (SenderIBAN + ReceiverIBAN + Amount + Date + Ref) with a 24-hour TTL. Check this first (sub-millisecond). 2) Historical Lookback (Database): If cache misses, async check typically not needed for real-time rejection unless window > 24h. 3) Key Design: Use the <InstrId> or UETR as the primary idempotent key. 4) Race Conditions: Use 'SETNX' (Set if Not Exists) in Redis to handle two identical requests arriving effectively simultaneously. 5) Response: Return the *original* status if duplicate found (Idempotency), or 'RJCT-DUPL' if it's a distinct submission of same data."
Day2_Payments_Fundamentals_ISO20022,"How would you handle a 'Split Brain' scenario in a distributed payment saga (e.g., Debit success, Credit timeout)?","Scenario: Debtor debited, but call to Creditor bank times out. Unknown if Creditor received it. Solution: 1) State Management: Saga persists state 'DEBIT_DONE, CREDIT_PENDING'. 2) Retry Logic: Background 'Reconciler' polls for 'PENDING' states > 1 min. 3) Idempotent Retry: Re-send the Credit instruction with same UETR. Creditor bank returns 'Already Processed' (Success) or processes it now. 4) Compensation: If max retries exhausted (e.g., Creditor Bank down for 24h), trigger manual intervention or auto-refund (Compensating Transaction) based on scheme rules. Never leave money in limbo."
Day2_Payments_Fundamentals_ISO20022,Explain the 'Golden Source' data strategy for Regulatory Reporting in a microservices landscape.,"Problem: Data scattered across Payment Engine, Sanctions, CRM. Solution: 1) Event Sourcing: All services emit domain events (PaymentCreated, Screened, Settled) to a centralized Kafka Log. 2) Reporting Consumer: A dedicated 'Regulatory Data Lake' consumes these events and builds a read-optimized model (the Golden Source). 3) Canonical Model: Map internal JSON/Avro events to the specific BOE/EBA reporting schema *at the edge* of the lake. 4) Lineage: Each report field is tagged with the source event ID. This ensures reports are consistent, audit-traceable, and don't degrade the transactional DB."
Day2_Payments_Fundamentals_ISO20022,"How do you solve the 'Thundering Herd' problem when a major payment scheme (e.g., SWIFT) comes back online after an outage?","Scenario: Queue builds up 100k payments. Scheme opens. All released at once crashes the gateway. Solution: 1) Exponential Backoff: Jitter the retry intervals so not all retry at t=0. 2) Rate Limiting/Throttling: Implement a 'Token Bucket' rate limiter on the outbound gateway (e.g., max 500 TPS). 3) Priority Queues: Process High-Value (CHAPS) queue first, then bulk (SEPA). 4) Circuit Breaker: Keep breaker 'Half-Open' allowing only small trickle of traffic to verify stability before opening floodgates."
Day2_Payments_Fundamentals_ISO20022,Critique the use of 'Low Code' platforms for Core Payment Processing vs Orchestration.,"Low Code (IPF etc.): Good for: Orchestration logic, simple routing rules, visualising flows for BAs, faster time-to-market for standard schemes. Bad for: High-performance compute (crypto/signing), complex custom integrations, massive scale concurrency (garbage collection overhead). Conclusion: Use Low Code for the 'Business Logic/Routing' layer (Orchestrator). Use custom Java/Go/Rust for the 'Heavy Lifting' (Parsing, Signing, Database I/O). Hybrid approach wins."
Day2_Payments_Fundamentals_ISO20022,Describe how you would implement 'Verifiable Lineage' for an ISO 20022 transformation project.,"Requirement: Prove to auditor that 'Field A' in source became 'Field B' in report without corruption. Solution: 1) Metadata Tagging: Every transformation function (map, truncate, enrich) logs metadata: {Input: 'ValA', Op: 'Map', Output: 'ValB', Timestamp}. 2) Hash Chaining: Compute hash of payload at Ingress, after Enrichment, and at Egress. 3) Traceability UI: Tool allowing search by UETR to see the 'Before' and 'After' payload of every transformation step. 4) Test Evidence: Automated regression suite comparing 10k production-like samples against expected output."
Day2_Payments_Fundamentals_ISO20022,How do you design for 'Regional Data Residency' (GDPR/sharding) in a global payment platform?,"Constraint: German data must stay in EU. US data in US. Design: 1) Sharded Database: Shard by 'Booking Entity Country'. EU Shard physically in Frankfurt AWS Region. US Shard in N. Virginia. 2) Routing Layer: API Gateway inspects 'Booking Entity' header and routes to correct regional cluster. 3) Replication: No cross-region replication of PII. Only replicate anonymized metadata for global analytics. 4) Access Control: EU Ops staff can only query EU Shard."
Day2_Payments_Fundamentals_ISO20022,What is the 'Ambiguity' problem in ISO 20022 Addresses and how does 'Structured Data' fix it for AI models?,"Ambiguity: Unstructured address '123 Oxford St, London' is a single string. AI/Rules Engine struggles: Is 'Oxford' the city or street? Is 'London' the city or a surname? Fix: Structured Data breaks this into <StrtNm>Oxford St</StrtNm>, <TwnNm>London</TwnNm>. Benefit: 1) Deterministic validation. 2) AI models for Fraud/Sanctions can feature-engineer on specific fields (e.g., 'Town Risk Score') with 100% confidence, drastically improving precision."
Day2_Payments_Fundamentals_ISO20022,Explain 'Version Coexistence' in APIs (v1 JSON vs v2 ISO-native) during migration.,"Strategy: 1) Router: API Gateway supports both /v1/pay (legacy JSON) and /v2/pay (ISO XML). 2) Translation: /v1 calls are internally mapped to the v2 ISO Canonical Model. 3) Downstream: Core engines only speak v2. 4) Response: v2 responses are mapped back to v1 JSON for legacy clients. 5) Deprecation: Monitor v1 usage. Announce sunset. Force clients to migrate to v2 to get new features (e.g., UETR tracking, Extended Remittance)."
Day2_Payments_Fundamentals_ISO20022,How do you handle 'Schema Evolution' (e.g. annual SWIFT Standards Release) without downtime?,"1) Decouple internal model from external schema. Internal model should be a superset of all versions. 2) Adapter Pattern: 'Ingress Adapter' reads old/new format -> converts to Internal Model. 'Egress Adapter' writes to specifically required version (2024 vs 2025). 3) Blue/Green Deploy: Deploy new adapters supporting 2025 spec alongside old ones. 4) Switch: On cut-over weekend, update config to route traffic to 2025 adapters. 5) Fallback: Keep old adapters active for receiving delayed messages from banks who haven't upgraded."
Day2_Payments_Fundamentals_ISO20022,Design a 'Sanctions Screening' integration that minimizes latency for Instant Payments.,"1) Async vs Sync: Must be Sync (blocking) for Instant. 2) Optimisation: Use a 'Fast Lane' for low-risk corridors (e.g., Domestic). 3) Caching: Cache 'Good Guys' (whitelisted benign false positives) to skip full screening engine (with TTL). 4) Parallelism: Run Screening in parallel with Accounting/Liquidity checks, join results at end. 5) Timeout: Set strict budget (e.g., 200ms). If timeout, fail safe (Reject) or fall back to manual queue depending on risk appetite."
Day2_Payments_Fundamentals_ISO20022,What is 'Reference Data Management' importance in Payment Orchestration?,"Reference Data: BIC Directory, Sort Codes, Country Codes, FX Rates. Importance: If stale, payments routing fails (e.g., sending to closed branch). Design: 1) Central Ref Data Service (Golden Source). 2) Cache Locally: Microservices cache Ref Data in memory (Guava/Caffeine) with short refresh (15 min) or subscribe to 'RefDataUpdated' events to invalidate cache. 3) Validation: Validate BICs against directory at Ingress (fail fast) rather than finding out at SWIFT Gateway."
Day2_Payments_Fundamentals_ISO20022,"Explain 'Message Correlation' in asynchronous request/reply flows (e.g., pain.001 -> pain.002).","Problem: You send a request, get a reply 10 mins later. Which request is it for? Solution: 1) Correlation ID: Generate unique ID (Instruction ID). 2) State Store: Save {ID, State='WAITING_ACK'} in DB. 3) Listener: When pain.002 arrives, extract <OrgnlInstrId>. 4) Match: Look up ID in DB. Update State='ACKNOWLEDGED'. 5) Callback: Trigger webhook to client. Challenge: If <OrgnlInstrId> is truncated/missing by intermediary, manual reconciliation is needed."
Day2_Payments_Fundamentals_ISO20022,How do you protect against 'Replay Attacks' in payment APIs?,"1) TLS/mTLS: Encrypt channel. 2) Timestamp Window: Reject requests where timestamp is > 5 mins old. 3) Nonce/JTI: Use a unique 'jti' (JWT ID) in the auth token that can only be used once. 4) Signature: Require client to sign the payload + timestamp. Server verifies signature. 5) Idempotency Key: As discussed, prevents processing same business request twice."
Day2_Payments_Fundamentals_ISO20022,What is the 'Outbox Pattern' and why use it for Payment Reliability?,"Problem: Saving Payment to DB and publishing Kafka event are two steps. If DB commits but Kafka fails, data is inconsistent. Solution: Outbox Pattern. 1) Transaction: Save Payment AND an 'Outbox Event' record to the SAME database table in one ACID transaction. 2) Relay: A background poller (Debezium/CDC) reads the Outbox table and publishes to Kafka. Guarantee: At-least-once delivery of events consistent with DB state."
Day2_Payments_Fundamentals_ISO20022,How would you architect a 'Liquidity Dashboard' for real-time Treasury monitoring?,"1) Sources: Ingest streams from CHAPS (RTGS), Faster Payments (Net), SEPA (TIPS). 2) Aggregator: Flink/Kafka Streams app aggregates 'Debits' and 'Credits' by Currency and Scheme window (1s, 1m). 3) Store: Time-Series DB (InfluxDB/Timescale). 4) UI: Grafana/React polling API. 5) Forecasting: Overlay ML model predicting 'Expected Outflow' based on historical pattern. 6) Alerts: Push notification if 'Actual Balance' < 'Predicted Requirement'."
Day2_Payments_Fundamentals_ISO20022,Describe 'Chaos Engineering' tests you would run on a Payment Platform.,"1) Latency Injection: Add 2s delay to DB calls. Does Circuit Breaker trip? 2) Node Kill: Kill random K8s pods. Do Sagas recover? 3) Network Partition: Block traffic between Payment Engine and Sanctions. Do messages queue or fail gracefully? 4) Time Travel: Skew clock on server. Does TOTP/Timestamp validation fail? Goal: Prove resilience before Production."
Day2_Payments_Fundamentals_ISO20022,How do you handle 'Precision Loss' in JSON (Float) vs ISO 20022 (Decimal) for amounts?,"Problem: JSON 'number' is floating point (IEEE 754). 0.1 + 0.2 != 0.3. Money must be exact. ISO uses BigDecimal. Solution: 1) API Contract: Always pass amounts as Strings in JSON ('100.50') not Numbers (100.5). 2) Internal: Use BigDecimal (Java) or Decimal (C#) types only. 3) DB: Use DECIMAL(19,4) or similar exact types. Never FLOAT/DOUBLE. 4) Validation: Regex check for max 2 decimal places (or 3 for TND/JOD currencies)."
Day2_Payments_Fundamentals_ISO20022,What is 'Payment Repair' logic and how can it be automated?,"Repair: Fixing invalid data (e.g., wrong BIC, missing code) to save the payment vs rejecting it. Manual Repair: Ops user UI. Auto Repair: Rules Engine. 1) Rule: If BIC invalid but Sort Code present, look up BIC. 2) Rule: If purpose missing and RmtInf contains 'Payroll', set Purpose='SALA'. 3) Audit: Must log 'Original' vs 'Repaired' value and 'Rule ID' applied for compliance. 4) Limits: Never auto-repair Amount, Currency, or Beneficiary Name (Fraud risk)."
Day2_Payments_Fundamentals_ISO20022,Design an 'Archive & Purge' strategy for Payment Data (GDPR vs Audit).,"Conflict: GDPR 'Right to be Forgotten' vs Bank 'Keep 7 Years' rule. Strategy: 1) Hot Storage (Postgres): Keep 3 months for live ops. 2) Warm Storage (S3/Parquet): Keep 2 years for analytics. 3) Cold Storage (Glacier): Keep 7-10 years for Audit. 4) Purge: Script runs daily to delete data > Retention Policy. 5) GDPR: If customer requests deletion, redact PII fields (Name -> '***') in all stores but Keep Transaction ID/Amount for financial balancing (Legal Obligation overrides Consent)."
Day1_DB_Concepts_Gemi,"Write a SQL query to find all payments (UETR, Amount) from the 'CHAPS_Payments' table that do not have a corresponding entry in the 'Sanctions_Screening_Log' table.","SELECT P.UETR, P.Amount FROM CHAPS_Payments P LEFT JOIN Sanctions_Screening_Log S ON P.UETR = S.UETR WHERE S.UETR IS NULL;"
Day1_DB_Concepts_Gemi,"Design a simple star schema for a 'Daily Payment Volume' report. List the Fact table and at least 3 Dimensions.","Fact Table: Fact_Payment_Transaction (Keys: DateKey, CustomerKey, SchemeKey, Metrics: Volume, Value). Dimensions: Dim_Date (Day, Month, Quarter), Dim_Customer (Name, Segment, Country), Dim_Scheme (Type, Currency, ClearingMechanism)."
Day1_DB_Concepts_Gemi,"A user complains that the 'Customer Transaction History' query is slow. The query filters by `CustomerID` and sorts by `TransactionDate` descending. What index would you create?","Create a composite non-clustered index on `(CustomerID, TransactionDate DESC)`. This allows the engine to seek the customer and scan the dates in the correct order without sorting."
Day1_DB_Concepts_Gemi,"Write a query using a Window Function to calculate the 'Previous Day Volume' for each Currency, alongside the 'Current Day Volume'.","SELECT Date, Currency, Volume, LAG(Volume, 1, 0) OVER (PARTITION BY Currency ORDER BY Date) AS Prev_Day_Vol FROM Daily_Aggregates;"
Day1_DB_Concepts_Gemi,"Explain how you would handle a requirement to store 'XML Payloads' (ISO 20022) in a relational database for auditing, while optimizing for frequent queries on specific fields like `DebtorName`.","Store the full XML in a CLOB/XML type column for audit compliance. Use 'Computed Columns' or extract key fields (DebtorName, Amount, UETR) into separate relational columns (indexed) during the ETL process for fast querying."
Day1_DB_Concepts_Gemi,"Write a query to identify duplicate UETRs in the `Inbound_Staging` table that arrived within the last 24 hours.","SELECT UETR, COUNT(*) FROM Inbound_Staging WHERE ArrivalTime > DATEADD(hour, -24, GETDATE()) GROUP BY UETR HAVING COUNT(*) > 1;"
Day1_DB_Concepts_Gemi,"Describe a strategy to archive payment data older than 7 years from a high-volume partitioned table without blocking live inserts.","Use Partition Switching. Create an empty staging table matching the partition schema. Switch the oldest partition (e.g., May 2015) out to the staging table (metadata operation, instant). Then back up and truncate the staging table."
Day1_DB_Concepts_Gemi,"You need to enforce that a 'Payment' cannot be inserted if the 'SettlementDate' is in the past. How do you implement this at the database level?","Add a CHECK Constraint on the `SettlementDate` column: `CHECK (SettlementDate >= CONVERT(date, GETDATE()))`."
Day1_DB_Concepts_Gemi,"Write a SQL snippet to update the status of a batch of payments to 'SETTLED' using a Join to a `Settlement_File` table.","UPDATE P SET P.Status = 'SETTLED' FROM Fact_Payments P INNER JOIN Settlement_File S ON P.UETR = S.UETR WHERE S.Confirmed = 1;"
Day1_DB_Concepts_Gemi,"How would you calculate the 'Failure Rate' (Failed / Total) per Scheme using a single SQL pass?","SELECT Scheme, SUM(CASE WHEN Status = 'FAILED' THEN 1 ELSE 0 END) * 1.0 / COUNT(*) AS FailureRate FROM Payments GROUP BY Scheme;"
Day1_DB_Concepts_Gemi,"Design a query to find 'Structuring' (Smurfing): Customers who made more than 3 transactions just under the reporting threshold ($10,000) in one day.","SELECT CustomerID, COUNT(*) as TxCount, SUM(Amount) as Total FROM Payments WHERE Amount BETWEEN 9000 AND 9999 AND TxDate = CAST(GETDATE() AS DATE) GROUP BY CustomerID HAVING COUNT(*) > 3;"
Day1_DB_Concepts_Gemi,"The regulatory requirement changes: we must now report on the 'Ultimate Debtor' field which is currently buried in the XML blob. How do you approach this schema change?","Alter the `Fact_Payments` table to add a new nullable column `UltimateDebtor`. Update the ETL pipeline to parse this tag from the XML and populate the column. Backfill historical data if required by the regulator."
Day1_DB_Concepts_Gemi,"Write a query to reconcile the sum of all payments grouped by Currency against a provided 'GL_Balances' table, showing only discrepancies.","WITH PaySum AS (SELECT Currency, SUM(Amount) as Total FROM Payments GROUP BY Currency) SELECT P.Currency, P.Total, G.Balance FROM PaySum P JOIN GL_Balances G ON P.Currency = G.Currency WHERE P.Total <> G.Balance;"
Day1_DB_Concepts_Gemi,"Explain the difference between `UNION` and `UNION ALL` and which is preferred for aggregating monthly payment tables for a yearly report.","`UNION` removes duplicates (expensive sort). `UNION ALL` just appends datasets. For aggregating distinct monthly tables, `UNION ALL` is preferred for performance as we know the months don't overlap."
Day1_DB_Concepts_Gemi,"How do you ensure that a 'Payment' and its associated 'Fee' (in a separate table) are inserted simultaneously?","Wrap both INSERT statements in a single explicit Transaction: `BEGIN TRANSACTION; INSERT INTO Payments...; INSERT INTO Fees...; COMMIT;`."
Day1_DB_Concepts_Gemi,"Write a query using `COALESCE` to display 'Unknown Region' if the `Region` column in the `Dim_Customer` table is NULL.","SELECT CustomerName, COALESCE(Region, 'Unknown Region') FROM Dim_Customer;"
Day1_DB_Concepts_Gemi,"Your 'Sanctions Hit' report is missing payments that were manually cleared (Status = 'False Positive'). The `HitStatus` column contains NULLs for uncleared items. How do you fix the query?","Ensure the WHERE clause handles NULLs or includes the specific status. E.g., `WHERE HitStatus = 'False Positive' OR HitStatus IS NULL` (depending on requirement) or simply remove the filter if all hits are needed."
Day1_DB_Concepts_Gemi,"Describe how to use a CTE (Common Table Expression) to simplify a complex report that requires 3 layers of aggregation.","Define the CTEs sequentially: `WITH DailySum AS (...), MonthlyAvg AS (SELECT * FROM DailySum...) SELECT * FROM MonthlyAvg`. This makes the code readable and modular compared to nested subqueries."
Day1_DB_Concepts_Gemi,"What is the risk of using a `VARCHAR(MAX)` or `TEXT` data type for the 'PaymentReference' column which is often joined on?","Performance degradation. Joins on large variable-length text fields are slow and consume vast memory. It prevents efficient indexing. Use a hashed key or fixed-length constraints if possible."
Day1_DB_Concepts_Gemi,"Write a query to find the 'Top 10' Customers by Volume, handling ties (e.g., if rank 10 and 11 have the same volume, show both).","SELECT TOP 10 WITH TIES CustomerID, Volume FROM Customer_Stats ORDER BY Volume DESC;"
Day1_DB_Concepts_Gemi,"Construct a SQL statement to insert a record into `Audit_Log` automatically whenever the `Payments` table is updated.","CREATE TRIGGER trg_AuditPayment ON Payments AFTER UPDATE AS BEGIN INSERT INTO Audit_Log (PaymentID, OldStatus, NewStatus) SELECT i.ID, d.Status, i.Status FROM inserted i JOIN deleted d ON i.ID = d.ID END;"
Day1_DB_Concepts_Gemi,"How would you use `PIVOT` to transform a table of `(Date, Scheme, Volume)` into a report with Schemes as columns?","SELECT Date, [CHAPS], [SEPA] FROM (SELECT Date, Scheme, Volume FROM Payments) AS SourceTable PIVOT (SUM(Volume) FOR Scheme IN ([CHAPS], [SEPA])) AS PivotTable;"
Day1_DB_Concepts_Gemi,"Write a query to calculate the 'Moving Average' of transaction volume over the last 3 days for each customer.","SELECT Date, CustomerID, AVG(Volume) OVER (PARTITION BY CustomerID ORDER BY Date ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS MovingAvg FROM DailyVolume;"
Day1_DB_Concepts_Gemi,"You need to delete duplicate payment records, keeping only the one with the latest timestamp.","WITH CTE AS (SELECT *, ROW_NUMBER() OVER (PARTITION BY UETR ORDER BY Timestamp DESC) AS rn FROM Payments) DELETE FROM CTE WHERE rn > 1;"
Day1_DB_Concepts_Gemi,"Write a query to find all 'Orphaned' payments in `Fact_Payments` that point to a non-existent `SchemeID`.","SELECT * FROM Fact_Payments P WHERE NOT EXISTS (SELECT 1 FROM Dim_Scheme S WHERE S.SchemeID = P.SchemeID);"
Day1_DB_Concepts_Gemi,"How do you find the number of days between the `InitiationDate` and `SettlementDate` for each payment?","SELECT DATEDIFF(day, InitiationDate, SettlementDate) AS DaysToSettle FROM Payments;"
Day1_DB_Concepts_Gemi,"Write a query to concatenate all 'Error Codes' for a single payment into a one comma-separated string (e.g. 'ERR01, ERR05').","SELECT PaymentID, STRING_AGG(ErrorCode, ', ') WITHIN GROUP (ORDER BY ErrorCode) FROM PaymentErrors GROUP BY PaymentID;"
Day1_DB_Concepts_Gemi,"You have a comma-separated string of 'InvoiceIDs' in a single column. How do you split this into multiple rows for analysis?","Use `STRING_SPLIT(ColumnName, ',')` (SQL Server) or `LATERAL FLATTEN` (Snowflake) to turn the array/string into a row set."
Day1_DB_Concepts_Gemi,"How would you implement 'Optimistic Concurrency' when updating a payment record?","Include a `Version` column. UPDATE Payments SET Amount = 100, Version = Version + 1 WHERE ID = 1 AND Version = 5. If 0 rows updated, someone else modified it."
Day1_DB_Concepts_Gemi,"Write a query to select the 'Second Highest' payment amount without using `MAX` or `TOP`.","SELECT Amount FROM (SELECT Amount, DENSE_RANK() OVER (ORDER BY Amount DESC) as Rank FROM Payments) as Ranked WHERE Rank = 2;"
Day1_DB_Concepts_Gemi,"Design a 'Date Dimension' table structure suitable for financial reporting.","Columns: DateKey (INT), FullDate (DATE), DayOfWeek, DayName, Month, MonthName, Quarter, Year, IsBusinessDay, IsHoliday, FiscalPeriod."
Day1_DB_Concepts_Gemi,"How do you check if a specific JSON path exists in a JSON column?","Use `JSON_VALUE(Column, '$.path.to.key') IS NOT NULL` or `JSON_EXISTS` depending on the DB dialect."
Day1_DB_Concepts_Gemi,"Write a query to generate a sequence of numbers from 1 to 100 without using a table (for test data generation).","WITH NumGen AS (SELECT 1 AS N UNION ALL SELECT N+1 FROM NumGen WHERE N < 100) SELECT N FROM NumGen OPTION (MAXRECURSION 100);"
Day1_DB_Concepts_Gemi,"How do you force a query to use a specific index if the optimizer is choosing a poor plan?","Use an Index Hint: `SELECT * FROM Payments WITH (INDEX(Idx_Date)) ...`. (Use with caution as data distribution changes)."
Day1_DB_Concepts_Gemi,"Write a query to find customers who have transacted in BOTH 'USD' and 'EUR'.","SELECT CustomerID FROM Payments WHERE Currency = 'USD' INTERSECT SELECT CustomerID FROM Payments WHERE Currency = 'EUR';"
Day1_DB_Concepts_Gemi,"How would you calculate the percentage of total daily volume that each scheme represents?","SELECT Scheme, Volume, Volume * 100.0 / SUM(Volume) OVER () AS PctOfTotal FROM DailySchemeStats;"
Day1_DB_Concepts_Gemi,"Write a query to find the 'Median' payment amount.","SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY Amount) OVER () FROM Payments;"
Day1_DB_Concepts_Gemi,"How do you perform a 'Case Insensitive' search on a Case Sensitive column?","`SELECT * FROM Users WHERE UPPER(Name) = 'JOHN'` or use `COLLATE Latin1_General_CI_AS`."
Day1_DB_Concepts_Gemi,"Design a table to store hierarchical 'Scheme Rules' (e.g., SEPA -> SEPA Inst -> Rule 1).","Table `SchemeHierarchy`: ID, ParentID, RuleName, RuleDescription. Use a recursive CTE to traverse."
Day1_DB_Concepts_Gemi,"Write a query to mask the first 12 digits of a PAN (Card Number) for a report.","SELECT CONCAT('************', RIGHT(PAN, 4)) FROM CardTransactions;"
"Day1_DB_Concepts_Perp","Design tables and SQL to compute daily CHAPS volume by corridor and compare with GL totals","Create payment table with settlement_date scheme amount debtor_country creditor_country and gl_daily_totals with booking_date gl_amount corridor key Use SELECT settlement_date debtor_country creditor_country SUM amount GROUP BY to compute corridor totals for scheme CHAPS then join to gl_daily_totals on date and corridor and compute differences between SUM amount and gl_amount to highlight mismatches"
"Day1_DB_Concepts_Perp","How would you model a payment fact table and dimension tables to support both AML and prudential reports","Create fact_payment with keys to dim_customer dim_account dim_country dim_scheme and fields like amount currency settlement_date sanctions_flag risk_scores fact holds transaction measures while dimensions hold descriptive attributes such as customer type region sector and scheme properties which lets AML and prudential reports group and filter by risk attributes without duplicating descriptions"
"Day1_DB_Concepts_Perp","Write a SQL query to compute STP rate per scheme per day using status in the payment table","SELECT settlement_date scheme SUM CASE WHEN status equals STP THEN one ELSE zero END as stp_count COUNT star as total_count CAST SUM CASE WHEN status equals STP THEN one ELSE zero END as decimal divided by CAST COUNT star as decimal as stp_rate FROM payment WHERE settlement_date between dates GROUP BY settlement_date scheme"
"Day1_DB_Concepts_Perp","How would you design indexes to support frequent daily CHAPS and SEPA volume reports without hurting write performance","Use a clustered index or partition on settlement_date and an included column scheme and add a non clustered index on scheme settlement_date amount or similar minimal covering keys Avoid indexing low cardinality flags and free text columns and keep OLTP indexes narrow while moving heavy ad hoc queries to the warehouse copy"
"Day1_DB_Concepts_Perp","Explain how you would implement a data quality check for payments without KYC using SQL","Join payment to customer on debtor_customer_id with a LEFT JOIN and filter rows where customer id is null Store results in a dq_missing_kyc table with payment id date and reason then schedule this query as a daily job and feed metrics such as count of missing KYC per corridor into MI and remediation dashboards"
"Day1_DB_Concepts_Perp","Design a SQL query that finds the top five high risk corridors by sanctions hit amount over the last thirty days","Join sanctions_hit to payment then group by debtor_country creditor_country and compute SUM amount filter to high severity hits and last thirty days sort by SUM amount descending and use ROW_NUMBER OVER ORDER BY SUM amount per corridor then filter where row_number is less than or equal to five"
"Day1_DB_Concepts_Perp","How would you refactor a cursor based report that loops through each scheme into a set based query","Instead of fetching one scheme at a time use a single query that groups by scheme and date to compute aggregates for all schemes at once for example SELECT settlement_date scheme COUNT star SUM amount GROUP BY settlement_date scheme store results into a reporting table and drop the cursor entirely which reduces overhead"
"Day1_DB_Concepts_Perp","Describe how to join payment and account tables to support a report that splits volumes by debtor segment","Join payment to account on debtor_account_id and then join account to customer on customer_id Select settlement_date scheme customer.segment and aggregate COUNT and SUM by these keys which yields daily scheme volume sliced by debtor segment for regulatory or business reporting"
"Day1_DB_Concepts_Perp","How would you use a window function to calculate a seven day rolling sum of FPS transaction amounts","Assuming a daily_stats table with scheme date and amount use SELECT scheme date amount SUM amount OVER PARTITION BY scheme ORDER BY date ROWS BETWEEN six PRECEDING AND CURRENT ROW as rolling_sum FROM daily_stats WHERE scheme equals FPS which gives each day the total amount of the last seven days including that day"
"Day1_DB_Concepts_Perp","Explain how you would detect duplicate payments in a fact table using SQL","Define business keys such as debtor_account creditor_account amount and settlement_date then run a query SELECT those keys COUNT star as dup_count FROM payment GROUP BY keys HAVING COUNT star greater than one to flag potential duplicates then decide whether duplicates are legitimate splits or require remediation"
"Day1_DB_Concepts_Perp","Design a reporting query that returns all payments which hit sanctions and their customer risk ratings","Join payment to sanctions_hit on payment id then join to customer through the relevant account and select payment identifiers amounts scheme settlement_date sanctions fields and customer risk_rating filtered where sanctions_hit exists then group or sort as needed for the report"
"Day1_DB_Concepts_Perp","How would you structure a stored procedure that generates a monthly cross border report","Create a procedure that accepts start_date and end_date stages data into a temp table by selecting payments where debtor_country not equal creditor_country aggregates by corridor and scheme writes results into a persistent reporting table or export file and logs run metadata and row counts for audit then schedule it monthly after core loads finish"
"Day1_DB_Concepts_Perp","Write a SQL pattern to find payments whose reported scheme does not match scheme inferred from local instrument codes","Join payment to scheme_dim on reported scheme then compare to a derived value from local_instrument column using a case expression and filter where reported scheme does not equal inferred scheme to highlight misclassified payments which can then be corrected before reporting"
"Day1_DB_Concepts_Perp","How would you design a reconciliation query between payment totals and GL for a specific product line","Join fact_payment_reg to gl_fact on booking_date currency and product_code aggregate payment amounts by those keys and compare to GL balances using a SELECT that shows both values and their difference filter where absolute difference exceeds a tolerance and deliver this as the base of a reconciliation report"
"Day1_DB_Concepts_Perp","Describe how to use NOT EXISTS to find sanctions hits that no longer have a corresponding payment record due to purge issues","Write SELECT from sanctions_hit s WHERE NOT EXISTS SELECT one FROM payment p WHERE p.id equals s.payment_id which returns any sanctions records without a matching payment then investigate purge policies so that sanctions data and payment data are retired consistently or archived together"
"Day1_DB_Concepts_Perp","How would you design a fact table for daily scheme volumes to speed up recurring regulatory queries","Create fact_scheme_daily with date scheme direction currency tx_count tx_amount and other measures populated from the detailed payment table in a nightly batch then point regulatory queries at this summary table which reduces data volume and improves response times while keeping detailed facts for drill down"
"Day1_DB_Concepts_Perp","Explain how GROUP BY and HAVING can be combined to support threshold based regulatory alerts","First GROUP BY dimensions such as corridor and scheme and compute aggregate sums then use HAVING clauses to filter groups where the sum exceeds a regulatory threshold so the result set represents only groups that need alerting or special handling"
"Day1_DB_Concepts_Perp","How would you implement a report run log table and populate it from a stored procedure","Create reg_report_run with report_name run_start run_end status row_count and error_message and in each reporting procedure insert a row at start then update status row_count and end time on successful completion or failure which provides an audit trail for operations and regulators"
"Day1_DB_Concepts_Perp","Design a query that calculates average payment amount per customer risk bucket for the last quarter","Join payment to customer through account filter settlement_date in last quarter group by customer risk_rating and compute AVG amount and COUNT distinct payments to get average and distribution per risk bucket used for AML and risk profiling"
"Day1_DB_Concepts_Perp","Explain how you would verify that a schema change did not break existing regulatory queries","Before deployment capture execution plans and result row counts for key regulatory queries then after schema change rerun the same queries compare counts and key aggregates and check plans for new scans or regressions using regression tests and a subset of production like data to ensure stability"
"Day1_DB_Concepts_Perp","How would you design a staging area for payments before loading into fact_payment_reg","Create narrow staging tables that closely mirror source structures such as raw_payment_iso and raw_ledger_entries include audit columns for load batch and timestamps apply only minimal type conversions here and run validation and mapping procedures that insert cleaned data from staging into fact_payment_reg and dimensions while logging any rejected rows"
"Day1_DB_Concepts_Perp","Describe a SQL based approach to track changes in customer risk rating over time for reporting","Use a slowly changing dimension pattern where dim_customer has effective_from and effective_to dates per version of each customer and ensure fact_payment joins to the correct version based on payment_date then reporting queries can reconstruct exposures by historical risk ratings rather than current ones"
"Day1_DB_Concepts_Perp","How would you implement near real time aggregation of sanctions hits per scheme using SQL and a warehouse","Use a streaming or micro batch pipeline that writes sanctions events into a staging table then periodically run an INSERT INTO summary table using INSERT SELECT with GROUP BY scheme and small time windows and either update or merge into a rolling summary allowing dashboards to query the summary instead of raw events"
"Day1_DB_Concepts_Perp","Explain how you would design indexes to support both corridor based and customer based AML reports","Create a clustered index on settlement_date and maybe scheme for main time based queries plus non clustered indexes on debtor_country creditor_country and another on debtor_customer_id and risk_rating include amount and status as included columns so corridor and customer reports can filter quickly without scanning entire tables"
"Day1_DB_Concepts_Perp","How would you write a query to find payment records that have no matching GL entry for the same date and amount","Join payment p to gl_entry g on date and amount using a LEFT JOIN and then filter where g.id is null such as SELECT p dot id p dot settlement_date p dot amount FROM payment p LEFT JOIN gl_entry g ON p dot settlement_date equals g dot booking_date AND p dot amount equals g dot amount WHERE g dot id IS NULL"
"Day1_DB_Concepts_Perp","Design a table structure to store regulatory report runs and link them back to the data snapshot used","Create reg_snapshot with snapshot_id snapshot_date and notes and reg_report_run with run_id snapshot_id report_name run_start run_end status row_count and foreign key to snapshot then always record which snapshot a given run used so auditors can see exactly which data slice produced each report"
"Day1_DB_Concepts_Perp","How would you handle regulatory reports that need both gross and net flows for the same population of payments","Store all payments in fact_payment_reg with signed amounts and create two different views gross_flows that uses SUM ABS amount for gross and net_flows that uses SUM amount with appropriate GROUP BY ensuring both derive from the same base rows to avoid discrepancies"
"Day1_DB_Concepts_Perp","Explain a strategy to phase in a new column required for reporting without breaking existing queries","Add the new column with a default or NULL value but do not reference it in existing views then backfill historical data with controlled batches and later update views and procedures to use the column while keeping compatible behaviours until all dependent components are updated"
"Day1_DB_Concepts_Perp","How would you design SQL to identify top ten customers by cross border volume over the last year","Select from fact_payment_reg joined to dim_customer filter cross border indicator equals true and settlement_date between last year dates group by customer_id and customer_name order by SUM amount descending and use TOP ten or LIMIT ten to return the largest volumes"
"Day1_DB_Concepts_Perp","Describe how you would use a CTE to simplify a complex regulatory query","Write a common table expression that first filters and shapes base payments for the reporting period such as WITH base AS SELECT columns FROM payment WHERE filters then in subsequent SELECT statements join base to dimensions and aggregate by corridors or risk buckets making the final query easier to read and maintain"
"Day1_DB_Concepts_Perp","How would you write SQL to find KYC records that exist without any active account and therefore without payments","Select from customer c WHERE NOT EXISTS SELECT one FROM account a WHERE a dot customer_id equals c dot id AND a dot status equals active and optionally filter where NOT EXISTS SELECT one FROM payment p JOIN account a ON p dot debtor_account_id equals a dot id WHERE a dot customer_id equals c dot id"
"Day1_DB_Concepts_Perp","Design a query to calculate median payment amount per scheme and explain any limitations","Many databases lack a built in median so use window functions such as ROW_NUMBER and COUNT OVER to locate the middle row ordering by amount per scheme and settlement period but explain that this can be expensive at scale and often needs a dedicated analytic warehouse or approximate median functions"
"Day1_DB_Concepts_Perp","Explain how you would detect and quantify truncation issues when mapping ISO 20022 text fields into legacy columns","Compare lengths and content between raw ISO staging columns and mapped legacy columns using queries that select cases where LEN iso_field greater than LEN legacy_field or where suffixes are missing and count occurrences per field and corridor to size the problem before designing a golden copy strategy"
"Day1_DB_Concepts_Perp","How would you write SQL to generate a simple stress test scenario where all payment amounts are increased by ten percent for capital calculations","Select from fact_payment_reg and in the SELECT list compute stressed_amount as amount multiplied by one point one grouped or aggregated as required for capital metrics while leaving base amount unchanged and writing results into a separate stress_test table for analysis"
"Day1_DB_Concepts_Perp","Describe a pattern to ensure that loading a large reporting table can be done with minimal downtime for consumers","Load data into a new table or partition then swap it into place with an atomic rename or partition switch operation update any metadata and let consumers continue to query the logical table name without being impacted by load activities"
"Day1_DB_Concepts_Perp","Explain how you would debug a production report that suddenly runs much slower after a data volume increase","Capture the new execution plan and compare to the previous baseline check for new scans spills or different join strategies look for skewed data distributions that break previous assumptions and adjust indexes statistics or query structure such as adding predicates or hints to recover acceptable performance"
Day2_ISO20022_Msgs_Gemi,"Design the SQL table structure for an 'ISO_Parties' table to store Debtor/Creditor details, ensuring you capture both Structured and Unstructured addresses.","Columns: PartyID (PK), Name, AddressType (Structured/Unstructured), StreetName, BuildingNb, PostCode, TownName, Country, AddressLine1, AddressLine2, PartyType (Dbtr/Cdtr/Ultmt). Logic: If <PstlAdr> contains <StrtNm>, populate Structured columns; if it contains <AdrLine>, populate AddressLine columns. Use COALESCE in reporting views to select the available data."
Day2_ISO20022_Msgs_Gemi,"Write a pseudo-code or SQL logic to calculate the 'End-to-End Latency' of a payment using `pain.001` and `pain.002` timestamps.","SELECT p1.EndToEndId, DATEDIFF(millisecond, p1.CreDtTm, p2.CreDtTm) as Latency_MS FROM Inbound_pain001 p1 JOIN Outbound_pain002 p2 ON p1.MsgId = p2.OrgnlMsgId WHERE p2.TxSts IN ('ACCP', 'RJCT');"
Day2_ISO20022_Msgs_Gemi,"Explain how you would architect a solution to detect 'Truncation Risk' when mapping ISO 20022 Names to a legacy 35-character limitation.","Create an ETL step that compares `LENGTH(ISO_Name)` against 35. If > 35, flag the record in a 'Truncation_Exception' table. For the legacy system, substring(ISO_Name, 1, 35). For the Regulatory Reporting layer, always store and use the full ISO_Name from the Golden Copy."
Day2_ISO20022_Msgs_Gemi,"Design a mapping logic to extract the 'True Beneficiary' for AML screening, considering Ultimate Creditor presence.","IF <UltmtCdtr> IS NOT NULL THEN Screening_Name = <UltmtCdtr><Nm> ELSE Screening_Name = <Cdtr><Nm>. Ensure <UltmtCdtr> takes precedence as it represents the sanctions risk."
Day2_ISO20022_Msgs_Gemi,"Describe the SQL logic to categorize payments into 'Corridors' (e.g., 'GB-US') using `pacs.008` data.","SELECT CONCAT(Dbtr_Country, '-', Cdtr_Country) as Corridor, COUNT(*) FROM (SELECT substring(Dbtr_IBAN, 1, 2) as Dbtr_Country, substring(Cdtr_IBAN, 1, 2) as Cdtr_Country FROM pacs008_Table) GROUP BY 1. If IBAN missing, fallback to <Dbtr><PstlAdr><Ctry>."
Day2_ISO20022_Msgs_Gemi,"How would you use `pacs.002` data to build a 'Bank Performance Dashboard' for senior management?","Aggregate `pacs.002` by `InstdAgt` (Receiving Bank). Measure: 1. % of `ACCP` vs `RJCT`. 2. Average time between `pacs.008` send and `pacs.002` receipt. 3. Count of `RJCT` with Reason `AC03` (Invalid Acct) to identify poor data quality."
Day2_ISO20022_Msgs_Gemi,"Explain the Data Lineage required to prove to an auditor that a 'Sanctions Hit' on an Ultimate Debtor was correctly linked to the original payment.","Trace: Sanctions_Alert_ID -> Payment_Internal_ID -> UETR -> Raw_XML_Storage (Golden Copy). Show the query extracting <UltmtDbtr><Nm> from the XML and the fuzzy match score generated by the screening engine against the Watchlist."
Day2_ISO20022_Msgs_Gemi,"Construct a query to identify 'Cover Payments' that might be missing the mandatory underlying customer details (Stripping).","SELECT UETR FROM pacs009_Table WHERE MsgDefId = 'pacs.009.001.08' (COV) AND (Underlying_Dbtr_Name IS NULL OR Underlying_Cdtr_Name IS NULL). This indicates a malformed COV message."
Day2_ISO20022_Msgs_Gemi,"Design the logic to handle 'Multi-Currency' reporting where `InstdAmt` is GBP but `IntrBkSttlmAmt` is USD.","Store both amounts and currencies in the Fact Table. Add a 'Reporting_USD' column. IF Currency = 'USD' THEN Amount ELSE Amount * Daily_FX_Rate. Highlight payments where `InstdCcy` <> `SttlmCcy` as 'FX Transactions'."
Day2_ISO20022_Msgs_Gemi,"How would you extract 'Remittance Information' for a 'Trade Finance' monitoring report?","Scan <RmtInf><Strd> for 'Invc' (Invoice) or 'Cmmrcl' (Commercial) codes. If <Ustrd> exists, use NLP or Regex to search for keywords like 'Bill of Lading', 'Shipment', 'Goods'."
Day2_ISO20022_Msgs_Gemi,"Create a validation rule for the ETL pipeline to reject `pacs.008` messages with invalid 'Country Codes'.","Check <Ctry> against the ISO 3166-1 alpha-2 reference table. IF <Ctry> NOT IN Reference_Table OR <Ctry> IS NULL THEN Trigger Data Quality Alert 'Invalid Country Code'."
Day2_ISO20022_Msgs_Gemi,"Describe how you would link a `camt.056` (Cancellation) to the original `pacs.008` in a relational database.","Use the `<OrgnlEndToEndId>` or `<OrgnlUETR>` in the `camt.056`. UPDATE Payment_Fact_Table SET Is_Cancellation_Requested = 1, Cancellation_Reason = <Rsn><Cd> WHERE UETR = <OrgnlUETR>."
Day2_ISO20022_Msgs_Gemi,"Explain the logic to differentiate 'Retail' vs 'Corporate' payments based on `pacs.008` data for segmentation reporting.","Check `<InitgPty><Id><OrgId>` (Organization) vs `<InitgPty><Id><PrvtId>` (Private Individual). OR check `<PmtTpInf><CtgyPurp>` (e.g., `SALA` = Retail/Payroll, `TREA` = Corp Treasury)."
Day2_ISO20022_Msgs_Gemi,"Design a 'Duplicate Check' mechanism for incoming `pacs.008` messages using UETR.","Create a Unique Index on the UETR column in the Staging Table. On Insert: IF UETR exists, check `CreDtTm`. If same, discard (Retry). If different timestamp but same UETR, flag as 'Duplicate UETR Error'."
Day2_ISO20022_Msgs_Gemi,"How would you calculate 'Straight Through Processing (STP) Rate' using ISO status messages?","Count(Total Payments) = Count(pacs.008). Count(Manual Intervention) = Count(pacs.002 with status `PDNG` or `RJCT` linked to repair queues). STP % = (Total - Manual) / Total * 100."
Day2_ISO20022_Msgs_Gemi,"Write a query to extract all unique 'Intermediary Agents' used in the last month to analyze correspondent banking concentration risk.","SELECT IntrmyAgt_BIC, COUNT(*) as Vol, SUM(Amt) as Val FROM Payment_Fact WHERE TxDate > DATEADD(month, -1, GETDATE()) AND IntrmyAgt_BIC IS NOT NULL GROUP BY IntrmyAgt_BIC ORDER BY Vol DESC."
Day2_ISO20022_Msgs_Gemi,"Explain how to map the `ChargeBearer` field to the General Ledger for fee accounting.","IF `ChrgBr` = 'DEBT': Debit Customer Full Amount + Fee, Credit Nostro Full Amount, Credit Fee Income. IF 'CRED': Debit Customer Full Amount, Credit Nostro (Amount - Fee), Credit Fee Income."
Day2_ISO20022_Msgs_Gemi,"Design the schema for storing `Structured` addresses to support future 'Fuzzy Logic' screening.","Columns: StreetName_Clean, City_Clean, PostCode_Clean. ETL Logic: Remove special characters, normalize abbreviations (St -> Street). Store phonetic hashes (Soundex/Metaphone) of the City and Street for faster fuzzy matching."
Day2_ISO20022_Msgs_Gemi,"How would you derive the 'Payment Purpose' if the `<Purp>` code is missing in the `pacs.008`?","Fallback Logic: 1. Check `<CtgyPurp>`. 2. Regex search `<RmtInf>` for keywords (e.g., 'Salary', 'Rent'). 3. Default to 'General/Unclassified' and flag for Data Quality improvement."
Day2_ISO20022_Msgs_Gemi,"Describe the mapping of `pacs.004` (Return) fields back to the original `pacs.008` for a 'Returns Analysis' report.","Join `pacs.004` on `<OrgnlUETR>`. Extract `<RtrRsnInf><Rsn><Cd>` (Reason) and map to a readable description (e.g., `AC01` -> 'Incorrect Account'). Group by Reason Code to find top causes of returns."
Day2_ISO20022_Msgs_Gemi,"Explain how to handle 'Batched' `pain.001` messages (One Group Header, Multiple PmtInf) in an ETL process.","Loop through each `PmtInf` block. Inside each, loop through `CdtTrfTxInf`. Denormalize: Copy `GrpHdr` ID and `PmtInf` Details (Execution Date, Debtor) into *every* transaction row in the target Fact Table."
Day2_ISO20022_Msgs_Gemi,"Design a report to identify 'Structured vs Unstructured' adoption rates by corporate customers.","SELECT Customer_ID, COUNT(CASE WHEN Address_Type = 'Structured' THEN 1 END) as Struct_Count, COUNT(*) as Total FROM Payment_History GROUP BY Customer_ID. Calc Adoption %."
Day2_ISO20022_Msgs_Gemi,"How would you store the `ExchangeRateInformation` (<XchgRateInf>) for multi-currency auditability?","Create a JSON or separate table `Payment_FX_Details` linked by PaymentID. Columns: SourceCurrency, TargetCurrency, Rate, ContractId (<CtrctId>). Essential for proving the rate applied was agreed upon."
Day2_ISO20022_Msgs_Gemi,"Write a logic to detect 'Split Payments' (Structuring) using ISO 20022 amounts and timestamps.","Window Function: SUM(Amount) OVER (PARTITION BY Debtor, Creditor ORDER BY Timestamp RANGE BETWEEN CURRENT ROW AND 24 HOURS PRECEDING). IF Sum > Threshold (10k) AND Count > 1 THEN Flag 'Potential Structuring'."
Day2_ISO20022_Msgs_Gemi,"Explain the significance of `SvcLvl` (Service Level) vs `LclInstrm` (Local Instrument) for routing logic.","Map `SvcLvl` to the broad scheme (e.g., SEPA). Map `LclInstrm` to the specific product (e.g., SEPA Instant vs SEPA Credit Transfer). Routing engine uses `LclInstrm` to select the correct clearing rail."
Day2_ISO20022_Msgs_Gemi,"How would you ensure that a 'Test' payment sent by a developer doesn't end up in the Regulatory Report?","Filter based on `PrCG` (Processing Code) or `GrpHdr` -> `TestCode` (<TstCd>). If <TstCd> exists/is true, route to 'Test_Log' table, exclude from 'Production_Fact' table."
Day2_ISO20022_Msgs_Gemi,"Design a 'Sanctions List Management' table to link with your ISO payments.","Table: Watchlist_Entity. Columns: EntityID, FullName, Alias_List (Array), Address_Fragments, List_Source (OFAC/UN), Last_Updated. Join logic uses Fuzzy Match between ISO Party Name and Watchlist Name."
Day2_ISO20022_Msgs_Gemi,"Explain how to map the `InstructingAgent` and `InstructedAgent` to a 'Network Graph' for visualization.","Nodes = BICs. Edges = Payment Flows. Source Node = `InstructingAgent`. Target Node = `InstructedAgent`. Edge Weight = `IntrBkSttlmAmt`. Visualize to see liquidity flow and bottlenecks."
Day2_ISO20022_Msgs_Gemi,"Write a query to identify 'Dormant' correspondents (Agents who haven't sent us a `pacs.008` in 90 days).","SELECT Agent_BIC FROM Agent_Master WHERE Agent_BIC NOT IN (SELECT DISTINCT InstgAgt_BIC FROM Inbound_pacs008 WHERE TxDate > DATEADD(day, -90, GETDATE()));"
Day2_ISO20022_Msgs_Gemi,"How do you map the ISO `Nm` (Name) field if it exceeds the database column limit (e.g., 70 chars vs 140 chars)?","Do NOT truncate silently. Implementation: 1. Increase DB column size (preferred). 2. Split into `Name_Part1`, `Name_Part2`. 3. Store full name in a separate `Long_Text_Overflow` table linked by ID."
Day2_ISO20022_Msgs_Gemi,"Explain how to validate that a `pacs.009` is NOT a 'Serial' payment when it should be 'Cover'.","Check if the `pacs.009` is settling a `pacs.008` sent via a different path. If yes, it MUST be `COV` (contain Sequence B). If it's a `CORE` 009 but settling customer funds indirectly, flag as 'Improper Use of Serial Method'."
Day2_ISO20022_Msgs_Gemi,"Design a 'Schema Version' tracking column for your Payment Table.","Add column `MsgDefId` (Message Definition ID). Store values like `pacs.008.001.08` or `pacs.008.001.09`. Use this to apply version-specific parsing logic (e.g., handling new fields added in version 09)."
Day2_ISO20022_Msgs_Gemi,"How would you parse the `IBAN` to extract the 'Country Code' and 'Bank Identifier' for analytics?","Substring(IBAN, 1, 2) = Country. Substring(IBAN, 5, 4) = Bank Code (in many formats like GB). Use a regex or lookup table specific to each Country's IBAN structure to extract the Branch/Account details."
Day2_ISO20022_Msgs_Gemi,"Explain the logic to identify 'U-Turn' payments (A -> B -> A) which might indicate wash trading.","Self-Join on `Fact_Payment`. Criteria: P1.Debtor = P2.Creditor AND P1.Creditor = P2.Debtor AND P1.Amount = P2.Amount AND P2.Time BETWEEN P1.Time AND P1.Time + 1 Hour."
Day2_ISO20022_Msgs_Gemi,"Design the 'Role-Based Access Control' (RBAC) for the ISO Golden Copy database.","1. 'System Admin': Full Access. 2. 'Compliance Officer': Read-Only on PII (Names/Addresses) for investigations. 3. 'Data Analyst': Read-Only with PII Masked/Hashed (can see amounts/countries, not names)."
Day2_ISO20022_Msgs_Gemi,"How would you handle a situation where `pacs.008` arrives with `IntrBkSttlmDt` in the past?","Flag as 'Back-Valued'. Report to Treasury/Liquidity desk immediately as it affects interest calculations. In Reporting, assume effective settlement is TODAY unless back-valuation is authorized."
Day2_ISO20022_Msgs_Gemi,"Write a query to find the top 5 'Purpose Codes' used by a specific Debtor.","SELECT Purp_Code, COUNT(*) as Usage_Count FROM Payment_Fact WHERE Dbtr_ID = @DebtorID GROUP BY Purp_Code ORDER BY Usage_Count DESC LIMIT 5."
Day2_ISO20022_Msgs_Gemi,"Explain how to map `RemittanceLocationMethod` (<RmtLctnMtd>) = `URID` (URI).","This means the actual invoice data is hosted on a website (URL), not in the message. Store the URL found in `<ElctrncAdr>`. For AML, you may need a separate process to crawl/scrape that URL if risk is high (rare/complex)."
Day2_ISO20022_Msgs_Gemi,"Design a dashboard widget for 'Payment Repair Queues'.","Query `pacs.002` (RJCT) + Internal Error Logs. Group by 'Error Type' (e.g., 'Invalid IBAN', 'Sanctions Hold', 'Schema Fail'). Display count of items in queue vs SLA (Time in Queue)."
Day2_ISO20022_Msgs_Gemi,"How would you use `InstdAmt` Currency vs `IntrBkSttlmAmt` Currency to report on 'FX Revenue Leakage'?","Identify payments where currencies differ. Compare the implied FX rate (Instd/IntrBk) against the 'Market Mid-Rate' at that timestamp. Difference = Spread/Margin. If Margin < 0, flag as Revenue Leakage."
Day2_Ext_Resources_Gemi,"Design a 'CBPR+ Compliance Validator' logic for the `ChargeBearer` field.","Rule: Check `ChrgBr`. IF 'DEBT' (Debtor) OR 'SHAR' (Shared) -> PASS. IF 'CRED' (Creditor) -> REJECT (CBPR+ restricts the use of 'CRED' for cross-border payments unless a specific Service Level Agreement exists). Warn if 'CRED' is found."
Day2_Ext_Resources_Gemi,"Explain how you would map the Bank of England's 'Purpose Code' requirement into your database schema for CHAPS.","Create a reference table `Ref_BoE_PurposeCodes` (Code, Description). In the `Payment_Ingestion` pipeline, extract `<Purp><Cd>`. Validate existence in the reference table. If valid, store in `Fact_Payment.PurposeCode`. If invalid/missing, flag as 'Repair' (as BoE mandates valid codes for certain flows)."
Day2_Ext_Resources_Gemi,"Construct a SQL query to identify violations of the FATF 'Travel Rule' in your outbound payment table.","SELECT P.PaymentID, P.DebtorName, P.DebtorAddress FROM Outbound_Payments P WHERE P.Amount > 1000 AND (P.DebtorName IS NULL OR P.DebtorAddress IS NULL). FATF 16 requires verifying originator info for transfers above threshold (EUR/USD 1000)."
Day2_Ext_Resources_Gemi,"Design the logic to handle a 'SEPA R-Transaction' (Reject) mapped back to the original payment.","Ingest `pacs.002` (Status Report). Extract `OrgnlMsgId` and `TxSts` (RJCT). Lookup original payment in `Fact_Payment` using `MsgId`. UPDATE `Fact_Payment` SET Status = 'REJECTED', RejectReason = `<Rsn><Cd>`. Trigger notification to Operations."
Day2_Ext_Resources_Gemi,"How would you implement the Wolfsberg Group's guidance on 'Cover Payment' monitoring in your AML system?","Configure the AML monitoring engine to specifically flag `pacs.009` messages. IF `Seq B` (Underlying Customer) exists -> Screen `Debtor` and `Creditor` in Seq B against Sanctions List. IF `Seq B` matches a watchlist entity, freeze the Cover Payment."
Day2_Ext_Resources_Gemi,"Describe the XML structure you would generate for a 'Structured Address' to meet CBPR+ 2025 requirements.","Generate `<PstlAdr>` block: `<StrtNm>` (Street), `<BldgNb>` (Number), `<PstCd>` (Zip), `<TwnNm>` (City), `<Ctry>` (ISO Code). Ensure no data is placed in `<AdrLine>` if structured data is available, as `<AdrLine>` will be deprecated/restricted."
Day2_Ext_Resources_Gemi,"Write a pseudo-code validation for 'AnyBIC' usage in a payment instruction.","IF `Agent_BIC` matches regex `^[A-Z0-9]{4}[A-Z]{2}[A-Z0-9]{2}([A-Z0-9]{3})?$` (Standard BIC) -> PASS. ELSE -> Flag as 'Non-Standard Identifier'. Check if `AnyBIC` flag is enabled in the channel configuration. If not enabled, Reject."
Day2_Ext_Resources_Gemi,"Design a 'Golden Source' storage strategy for ISO 20022 messages to comply with XMLdation's best practices.","Store the raw XML blob in an immutable `Data Lake` bucket (S3/Azure Blob). Index metadata (UETR, MsgId, Date, Sender, Receiver) in a lightweight SQL table pointing to the blob. This ensures perfect fidelity for audit/resubmission without schema constraints."
Day2_Ext_Resources_Gemi,"Explain how to map a `camt.053` (Statement) entry back to the original `pacs.008` for reconciliation.","Extract `<Ntry><NtryDtls><TxDtls><Refs><EndToEndId>` from `camt.053`. Query `Fact_Payment` where `EndToEndId` matches. Verify `Amount` matches `<Ntry><Amt>`. If matched, update `Fact_Payment` status to 'RECONCILED_LEDGER'."
Day2_Ext_Resources_Gemi,"Design a rule to detect 'Stripping' in a `pacs.009` payment stream.","Filter for `pacs.009` messages where `SettlementMethod` = 'COV'. Check if `UnderlyingCustomerCreditTransfer` block is NULL or Empty. If True -> Flag High Risk 'Potential Stripping'. Also check if `Debtor` name is generic (e.g., 'Client')."
Day2_Ext_Resources_Gemi,"How would you calculate 'Payment Density' by Purpose Code using CHAPS data?","SQL: SELECT PurposeCode, COUNT(*) as Vol, SUM(Amount) as Val FROM CHAPS_Payments GROUP BY PurposeCode. Use this to analyze economic activity (e.g., spike in 'Property' payments)."
Day2_Ext_Resources_Gemi,"Describe the ETL transformation required to handle 'Extended Character Sets' if a bank's legacy system only supports ASCII.","Input: ISO XML (UTF-8). Transformation: Apply `diacritic removal` (e.g., 'Müller' -> 'Mueller', 'François' -> 'Francois'). If non-Latin (Chinese/Arabic), map to a placeholder 'REFER_TO_XML' and force manual repair/review in a unicode-capable tool."
Day2_Ext_Resources_Gemi,"Design a 'Version Agnostic' ingestion pipeline for ISO 20022.","Use an 'Abstract Data Model' (Canonical Model) in the application. Create 'Adapters' for each version (`pacs.008.001.08`, `.09`). The Adapter parses the specific XML version and maps it to the internal Canonical object. This isolates the core system from annual SWIFT standards updates."
Day2_Ext_Resources_Gemi,"Explain how to validate the 'Interbank Settlement Date' against the 'Creation Date' for a TARGET2 payment.","Rule: `InterbankSettlementDate` must be >= `CreationDate` (Date part). TARGET2 does not accept back-valued payments. If `SettlementDate` < `CurrentDate`, Reject or hold for next day (depending on SLA)."
Day2_Ext_Resources_Gemi,"Construct a SQL query to find 'Dormant Corridors' (Country pairs that haven't traded in 6 months).","SELECT Dbtr_Country, Cdtr_Country, MAX(TxDate) as Last_Tx FROM CrossBorder_Payments GROUP BY Dbtr_Country, Cdtr_Country HAVING MAX(TxDate) < DATEADD(month, -6, GETDATE());"
Day2_Ext_Resources_Gemi,"Design the logic to extract 'Ultimate Beneficiary' data for a FATF Travel Rule report.","Check `<UltmtCdtr>` block. If present, extract Name, ID, Address. If absent, fallback to `<Cdtr>`. FATF requires identifying the natural person; if `<UltmtCdtr>` is populated, it represents the true beneficiary and must be reported."
Day2_Ext_Resources_Gemi,"How would you map the `RemittanceInformation` field to a legacy 140-char column without losing critical data?","Check `Structured` first. Concatenate `Type` + `Reference`. If `Unstructured`, take the first 140 chars. Store the FULL text in a separate `Overflow_Note` table. Append a flag '*' to the truncated text to indicate 'More data available'."
Day2_Ext_Resources_Gemi,"Explain how to use the `InstrId` (Instruction ID) vs `EndToEndId` in a duplicate check.","`InstrId` is unique only per link (Sender->Receiver). `EndToEndId` is unique per transaction lifecycle. For internal duplicate check, use `InstrId` + `SenderBIC` + `Date`. For global/customer duplicate check, use `EndToEndId` + `Debtor` + `Date`."
Day2_Ext_Resources_Gemi,"Design a validation rule for SEPA 'IBAN Only' compliance.","IF Scheme = 'SEPA': Check `DbtrAcct.Id.IBAN` and `CdtrAcct.Id.IBAN`. Verify valid format. Ensure `DbtrAgt.BIC` and `CdtrAgt.BIC` are OPTIONAL (not mandatory). If BIC is missing, derive Bank Name from IBAN Bank Code using a reference directory."
Day2_Ext_Resources_Gemi,"Write a logic to determine if a payment is 'On-Us' (Internal) using BIC matching.","IF `InstructingAgent.BIC` (Sender) == `InstructedAgent.BIC` (Receiver) -> Flag 'Internal Transfer'. Route to Internal Bookkeeping Engine instead of SWIFT Network to save costs."
Day2_Ext_Resources_Gemi,"How would you implement a 'Sanctions Screen' on the `PlcOfBirth` (Place of Birth) field in `pacs.008`?","Extract `<Dbtr><PrvtId><DtAndPlcOfBirth><PlcOfBirth>`. Run fuzzy match against Sanctioned Cities/Regions (e.g., 'Tehran', 'Pyongyang'). If Match > Threshold -> Trigger Alert. This is critical as the `Country` field might be 'clean' (e.g., residence in UK) but birth place reveals risk."
Day2_Ext_Resources_Gemi,"Design a dashboard metric for 'CBPR+ Readiness'.","Metric: '% of Outbound Payments with Structured Addresses'. Target: 100% by Nov 2025. Visual: Line chart showing trend over time. Drill-down by 'Client Segment' to identify customers sending unstructured data."
Day2_Ext_Resources_Gemi,"Explain the mapping of `pacs.002` 'Transaction Status' codes to a simplified 'Green/Amber/Red' dashboard.","Green: `ACSC` (Settled), `ACCP` (Accepted). Amber: `PDNG` (Pending), `ACWC` (Accepted with Change). Red: `RJCT` (Rejected). Map these codes to a `Status_Color` column for visualization."
Day2_Ext_Resources_Gemi,"Construct a query to audit 'Manual Repairs' on payment messages.","SELECT P.UETR, P.Original_Msg, P.Final_Msg, U.UserName FROM Payment_Audit P JOIN Users U ON P.User_ID = U.ID WHERE P.Action = 'REPAIR'. Analyzing this helps identify systemic data quality issues (e.g., bad static data)."
Day2_Ext_Resources_Gemi,"Design a logic to handle 'Return of Funds' (pacs.004) reconciliation.","When `pacs.004` arrives: 1. Match `<OrgnlUETR>` to original credit. 2. Verify `ReturnAmount` <= `OriginalAmount`. 3. Create a 'Negative' entry in the Daily Volume report linked to the original transaction date (or current date depending on accounting rules)."
Day2_Ext_Resources_Gemi,"How would you ensure that a `pain.001` file signature is validated before parsing?","Implement 'Business Application Header (BAH)' validation. Check the `<Sgntr>` block (Digital Signature). Verify against the sender's Public Key stored in the PKI infrastructure. If signature invalid, reject file before opening the payload."
Day2_Ext_Resources_Gemi,"Explain how to map the `LocalInstrument` code to the Clearing System in CHAPS.","IF `<LclInstrm><Cd>` = 'UK.CHAPS' OR `<LclInstrm><Prtry>` = 'CHAPS', route to the CHAPS Gateway. Ensure the message validates against the specific BoE ISO Schema (which is stricter than generic ISO)."
Day2_Ext_Resources_Gemi,"Design a 'Liquidity Reservation' mechanism for High Value Payments.","On ingestion of `pacs.008`: 1. Check `SettlementAmount`. 2. Reserve funds in the Debtor's account (Shadow Posting). 3. If balance insufficient, place in 'Retry Queue' for X hours. 4. If funds arrive, release to Clearing."
Day2_Ext_Resources_Gemi,"Write a logic to extract 'Remittance Location' (URL) for a trade finance portal.","Check `<RmtLctnMtd>` = 'URID'. Extract `<ElctrncAdr>`. Store URL in `Trade_Docs_Link` column. Display as a clickable link in the Operations UI for investigators to view the invoice."
Day2_Ext_Resources_Gemi,"How would you map the 'Service Level' code `G001` (SWIFT gpi) to internal SLAs?","IF `<SvcLvl><Cd>` = 'G001', Set `Target_Latency` = 30 mins (or applicable gpi rule). Flag `Priority` = 'High'. Enable `Tracker_Update` API calls to SWIFT to report status changes in real-time."
Day2_Ext_Resources_Gemi,"Design a check for 'Invalid Characters' in a payment instruction.","Scan all text fields against the 'SWIFT X Character Set' (or CBPR+ allowed set). Regex: `[^a-zA-Z0-9/-?:().,'+ ]`. IF match found -> Replace with space or '?' (Sanitization) OR Reject (Strict Mode)."
Day2_Ext_Resources_Gemi,"Explain how to handle a `pacs.008` where `InstgAgt` (Sender) is NOT the direct connection (i.e., sent via a Concentrator).","Check the `BAH` (Business Application Header) `<Fr>` (From) field to identify the Concentrator (Technical Sender). Check `pacs.008` `<InstgAgt>` for the Business Sender. Record both for lineage: 'Received via [Concentrator] on behalf of [Bank]'."
Day2_Ext_Resources_Gemi,"Construct a SQL query to compare 'Instructed Currency' vs 'Settlement Currency' to find FX volumes.","SELECT Instd_Ccy, Sttlm_Ccy, SUM(Instd_Amt) as Vol FROM Payment_Fact WHERE Instd_Ccy <> Sttlm_Ccy GROUP BY Instd_Ccy, Sttlm_Ccy. This highlights the bank's FX activity."
Day2_Ext_Resources_Gemi,"Design a logic to route payments based on `CategoryPurpose` (e.g., Salary vs Tax).","IF `<CtgyPurp>` = 'SALA' -> Route to 'Retail Clearing' (BACS/SEPA). IF `<CtgyPurp>` = 'TAX' -> Route to 'Government Gateway' (if applicable) or prioritize to ensure tax deadlines are met."
Day2_Ext_Resources_Gemi,"How would you implement 'Duplicate Detection' for a `pain.001` batch file?","Hash the entire file content (SHA-256). Check `File_Hash_Log`. IF Hash exists -> Reject 'Duplicate File'. IF Hash new, check `MsgId`. IF MsgId exists -> Reject 'Duplicate Message ID'. This prevents double processing."
Day2_Ext_Resources_Gemi,"Explain the mapping of `ChargeBearer` 'SHAR' to the billing engine.","Sender Bank bills Sender (Debtor) for outbound fees. Receiver Bank bills Receiver (Creditor) for inbound fees. No interbank fee transfer occurs in the principal amount."
Day2_Ext_Resources_Gemi,"Design a 'Payment Repair' screen layout for an Operations User.","Left Panel: Raw XML tree (collapsible). Right Panel: Form fields for Editable Data (e.g., Creditor Account). Highlight 'Error Fields' in Red based on validation report. Button: 'Resubmit' (Generates new XML) or 'Cancel'."
Day2_Ext_Resources_Gemi,"Write a logic to extract 'Postal Address' for Sanctions Screening from a structured block.","Concat: `StrtNm` + ' ' + `BldgNb` + ', ' + `TwnNm` + ', ' + `Ctry`. Pass this full string to the Screening Engine 'Address' field. Also pass `Ctry` separately to the 'Country' field for geographic blocking."
Day2_Ext_Resources_Gemi,"How would you identify 'Serial' payments in a `pacs.008` dataset?","Check for absence of `SettlementMethod` = 'COV'. Check if `IntrmyAgt` is present. If `IntrmyAgt` exists AND it is a `pacs.008`, it is a Serial payment (Bank A -> Intermediary -> Bank B)."
Day2_Ext_Resources_Gemi,"Design a 'Regulatory Archive' policy for ISO 20022 messages.","Policy: Retain 'Hot' data (SQL) for 2 years. Retain 'Cold' data (Raw XML in Glacier/Tape) for 7-10 years (as per local law). Ensure 'Search by UETR' retrieves the XML from Cold Storage within SLA (e.g., 24 hours)."
"Day2_ISO20022_Msgs_Perp","Design a table structure and mapping logic to extract AML relevant fields from pacs.008 into a fact_payment_aml table","Create fact_payment_aml with keys such as payment_id uetr debtor_party_id creditor_party_id corridor scheme amount currency settlement_date plus flags and risk attributes Map Dbtr and Cdtr names and identifiers to debtor and creditor fields map UltmtDbtr and UltmtCdtr to ultimate originator and beneficiary fields map DbtrAgt CdtrAgt and IntrmyAgt BICs to bank role columns derive corridor from party or agent countries and include Purp CategoryPurpose and ChrgBr Use one row per credit transfer and enforce references to KYC master data"
"Day2_ISO20022_Msgs_Perp","Explain how you would use pain.002 and pacs.002 messages to compute an end to end failure rate report per scheme","First join pain.002 status records to their original pain.001 transactions by message and instruction ids to get customer side acceptance vs rejection then join pacs.002 statuses to pacs.008 pacs.009 messages using original end to end ids or UETR Aggregate failures and successes by scheme and date from both views and combine these counts to compute end to end failure and return rates distinguishing between customer side rejections and interbank failures"
"Day2_ISO20022_Msgs_Perp","Describe a SQL or ETL approach to detect truncation between ISO 20022 addresses and legacy MT fields","Load full ISO postal address fields into an iso_address table and the mapped MT address lines into an mt_address table keyed by common transaction id Then run SQL comparisons such as where LEN iso_full_address greater than LEN mt_concat_address or where substrings differ around field limits Log all cases where city or street components are missing in MT but present in ISO and produce corridor and bank breakdowns to quantify truncation impact"
"Day2_ISO20022_Msgs_Perp","Show how you would derive a cross border indicator and corridor from ISO 20022 fields for regulatory reporting","Derive debtor_country from Dbtr postal address or debtor agent BIC country and creditor_country from Cdtr postal address or creditor agent BIC Set cross_border_flag to true when debtor_country differs from creditor_country Derive corridor as debtor_country hyphen creditor_country and store it in fact_payment tables for use in regulatory aggregates"
"Day2_ISO20022_Msgs_Perp","Design a canonical party dimension table fed from Debtor and Creditor elements in ISO 20022 messages","Create dim_party with party_id party_type debtor or creditor name address fields country lei kyc_risk attributes and links to KYC systems When parsing ISO messages extract Dbtr and Cdtr elements generate or match party_ids based on identifiers and then reference dim_party from fact_payment_aml by debtor_party_id and creditor_party_id so multiple payments reuse the same master party data"
"Day2_ISO20022_Msgs_Perp","Explain how you would map ISO 20022 purpose information into a reporting dimension for AML typologies","Create dim_purpose with purpose_code source standard description category and risk_relevance Map Purp codes the CategoryPurpose codes to this dimension using a controlled reference table When loading fact_payment_aml store purpose_code and category_purpose_code foreign keys so AML reports can quickly group flows by these codes and reason about suspicious uses versus declared purposes"
"Day2_ISO20022_Msgs_Perp","Describe an ETL pattern to populate a golden copy store for ISO 20022 messages while still supporting MT based systems","Implement an ingestion pipeline that receives ISO XML validates it against schemas and stores the raw payload plus a normalized representation in an iso_message_store keyed by uetr and internal ids Downstream systems that still require MT receive converted messages but compliance and reporting always source fields from iso_message_store ensuring no truncation or field loss even though MT exists in the processing chain"
"Day2_ISO20022_Msgs_Perp","How would you design a status history table that captures both customer and interbank statuses for each payment","Create fact_payment_status with payment_id uetr status_source customer or interbank message_type status_code reason_code timestamp and scheme Insert one row per status event parsing pain.002 for customer facing statuses and pacs.002 for interbank statuses Link them via payment_id or uetr so analysts can reconstruct full status timelines and compute STP and failure metrics"
"Day2_ISO20022_Msgs_Perp","Outline a SQL approach to find payments where Ultimate Debtor is located in a different country than the Debtor account","Join fact_payment_aml to dim_party for debtor and ultimate debtor get debtor_country from debtor address or BIC and ult_debtor_country from UltmtDbtr attributes Then filter where ult_debtor_country is not equal to debtor_country and settlement_date falls in the analysis window and project scheme corridor and amounts to feed AML scenarios on on behalf of payments"
"Day2_ISO20022_Msgs_Perp","Explain how you would normalize Remittance Information from ISO 20022 into a structure usable for analytics","Create a remittance_fact table with payment_id rmt_type free text versus structured reference invoice_number and optional code fields For structured RmtInf parse standard references and codes into separate columns For Ustrd free text potentially apply text mining or pattern detection to capture invoice like tokens This normalized structure lets reports aggregate by invoice or reference even when original XML had mixed representations"
"Day2_ISO20022_Msgs_Perp","Design a fact table to support sanctions effectiveness reporting using ISO 20022 data","Define fact_sanctions with payment_id uetr hit_flag list_name severity true_positive_flag detection_time disposition and processing times Join to underlying fact_payment_aml to bring corridor amount parties and banks Populate fact_sanctions from screening engine results that reference specific ISO fields and then compute metrics such as hit rates false positive rates and time to decision by corridor and scheme"
"Day2_ISO20022_Msgs_Perp","How would you use ISO 20022 messages to distinguish domestic versus cross border SEPA payments in a report","Use pacs.008 and pain.001 for SEPA flows Map debtor and creditor country codes from party addresses or bank identifiers For SEPA domestic flows debtor and creditor countries are the same and in the SEPA area Cross border SEPA flows have debtor and creditor countries within SEPA but different from each other Apply these rules in the ETL to derive domestic versus cross border flags per transaction"
"Day2_ISO20022_Msgs_Perp","Describe how to build an audit trail from regulatory report fields back to ISO 20022 elements","For each report field maintain a metadata table mapping report_column_name to source_message_type source_xpath transformation_rule and target_column_name In ETL populate report tables by referencing this mapping and store lineage ids alongside Each reported number then has an associated record showing which ISO message and field combination generated it enabling auditors to trace values end to end"
"Day2_ISO20022_Msgs_Perp","Explain how you would identify payments that used a high risk intermediary bank using ISO 20022 fields","From pacs.008 and pacs.009 parse IntrmyAgt elements and extract BICs Join these to a dim_bank table that includes a risk_rating attribute Then query fact_payment_aml joined to dim_bank on intermediary_bic where bank_risk_rating is high or jurisdiction is sanctioned and aggregate counts and values by scheme and corridor for AML monitoring"
"Day2_ISO20022_Msgs_Perp","Design a validation rule set for checking the completeness of ISO 20022 messages for sanctions screening","Define rules that check that Dbtr and Cdtr names are not null that at least one usable address or country is present for each party that DbtrAgt and CdtrAgt BICs exist and that UETR is present for cross border flows Implement these as SQL or rules in the ingestion layer and route messages failing checks to an exception queue while also counting them for data quality reporting"
"Day2_ISO20022_Msgs_Perp","How would you design an ETL job to enrich ISO 20022 payment facts with customer KYC risk ratings","After loading ISO message fields into fact_payment_aml join them to dim_customer or KYC systems using customer identifiers and LEIs Add risk_rating pep_flag and segment columns into the fact table or as foreign keys to dim_customer Ensure this enrichment table is refreshed regularly so regulatory and AML reports see up to date risk classifications for each payment"
"Day2_ISO20022_Msgs_Perp","Describe how you would map fields from pain.001 into staging tables before creating a canonical fact","Create stg_pain001_header for GrpHdr data stg_pain001_pmtinf for each PmtInf block and stg_pain001_tx for each CdtTrfTxInf Record message ids and instruction ids in all three tables Then define a transformation that joins these staging tables and maps relevant fields like debtor creditor ultimate parties purpose remittance and amounts into a single fact_payment_initiation table which then feeds downstream AML and reporting fact tables"
"Day2_ISO20022_Msgs_Perp","How can you use pacs.002 status codes to prioritize operational remediation for regulatory deadlines","Parse pacs.002 into fact_payment_status with fields such as original message id transaction reference status code and reason code Define severity rankings for specific statuses like reject due to sanctions or invalid beneficiary account Then build dashboards that sort open exceptions by severity and age ensuring that issues impacting regulatory submissions or customer outcomes are handled first"
"Day2_ISO20022_Msgs_Perp","Explain how you would construct a regulatory report that shows largest cross border corridors by value using ISO 20022 data","Use fact_payment_aml where cross_border_flag is true group by corridor defined from debtor and creditor country and sum amounts over the reporting period Order by total amount descending and select top corridors Include breakdowns by scheme and customer segment by joining to relevant dimensions so regulators see which corridors contribute most to overall flows"
"Day2_ISO20022_Msgs_Perp","Design a mapping from ISO 20022 fields into a canonical transaction identification model","Define a transaction_id table with columns such as internal_id uetr original_end_to_end_id instructing_agent_bic instructed_agent_bic and channel_reference Map each ISO message to this table extracting UETR from application headers and body and storing multiple related identifiers This gives a stable cross system key to join operational and reporting data around the same transaction"
"Day2_ISO20022_Msgs_Perp","How would you detect cases where Remittance Information might contain structured invoice references even though it is in free text","Analyse RmtInf Ustrd using pattern matching for typical invoice reference formats such as prefixes numbers or delimiters and record matches in a separate parsed_remittance table Optionally use regular expressions and dictionaries of known customer formats This detection enables partial structuring and improved linkage between payments and invoices for AML and trade reporting"
"Day2_ISO20022_Msgs_Perp","Describe a strategy for handling optional ISO 20022 fields that become mandatory for new regulatory requirements","Initially store such fields as nullable in fact tables and track their population rates Once a regulation requires them add data quality rules that flag missing values for in scope flows and work with upstream teams to ensure channels supply them Over time enforce not null constraints for those flows and update mapping documentation so future schema changes continue to honor the requirement"
"Day2_ISO20022_Msgs_Perp","Explain how you would design a warehouse model that supports both ISO 20022 initiated and legacy MT initiated payments","Create a unified fact_payment table with generic columns for parties amounts and corridors plus additional columns like iso_source_flag and mt_source_flag For ISO initiated flows map directly from ISO elements For MT initiated flows map MT fields as far as possible and store missing ISO equivalent fields as null while still keeping transaction identifiers consistent This lets regulatory reports run across both populations while making data gaps explicit"
"Day2_ISO20022_Msgs_Perp","How would you set up a data quality dashboard for ISO 20022 completeness by corridor","Compute metrics per corridor such as percentage of payments with structured addresses presence of ultimate parties population of Purp and CategoryPurpose and presence of UETR Store these metrics in a corridor_dq_fact table refreshed daily Then visualise trends over time highlighting corridors where completeness falls below thresholds which may impact AML and reporting quality"
"Day2_ISO20022_Msgs_Perp","Describe how ISO 20022 messages can be used to build a history of customer payment behavior for AML models","From fact_payment_aml group transactions per customer or party over time capturing velocity amounts corridors purposes and counterpart types Store aggregates like rolling volume average transaction size and most common routes in a behavior_fact table AML models can then use these historical features plus current transaction attributes to score new payments"
"Day2_ISO20022_Msgs_Perp","Explain how to design a process that reconciles ISO 20022 based reported volumes to GL based financial totals","Aggregate fact_payment_aml by date currency and product lines to produce payment_side_totals Join to gl_totals aggregated over the same keys Compute differences and store them in a reconciliation_fact table Investigate and classify differences such as timing mismatches internal transfers or out of scope items and design adjustments or exclusions so recurring regulatory reports reconcile within agreed tolerances"
"Day2_ISO20022_Msgs_Perp","How would you use ISO 20022 fields to identify payments related to particular high risk industries for regulatory reporting","Link UltmtDbtr and UltmtCdtr parties to KYC data that includes industry or NAICS codes Use Purp and CategoryPurpose to further refine type of payment Then query fact_payment_aml joined to dim_industry to group transaction volumes by industry with a focus on high risk categories like casinos or virtual asset service providers for regulatory disclosures"
"Day2_ISO20022_Msgs_Perp","Design a process to ensure UETR uniqueness and correct propagation in your internal systems","At ingestion validate UETR format as a UUID and enforce uniqueness per transaction using constraints or checks For internally generated flows create UETR once and propagate it consistently through all messages and internal events Always use UETR as a join key when loading fact tables and do not allow downstream systems to override it so traceability is preserved for compliance and investigations"
"Day2_ISO20022_Msgs_Perp","Explain how you would detect payments that appear to be split just below a reporting threshold using ISO 20022 data","Within fact_payment_aml group payments by debtor party corridor and short time windows such as same day and sort by amount Then use window functions to identify clusters of amounts just below a regulator threshold Summarize suspicious clusters where multiple payments within the window sum to significantly above the threshold suggesting structuring behavior"
"Day2_ISO20022_Msgs_Perp","How would you design a multi jurisdiction reporting layer from a single ISO 20022 canonical model","Create a core fact_payment_aml and dimension set representing global canonical attributes Then for each jurisdiction design views or derived tables that apply local business rules thresholds and filters over the canonical data without duplicating ingestion mappings This lets each jurisdictional report conform to local regulations while still using the same underlying ISO sourced fields"
"Day2_ISO20022_Msgs_Perp","Describe how to use ISO 20022 data to monitor compliance with scheme specific rules such as SEPA reachability or cut off times","From pain.001 and pacs.008 track timestamps requested execution dates and scheme codes Compare actual processing and settlement times against scheme cut offs and rule books Store breaches or near misses in a scheme_compliance_fact with attributes like time_overrun and reason which feeds regular scheme performance and compliance reports"
"Day2_ISO20022_Msgs_Perp","Explain how you would implement lineage that distinguishes between fields populated from original ISO messages and those derived or enriched later","For each column in fact tables add lineage metadata columns that record lineage_source such as ISO or derived and lineage_rule identifiers Maintain a lineage_rules table describing transformations For example debtor_country may be marked as derived from Dbtr address while raw city and street come directly from ISO This allows regulators to see which attributes are original and which are calculated"
"Day2_ISO20022_Msgs_Perp","How would you design ETL to support both historical backfill and incremental loads for ISO 20022 based reporting","Partition iso_message_store and fact tables by settlement_date Use an initial backfill job that loads historical ISO files into partitions and builds facts in large batches For incremental loads implement streaming or daily batch processing keyed by message creation time or settlement_date Ensure idempotent logic keyed by uetr and message ids so reprocessing does not create duplicates and use change tracking tables to manage retry and rerun scenarios"
"Day2_ISO20022_Msgs_Perp","Describe how you would incorporate camt.053 data into an ISO 20022 based reporting architecture","Use camt.053 as an additional source of booked entries by loading statement lines into a fact_statement_entry table keyed to accounts and dates Then reconcile these entries to fact_payment_aml on amounts accounts and dates to validate completeness For some simple reports you may use camt.053 directly but for rich AML and corridor analytics always join back to ISO based payment facts"
"Day2_ISO20022_Msgs_Perp","Explain how you would design SQL queries to produce a daily regulatory report of large cross border payments above a specified threshold","In fact_payment_aml filter where cross_border_flag is true and amount is greater than or equal to the regulatory threshold and settlement_date equals the report date Join to dim_party for debtor and creditor details and dim_corridor for descriptions Then SELECT corridor debtor_country creditor_country COUNT and SUM amount with GROUP BY corridor and corridor related fields and export the result as the report dataset"
"Day2_ISO20022_Msgs_Perp","How would you structure a mapping specification document for ISO 20022 fields to regulatory report columns","For each report column specify the ISO message type the exact XPath to the source element any conditions or filters transformation logic target data type and target column name Include examples and describe how optional fields and missing data are handled Maintain versioning so when ISO schemas or regulatory definitions change the mapping document and ETL can be updated consistently"
"Day2_ISO20022_DeepDive_Perp","How would you explain to a VP the difference between what ISO 20022 standardises and what schemes or regulators still define","Explain that ISO 20022 standardises the common language and structure of payment data including fields for parties amounts and references but it does not define scheme specific business rules SLAs pricing or legal reporting obligations Those are still defined by schemes and regulators ISO simply ensures that when a scheme asks for a field like debtor country or purpose every participant has a standardised way to supply it"
"Day2_ISO20022_DeepDive_Perp","Design a short pitch showing why re wrapping MT messages in ISO XML without populating richer fields is a wasted opportunity","State that re wrapping only changes syntax not semantics so screening and reporting still rely on truncated free text Instead propose using ISO to populate structured names addresses LEIs purpose and ultimate parties and then show examples where this unlocks better detection of high risk corridors and faster root cause analysis versus MT based data"
"Day2_ISO20022_DeepDive_Perp","Describe how you would choose the top ten ISO 20022 fields that must always be present for cross border AML reporting","Start from AML and regulatory use cases and choose UETR debtor and creditor identifiers and countries structured addresses ultimate parties Purp and CategoryPurpose scheme and amounts Define them as critical in usage guidelines and mapping specs enforce not null or conditional rules in ETL and monitor completeness dashboards so gaps are visible and fixed upstream"
"Day2_ISO20022_DeepDive_Perp","Using CBPR plus guidelines how would you design validation rules at ingestion for pacs.008 messages","Read CBPR plus specs for pacs.008 identify all M and R mandatory required fields for your flows and then implement schema validation plus business rules such as ensuring UETR presence structured addresses where expected and valid code lists For violations route messages to repair queues and track metrics so your bank adheres to network and regulatory expectations"
"Day2_ISO20022_DeepDive_Perp","Explain how you would map CBPR plus pacs.008 elements into a canonical warehouse model without losing compliance relevant details","Create a fact_payment table keyed by uetr and transaction id and include normalized columns for debtor creditor ultimate parties party countries structured address components agent BICs Purp ChrgBr amounts and dates Ensure one row per transaction and reference dimension tables for parties banks and purposes so analytics and regulatory reports can access all CBPR plus mandated data"
"Day2_ISO20022_DeepDive_Perp","Describe the main modelling differences between a SEPA only payments mart and a global ISO 20022 mart","A SEPA only mart can assume euro currency SEPA schemes specific character sets and SHA charges so its schema can simplify certain dimensions While a global mart must support multiple currencies wider code lists and varied charge options The global model needs additional attributes for non SEPA schemes cut offs and FX while SEPA mart can be a constrained view on top of the global canonical model"
"Day2_ISO20022_DeepDive_Perp","How would you design SLA monitoring tables to prove SCT Inst compliance to a regulator","Persist timestamps from customer initiation pain.001 through interbank processing pacs.008 and final acceptance pacs.002 For each payment compute elapsed times at each stage and store in a fact_sla table with scheme inst flag and corridor Then aggregate to show percentage of SCT Inst transactions meeting specified thresholds and keep distributions for audit evidence"
"Day2_ISO20022_DeepDive_Perp","Design three data quality KPIs on ISO 20022 data that you would present to an AML committee","Propose percentage of cross border payments with structured addresses for both parties percentage with UltmtDbtr or UltmtCdtr populated where relevant and percentage with valid purpose codes from controlled lists Track these daily across corridors and schemes so the committee sees whether upstream channels are supplying data that enables robust AML screening and analytics"
"Day2_ISO20022_DeepDive_Perp","How would you architect a reporting solution that meets Wolfsberg expectations for transparency in correspondent banking using ISO 20022","Build a fact_corr_payment table that includes debtor and creditor banks plus all intermediary agents from pacs.008 and pacs.009 COV along with corridor amounts and purposes Join to dim_bank with risk ratings and jurisdictions Then create dashboards showing volumes and high risk corridors per correspondent including underlying customer segments to evidence transparency and control to auditors"
"Day2_ISO20022_DeepDive_Perp","Explain how you would use pacs.009 COV to build a link between FI cover payments and underlying customer transactions","Parse pacs.009 COV fields that reference underlying customer transactions such as original end to end ids or contained remittance structures Link those ids to fact_payment_aml rows sourced from pain.001 and pacs.008 Store this linkage in a fact_cover_link table so investigators and reports can move from FI leg to customer leg in one join"
"Day2_ISO20022_DeepDive_Perp","Design ISO based metrics for CPMI pillars cost speed transparency and access","For cost derive average and distribution of fees and charge patterns per corridor from ChrgBr and explicit fee fields For speed compute median and tail latency between initiation and final status per scheme For transparency measure share of flows with UETR and full party data and for access count active corridors counterparties and schemes by geography using BIC coverage"
"Day2_ISO20022_DeepDive_Perp","How would you select ten ISO driven fields and metrics to show a central bank examiner that your cross border statistics are robust","Choose settlement amount and currency debtor and creditor residency and sector UETR scheme codes corridor derived from party countries and counts of transactions enriched with KYC show tables and charts of flows by corridor and sector plus reconciliation to GL and RTGS totals to demonstrate completeness and consistency"
"Day2_ISO20022_DeepDive_Perp","Explain how you would combine ISO 20022 fields and KYC data to build ECB style payment statistics","From ISO messages extract transaction amounts currencies and basic party countries Join parties to KYC datasets providing residency and sector codes Then aggregate transactions by instrument type scheme currency residency pair and sector pair to produce the tables required by ECB templates and maintain clear lineage between report cells and source fields"
"Day2_ISO20022_DeepDive_Perp","Design an ISO MT coexistence architecture that avoids relying on truncated MT data for regulatory reporting","Ingest all ISO 20022 messages into a golden copy store and map them into canonical fact and dimension tables Use translation services to generate MT for counterparties that still require it but ensure regulatory and AML reporting always queries the ISO based warehouse not MT For MT originated flows ingest MT and map into the same canonical model but mark data gaps explicitly"
"Day2_ISO20022_DeepDive_Perp","How would you document and enforce a do not truncate list for critical ISO fields during MT conversion","Create a list of fields such as ultimate parties structured addresses UETR and Purp considered critical for compliance Document exact mapping rules and acceptable truncations For each conversion path implement automated tests that compare source and target to detect loss for these fields and fail builds or jobs when truncation thresholds are violated and report issues to governance forums"
"Day2_ISO20022_DeepDive_Perp","Describe how RTGS ISO migration changes the data you can use in intraday liquidity risk reporting","With RTGS on ISO 20022 you receive structured messages for each high value payment including richer party and channel attributes You can now build intraday liquidity dashboards that group flows by counterparties corridors and business lines not just account level balances using real time updates from RTGS messages and reconciling these with internal ledgers"
"Day2_ISO20022_DeepDive_Perp","How would you reconcile RTGS ISO 20022 messages with internal payment engine records and general ledger totals","Assign each RTGS payment a stable internal transaction id and store RTGS message ids and UETR in a reconciliation table Link internal payments to RTGS entries on amount currency account and timing and ensure GL postings reference the same internal id Then build reconciliation queries and reports that compare aggregates across the three sources and highlight mismatches for investigation"
"Day2_ISO20022_DeepDive_Perp","Summarise the common obstacles banks mention at Sibos when using ISO 20022 for AML and how you would mitigate them","Banks often cite inconsistent field population old channels that cannot supply structured data and lack of ownership for data quality To mitigate define critical fields and KPIs invest in channel upgrades that capture full addresses and identifiers implement golden copy storage and set up governance forums where business owns and monitors data quality metrics"
"Day2_ISO20022_DeepDive_Perp","Explain how you would use MyStandards as the single source of truth for ISO field usage in your programme","Synchronise your internal mapping specifications with MyStandards profiles so every field in your schema traces to a MyStandards definition Use exported spreadsheets or APIs to drive validation rules and code generation and require any change request for ISO usage to be reflected first in MyStandards then cascaded into ETL and database designs"
"Day2_ISO20022_DeepDive_Perp","Design a small relational model centred on UETR to support cross border investigations","Create a uetr_fact table with one row per UETR containing global attributes such as first seen last status and overall amount Link it to message tables for pain.001 pacs.008 pacs.009 pacs.002 and camt messages via foreign keys keyed on both UETR and message ids This allows investigators to query uetr_fact by corridor or customer and then quickly pull all related messages and statuses"
"Day2_ISO20022_DeepDive_Perp","How would you answer in an interview if asked how to use UETR ISO 20022 and your warehouse to investigate a sanctions case across three intermediaries","Explain that you would take the UETR of the suspicious payment query your warehouse for all messages carrying that UETR including pacs.008 pacs.009 and pacs.002 at each leg and join them to sanctions hit tables and party data This gives a full hop by hop view showing where the hit occurred which banks were involved and what statuses and actions were taken"
"Day2_ISO20022_DeepDive_Perp","Describe how you would build a Wolfsberg aligned dashboard in your BI tool using ISO fields","From fact_corr_payment aggregated from pacs.008 and pacs.009 build visuals showing volumes and values per correspondent bank corridor and risk rating Add filters for high risk jurisdictions and show counts of payments with missing ultimate party data Use these to evidence to auditors and Wolfsberg assessments that you monitor correspondent flows systematically"
"Day2_ISO20022_DeepDive_Perp","How would you pitch an ISO 20022 driven AML data quality improvement programme to a VP in a few key points","Highlight that richer ISO data can directly reduce false positives and investigation time demonstrate gaps in current population rates for critical fields propose incremental channel improvements and DQ dashboards and stress that regulators FATF and Wolfsberg increasingly expect banks to exploit structured data not just run old controls on new syntax"
"Day2_ISO20022_DeepDive_Perp","Design a mapping and ETL flow that lets you respond quickly when a new ECB or central bank reporting template appears","Base the ETL on a canonical fact model sourcing ISO fields generically without hard coding any single report Then implement a reporting layer with SQL views or semantic models that map canonical columns into specific templates When a new template arrives you only need to design a new mapping view not change ingestion allowing fast response with high confidence in data consistency"
"Day2_ISO20022_DeepDive_Perp","Explain how you would create a multi jurisdiction reporting layer from one ISO 20022 canonical model","Construct jurisdiction specific views or marts that apply local filters thresholds and derived classifications over a shared canonical ISO based fact and dimension model For example create separate schemas for EU UK and US reports but have them all select and transform from the same core fact_payment and dim_party tables ensuring consistency while satisfying local templates"
"Day2_ISO20022_DeepDive_Perp","Describe a lineage approach that lets auditors see exactly which ISO field each report column came from","Maintain a mapping table with columns for report_name report_column source_message_type xpath transformation and target_table_column Use this mapping to drive ETL so each transformation step logs which mapping rule it executed For auditors provide a view showing for any report column the exact ISO source field transformations applied and any intermediate columns used"
"Day2_ISO20022_DeepDive_Perp","How would you design a process to periodically test whether ISO to MT conversions are losing critical AML information","Schedule test runs where synthetic and sampled real ISO messages are converted to MT then back to ISO like structures Compare key fields especially parties addresses UETR and purposes between original and round tripped versions Log differences and generate metrics and investigations for any systematic truncation in sensitive fields and involve compliance in remediation"
"Day2_ISO20022_DeepDive_Perp","Explain how you would support historical backfill and incremental loads for an ISO based regulatory warehouse","Implement date and message id based partitioning of iso_message_store and fact tables Use dedicated backfill jobs to load historical ISO files partition by partition while incremental jobs process new messages daily or in near real time Ensure idempotency by using uetr and message ids as keys and track load windows so reports can consistently reference complete partitions for each reporting date"
"Day2_ISO20022_DeepDive_Perp","Describe how camt.053 data complements ISO payment messages in a regulatory architecture","Use camt.053 to confirm booked entries and closing balances for accounts by loading statement lines into a fact_statement table Link these entries to fact_payment transactions using account id date and amount to validate completeness For some simple cash balance disclosures camt.053 may be primary while for AML and corridor analytics you rely on ISO payment facts and use camt.053 only for reconciliation"
"Day2_ISO20022_DeepDive_Perp","Design a SQL style query skeleton to produce a daily regulatory report of large cross border payments above a threshold from ISO based facts","Assuming a fact_payment_aml table include WHERE cross_border_flag equals true and amount greater than or equal to threshold and settlement_date equals a parameter Then GROUP BY corridor debtor_country creditor_country and perhaps scheme SELECT corridor debtor_country creditor_country COUNT star and SUM amount and output the result set as the report dataset with joins to dim_party for extra attributes if needed"
"Day2_ISO20022_DeepDive_Perp","How would you build a behaviour history for customers using ISO 20022 to feed AML models","Aggregate fact_payment_aml per customer over rolling windows extracting features like number of payments average and maximum amounts most common corridors purposes and counterparties Store these in a behaviour_fact table keyed by customer and period Models then use both current transaction attributes from ISO fields and historical behaviour features to score risk"
"Day2_ISO20022_DeepDive_Perp","Explain how you would use ISO based reconciliations to increase regulator confidence in your reporting","Show reconciliations where sums of amounts by date currency and product in fact_payment_aml match GL totals and RTGS figures within tight tolerances Document and log reconciliation runs with exceptions analysed and resolved This demonstrates that the same ISO sourced data powers both internal books and regulatory outputs reducing suspicion of inconsistent reporting"
"Day2_ISO20022_DeepDive_Perp","Describe a concrete SQL pattern to flag payments that appear to be split just below a reporting threshold using ISO fields","Group fact_payment_aml by debtor_party_id corridor and settlement_date and order transactions by amount Use window functions to calculate running sums of amounts where each individual payment is slightly below the threshold but groups of them exceed it Flag clusters where several payments within a short time window collectively exceed the threshold as potential structuring for AML review"
"Day2_ISO20022_DeepDive_Perp","How would you structure MyStandards driven mapping specs so that devs testers and BAs stay aligned during ISO projects","Use MyStandards as the master for message usage and export its field list and rules into a controlled mapping document that lists for each ISO field the target table and column transformations and data quality rules Require all teams to sign off on this document and update it only through change control when MyStandards profiles change ensuring that everyone implements the same agreed behaviour"
"Day2_ISO20022_DeepDive_Perp","Summarise how you would present an ISO 20022 data platform to a central bank examiner in three main messages","First emphasise that the bank stores full golden copy ISO payloads with stable identifiers Second explain that a canonical fact and dimension model enriched with KYC supports all regulatory templates and cross checks to GL and RTGS and third demonstrate dashboards and reconciliation reports showing data quality trends corridors and scheme metrics that align with CPMI and ECB expectations"
"Day1_DataContracts_Perp","Describe how you would define a data contract between a CHAPS payment engine and a Kafka topic carrying payment_initiated events","Start by listing all fields needed for downstream AML and reporting such as uetr scheme amount currency debtor and creditor ids countries and timestamps then define an Avro or Protobuf schema with strict types and descriptions Register it in the schema registry and agree versioning rules so producers cannot remove or change fields without coordination and document DQ expectations like non null for cross border attributes"
"Day1_DataContracts_Perp","Explain how you would translate a BOE cross border volume template into a column level contract for fact_payment_reg","Identify BOE dimensions such as residency of payer and payee currency instrument and value date then design columns like debtor_country_code creditor_country_code currency instrument_type and booking_date in fact_payment_reg Specify types allowed codes and null rules and document which upstream ISO fields and KYC attributes feed each column so any report can rely on them consistently"
"Day1_DataContracts_Perp","How would you design OLTP tables for a high volume FPS engine to maximise integrity and support later reporting","Use a normalized schema with separate payment ledger_entry account and customer tables enforce primary and foreign keys and use appropriate isolation levels such as READ COMMITTED or better for debits and credits Store business keys like uetr and scheme codes and avoid denormalisation so every payment is uniquely and consistently linked to accounts and customers from which OLAP will later derive attributes"
"Day1_DataContracts_Perp","Design a basic star schema for regulatory reporting of CHAPS and SEPA payments","Create fact_payment_reg with a surrogate key plus measures like amount and ids linking to dim_party dim_account dim_scheme dim_country and dim_time Include corridor_code and flags for cross border Then design dimensions with descriptive attributes such as scheme name party sector and country names so reports can group flows by corridor scheme sector and date without returning to OLTP tables"
"Day1_DataContracts_Perp","Explain when you would duplicate debtor_country into fact_payment_reg instead of always joining dim_party","Duplicate debtor_country when it is a very common grouping and filtering attribute in many regulatory reports so storing it in the fact avoids repeated joins and makes partition pruning easier At load time derive it from dim_party or ISO fields and keep it consistent with SCD logic but treat dim_party as the master source for more detailed attributes"
"Day1_DataContracts_Perp","How would you handle slowly changing customer sectors and risk ratings in a regulatory fact model","Implement SCD type 2 in dim_customer with effective_from and effective_to and a surrogate customer_sk Then in fact_payment_reg store customer_sk so each payment links to the correct version of attributes at transaction time For very commonly used attributes like risk_bucket optionally denormalise them into the fact at load while still validating against dim_customer"
"Day1_DataContracts_Perp","Describe how you would design staging tables for ISO 20022 pain.001 to feed canonical facts","Create stg_pain001_header for GrpHdr data stg_pain001_pmtinf for batch level attributes and stg_pain001_tx for each CdtTrfTxInf transaction capturing debtor creditor ultimate parties purposes and amounts Include message and instruction ids and timestamps and apply minimal validation Then build transformation processes that join these staging tables and map clean fields into fact_payment_initiation and dimension tables"
"Day1_DataContracts_Perp","How would you incorporate pacs.008 and pacs.009 into your canonical payment model to support both customer and interbank views","Use fact_payment_reg to store end to end customer flows with uetr and customer level attributes and maintain a fact_interbank_leg or a leg_type column to capture FI to FI movements from pacs.008 and pacs.009 Include agent BICs and RTGS flags so regulators can see both customer and interbank dimensions while lineage tables record whether each row came from pain.001 pacs.008 or pacs.009"
"Day1_DataContracts_Perp","Explain how you would design a data contract that covers coexistence of ISO and MT based flows into the same warehouse","Define canonical facts and dimensions that express business attributes independently of message standard such as uetr parties corridors and amounts For ISO flows map directly using rich fields For MT flows map MT tags as far as they go and mark fields that cannot be populated Document which canonical columns are mandatory for each source type and ensure the contract distinguishes between full ISO and partial MT populations"
"Day1_DataContracts_Perp","Describe how you would enforce data quality SLAs on cross border attributes in fact_payment_reg","Define metrics such as percentage of cross border payments with populated debtor_country creditor_country corridor and purpose_code Implement ETL checks that compute these metrics per day and per source system and store them in a DQ fact table Alert when thresholds are breached and treat these breaches as contract violations requiring remediation with upstream teams"
"Day1_DataContracts_Perp","How would you version and evolve an Avro schema for payment events without breaking existing consumers","Use schema registry and design schemas so most changes are additive for example adding optional fields with defaults Avoid removing or renaming fields or changing types and when a breaking change is unavoidable register a new schema version under a different subject like payment_initiated_v2 and gradually migrate consumers to the new topic or field set while running both versions in parallel"
"Day1_DataContracts_Perp","Design a process to change the canonical definition of corridor without breaking historical reports","Create a dim_corridor table and mapping logic that translates debtor and creditor countries into corridor codes Version corridor mapping rules and store a corridor_version_id with each fact row When the definition changes create a new version in dim_corridor and use it for new data while leaving historical rows with old version ids so reports can specify whether to use historical or restated corridor definitions"
"Day1_DataContracts_Perp","Explain how you would build lineage metadata linking ISO 20022 fields to canonical warehouse columns","Maintain a mapping table where each row contains source_system message_type source_xpath target_table target_column and transformation_description and give each mapping a version id Use ETL that logs which mapping version populated each batch into fact tables and expose lineage views that allow an auditor to query a report column and see which ISO fields and rules produced it"
"Day1_DataContracts_Perp","Describe how you would design fact_sanctions_hit and its relationship to fact_payment_reg","Define fact_sanctions_hit with a surrogate key payment_id hit_timestamp list_name severity true_positive_flag and disposition Store one row per hit or screening event and include foreign keys to fact_payment_reg Then reports can analyse hit rates and severities by scheme corridor and customer attributes by joining facts while keeping sanctions logic separate from the main payment fact"
"Day1_DataContracts_Perp","How would you design partitioning for fact_payment_reg to support both daily regulatory runs and ad hoc historic analysis","Partition by booking_date or settlement_date at least at daily granularity and ensure most regulatory queries filter on that date column Add clustered indexes or clustering keys that include scheme and currency for common groupings so daily reports use a few partitions and historic analysis can still prune by broader date ranges while relying on other indexes for selective queries"
"Day1_DataContracts_Perp","Explain how you would design OLTP and OLAP models so that 2200 reports can be delivered without touching OLTP","Keep OLTP models minimal and normalized for correctness and push all analytics into an OLAP warehouse fed via CDC or ETL Design a rich dimensional model with fact_payment_reg and related facts and dimensions that expose all attributes required by BOE EBA AML and scheme reports Provide a semantic layer on top so report builders and extract jobs read from OLAP only never from OLTP tables"
"Day1_DataContracts_Perp","Describe how you would design a canonical event model for payment lifecycle events feeding the warehouse","Define events such as PaymentInitiated PaymentScreened PaymentSettled PaymentReturned and map each to Avro schemas with consistent keys like uetr and internal_payment_id Include a core set of attributes repeated across events and event specific details Then design ETL that merges events into fact_payment_reg and fact_payment_status histories using these keys"
"Day1_DataContracts_Perp","How would you support both daily and monthly regulatory runs from the same warehouse without recalculating everything twice","Build base daily level facts such as fact_payment_reg and fact_scheme_daily then implement monthly aggregate tables or views that roll daily data up by month and other dimensions Schedule daily jobs to populate daily facts and separate jobs to aggregate into monthly structures ensuring both use the same canonical facts and lineage"
"Day1_DataContracts_Perp","Explain how you would create data contracts between the canonical warehouse and BI tools such as Power BI or Tableau","Define a semantic model or set of certified views that expose stable business friendly names measures and dimensions and document these as the only supported interfaces for reports Specify refresh frequencies and allowed filter dimensions and enforce role based access so BI tools do not connect directly to raw tables or invent their own calculations"
"Day1_DataContracts_Perp","Describe how you would introduce a new field regulator_reportable_flag into fact_payment_reg without breaking existing queries","Add the new column as nullable with a default value and backfill it gradually for historical data Do not change existing views or queries until the backfill is complete Then update documentation and encourage new reports to use the field while ensuring any old queries still behave as before because they ignore or simply see null values"
"Day1_DataContracts_Perp","How would you design a canonical model to support CHAPS FPS and SEPA reporting together","Create fact_payment_reg with fields such as scheme_code instrument_type amount currency dates parties corridors and flags for high value or instant Add dim_scheme describing each scheme and dim_product for product grouping Ensure the model can express both RTGS style CHAPS and retail FPS SEPA flows by treating scheme specific rules as dimension attributes not separate fact structures"
"Day1_DataContracts_Perp","Explain how you would support both ISO 20022 and MT reporting fields in one canonical fact without confusion","Include canonical columns that represent business attributes like debtor_name creditor_country and uetr and add technical origin fields such as iso_source_flag and mt_source_flag plus raw_source_reference For ISO flows populate all rich fields while for MT flows populate only what is available and set flags so reports can filter or highlight where data quality is lower"
"Day1_DataContracts_Perp","Describe how you would implement contract tests between a payment engine team and the data warehouse team","Define sample payloads and schema expectations for key events then implement automated tests that run in CI where the warehouse test harness validates incoming messages against agreed schema and business rules Any incompatible schema change in the engine pipeline fails the build until both sides update the contract and implementation"
"Day1_DataContracts_Perp","How would you phase out an old version of fact_payment_reg while ensuring no report is broken","Introduce a new version or extended schema in parallel and populate both for a transition period Update views and semantic models to point to the new table and update key regulatory reports After verifying results and updating dependencies retire the old table by redirecting any remaining consumers and documenting the decommissioning timeline"
"Day1_DataContracts_Perp","Explain how you would capture and use metadata about report runs in a governance friendly way","Create a reg_report_run fact with report_name run_id as_of_date snapshot_id row_count status start_time end_time and version identifiers for schemas and mapping rules Populate it whenever a report is generated so auditors can see exactly which data slice and contract versions were used for each submission"
"Day1_DataContracts_Perp","Describe how you would align ISO 20022 ingestion contracts with BOE and EBA requirements from day one","Start from BOE and EBA templates to list all required and likely future attributes then design ISO staging schemas and canonical facts that capture those fields with appropriate granularity Ensure ingestion contracts explicitly include fields such as residency sector instrument and corridor so later regulatory work becomes configuring views not asking for new source data"
"Day1_DataContracts_Perp","How would you ensure that customer and account identifiers are consistent across OLTP and OLAP models","Define a master identifier strategy where customer_id and account_id are generated and managed centrally and are carried in both OLTP tables and OLAP dimensions Use ETL that maps OLTP ids into warehouse dimensions without rewriting keys and enforce referential integrity checks so facts never reference unknown or re keyed identities"
"Day1_DataContracts_Perp","Explain how you would detect and handle schema drift in ISO 20022 messages coming from multiple gateways","Implement strict XSD validation and mapping logic that logs any unexpected or new elements in ISO messages When new fields appear evaluate them in governance forums and update mapping contracts and schemas deliberately rather than letting ad hoc parsing or ignoring drift creep into the system"
"Day1_DataContracts_Perp","Describe a governance process for approving changes to canonical payment schemas","Set up a data design authority where proposals for schema changes include impact analysis on contracts and reports Review proposals with stakeholders from payments data engineering and reporting approve or reject them and record decisions and timelines Update documentation mapping specs and tests and schedule coordinated releases with blue green or dual write where needed"
"Day1_DataContracts_Perp","How would you design a single payments mart that can support both management MI and formal regulatory reports","Base the mart on canonical facts and dimensions with all attributes needed for regulatory templates and MI then expose separate layers a curated MI layer with friendly measures and a regulated layer with strict lineage and constraints Use the same underlying tables so MI and regulatory numbers reconcile while allowing different consumer views"
"Day1_DataContracts_Perp","Explain how you would treat time zones in contracts for global cross border reporting","Standardise on a canonical time zone such as UTC for settlement and booking times in facts and document this in contracts while keeping local times in separate columns when needed Ensure all date based partitions use canonical times and that report definitions clearly state whether they use local or canonical dates to avoid ambiguity"
"Day1_DataContracts_Perp","Describe how you would build a test data set to validate a new fact_payment_reg design before go live","Synthetically generate or sample real payments covering multiple schemes corridors currencies and risk scenarios with known expected aggregations and compliance metrics Load them into a sandbox implementation of fact_payment_reg and compare report results to expected values and to legacy systems ensuring the new design supports all required calculations and edge cases"
"Day1_DataContracts_Perp","How would you support scenario analysis such as stress testing using the same canonical facts used for regulatory reporting","Design stress or scenario tables that reference fact_payment_reg keys and store stressed measures such as shocked_amount or alternative risk weights Compute these from base facts using repeatable rules leaving original facts untouched Regulators then see both base and stressed numbers traced back to the same underlying transactions"
"Day1_DataContracts_Perp","Explain how you would design the contract between payments data and AML models so model features are stable over time","Document a feature contract listing each feature name its source canonical column type and transformation logic and store this alongside schemas Ensure any field used in features is treated as non negotiable in contracts and evolve features with versioning so new models can add features without changing meanings of existing ones preserving back testing and comparability"
"Day1_DataContracts_Perp","Describe how you would ensure that all 2200 reports can be regenerated consistently for a past reporting date","Base every report on partitioned canonical facts keyed by booking_date or as_of_date and ensure SCD and denormalised attributes capture the state as of that date Store report run metadata with snapshot ids and forbid using volatile current time based calculations so re running for the same as_of_date with the same snapshot yields identical numbers"
"Day1_DataContracts_Perp","Summarise how you would communicate the value of strong data contracts and canonical modelling to senior stakeholders","Explain that strong contracts and canonical models reduce the cost and risk of change allow fast delivery of new reports like BOE or EBA templates ensure consistency between MI and regulatory submissions and provide clear lineage for regulators They also protect real time payment engines from reporting workloads and give a scalable foundation for analytics and AML"
"Day1_DataContracts_Ext_Perp","How would you use insights from SWIFT ISO 20022 for dummies to explain ISO 20022 to a non technical VP in your interview","Frame ISO 20022 as a common language that standardises payment data structures across markets allowing richer information about parties purpose and charges to flow consistently Emphasise that this reduces point to point mappings improves accuracy of AML and regulatory reports and supports future schemes without constantly rebuilding integrations"
"Day1_DataContracts_Ext_Perp","After studying ISO 20022 for dummies how would you argue against a pure MT to ISO to MT transliteration strategy","Explain that transliteration only changes wrappers and does not leverage structured fields like ultimate parties structured addresses and purpose codes Without populating these fields the bank keeps MT era limitations and misses improvements in screening analytics and regulator expectations so the migration cost is paid without obtaining real benefits"
"Day1_DataContracts_Ext_Perp","Using CBPR plus guidance how would you define a minimal mandatory field set for cross border reporting in your canonical model","Start with CBPR plus mandated elements such as uetr debtor and creditor party details structured addresses country codes debtor_agent and creditor_agent BICs amounts currencies and charge information Document these as non negotiable columns in fact_payment_reg and ensure ingestion and validation enforce their presence for CBPR plus eligible flows"
"Day1_DataContracts_Ext_Perp","How would CBPR plus usage rules influence the design of your ISO staging tables","Design stg_pacs008 and stg_pacs009 schemas that reflect all CBPR plus required fields with correct types and code sets Include constraints and validations that reject or flag messages that do not satisfy usage rules such as missing uetr or unstructured only addresses and keep these staging tables versioned alongside CBPR plus updates"
"Day1_DataContracts_Ext_Perp","After reading EPC SEPA CT guidelines how would you design a SEPA only reporting mart that reuses your global model","Create a SEPA specific view or mart on top of fact_payment_reg filtered by scheme_code equals SEPA_CT and currency equals EUR and enforce SEPA character set and charge rules via validations Reuse global dimensions like dim_party and dim_country but add SEPA specific attributes such as SEPA category and reason codes making SEPA reports light configurations on the global model"
"Day1_DataContracts_Ext_Perp","How would you prove SCT Inst SLA compliance using ISO 20022 timestamps in a warehouse","Capture timestamps from customer initiation pain.001 receipt at debtor bank pacs.008 send and final pacs.002 acceptance Then compute an elapsed_time_ms column in fact_payment_reg or a dedicated sla_fact and aggregate per day scheme and corridor to show distributions for SCT Inst flows demonstrating that almost all meet the required threshold and storing evidence for audits"
"Day1_DataContracts_Ext_Perp","Using FATF digital transformation guidance how would you prioritise ISO fields for AML analytics","List FATF emphasised attributes such as full names structured addresses residency identifiers and transaction patterns and map them to ISO fields like Dbtr Cdtr PstlAdr LEI Purp and amounts Make these fields first class citizens in fact_payment_aml and ensure DQ checks monitor their completeness since they directly impact model performance and detection capabilities"
"Day1_DataContracts_Ext_Perp","How would you set up data quality KPIs inspired by FATF to monitor ISO 20022 data readiness for AML","Define KPIs such as percentage of cross border flows with structured addresses percentage with populated ultimate parties and percentage with valid purpose codes Implement ETL jobs that compute these per corridor and scheme store them in a dq_fact table and review them regularly in AML governance forums using thresholds that trigger remediation with channel owners"
"Day1_DataContracts_Ext_Perp","Given Wolfsberg expectations what additional attributes would you store in your correspondent banking fact tables","Include correspondent and respondent BICs as separate dimension keys underlying customer types and sectors from KYC high risk corridor flags information about intermediary agents and reference to cover versus underlying messages so that correspondents can be analysed by who their customers are and where flows travel not just by volumes"
"Day1_DataContracts_Ext_Perp","How would you use pacs.009 COV information to support Wolfsberg aligned AML monitoring","Parse pacs.009 COV to extract references to underlying customer transactions and join them to fact_payment_aml so FI to FI legs can be tied to end users Then build analytics that show high risk patterns such as certain respondents frequently sending high value flows from high risk customers even though flows appear as bank to bank at the FI layer"
"Day1_DataContracts_Ext_Perp","After reading CPMI reports how would you design metrics for cost speed transparency and access from your canonical warehouse","Define cost metrics using fee and charge related columns for each corridor speed metrics using time differences between initiation and final settlement transparency metrics from presence of uetr structured party data and accessible status information and access metrics based on number of reachable corridors schemes and counterparties measured using ISO based dimensions"
"Day1_DataContracts_Ext_Perp","How would you convince stakeholders that investing in ISO based metrics for CPMI's four challenges is strategically valuable","Explain that regulators are pushing for better cross border performance and that having ISO based metrics ready allows the bank to benchmark itself respond quickly to CPMI or central bank requests and demonstrate improvements in cost speed transparency and access ahead of competitors which may also reduce supervisory pressure"
"Day1_DataContracts_Ext_Perp","From the Valcon case study how would you justify building a canonical regulatory warehouse instead of bespoke solutions for each JP Morgan report","Highlight that Valcon showed fragmented report specific solutions lead to inconsistent measures and high change cost while a central warehouse with a common model lets new regulations like BOE or EBA updates be met by adding views rather than new pipelines improving consistency and time to market for hundreds of reports"
"Day1_DataContracts_Ext_Perp","How would you reuse Valcon's approach to handle a new EBA report using existing ISO and warehouse data","Interpret the new EBA template in terms of existing canonical facts and dimensions identify any missing attributes and update the mapping specification Add a new reporting view that groups fact_payment_reg by the required dimensions instead of creating a new ingestion pipeline and ensure lineage metadata links EBA cells back to canonical columns and ISO sources"
"Day1_DataContracts_Ext_Perp","What lesson from the Diceus warehouse case would you emphasise when discussing integration of legacy payment systems","Stress that integrating many legacy core systems into a central warehouse provides a unified view of payments and balances which is essential for regulatory reports and MI and show that trying to build reports directly on top of each legacy system individually is brittle and hard to scale"
"Day1_DataContracts_Ext_Perp","How would you design ETL from multiple payment engines similar to the Diceus case","Standardise on a canonical payment model and build source specific extraction jobs that map from each engine's schema into canonical staging tables then load fact_payment_reg and dimensions through shared transformation logic This ensures each engine's quirks are handled once while reports and regulatory extracts use the harmonised view"
"Day1_DataContracts_Ext_Perp","Using Martin Fowler's consumer driven contracts how would you reduce risk when changes are made to payment events used by regulatory ETL","Have regulatory ETL teams write contract tests specifying which fields they rely on and how they interpret them and run those tests in CI whenever event schemas or producers change This ensures any schema or semantic changes that could break regulatory pipelines are caught early and negotiated before deployment"
"Day1_DataContracts_Ext_Perp","How could consumer driven contracts improve collaboration between payments engine teams and reporting teams","They formalise expectations by letting reporting teams express required fields and invariants in executable tests while engine teams agree to honour them when evolving schemas creating a shared artefact beyond documentation and reducing miscommunication about what changes are safe"
"Day1_DataContracts_Ext_Perp","If CBPR plus updates introduce a new recommended field for sanctions analysis how would you integrate it into your contracts and model","Update ISO ingestion contracts and schemas to include the new field update mapping specifications and lineage metadata to show where it lands in fact_payment_aml and add DQ checks monitoring its population Use a non breaking schema evolution strategy so existing consumers remain functional while new ones can use the field"
"Day1_DataContracts_Ext_Perp","How would you design a data model that supports both SWIFT CBPR plus flows and SEPA CT SEPA Inst flows","Use a unified fact_payment_reg with generic attributes such as scheme_code instrument_type amount currencies parties and corridor Add scheme specific dimensions like dim_scheme and dim_service_level capturing CBPR plus or SEPA attributes and use flags or codes in facts to distinguish flows while keeping base columns consistent across schemes"
"Day1_DataContracts_Ext_Perp","How can EPC SCT Inst documentation influence your design of real time monitoring dashboards","It defines precise time limits and operational rules so you design dashboards that read from fact_payment_status and show per scheme and corridor the percentage of payments meeting time limits track near misses and highlight breakdowns in instant flows using timestamps derived from ISO messages"
"Day1_DataContracts_Ext_Perp","How would you apply FATF recommendations when deciding which ISO fields to treat as critical in data contracts","Classify fields related to identity such as names addresses LEIs countries and transaction context such as purpose and amount as critical in contracts mark them as mandatory or conditionally mandatory and set strict DQ thresholds acknowledging that incomplete or inconsistent values directly undermine AML model performance"
"Day1_DataContracts_Ext_Perp","How would Wolfsberg guidance shape your design of correspondent banking MI and regulatory reports","Ensure your model distinguishes between respondent and correspondent roles includes all intermediary banks and links FI payments to underlying customer flows and then build reports that show by correspondent bank the flows from high risk countries high risk sectors or with frequent sanctions hits matching Wolfsberg's emphasis on transparency"
"Day1_DataContracts_Ext_Perp","If CPMI asks your bank for a study on improving cross border access how could your ISO based warehouse support this","Use dim_bank and dim_corridor fed by ISO BICs and country codes to identify where the bank already has reachable counterparties and corridors analyse gaps for priority markets and show how participating in new schemes or networks could expand reach supporting CPMI's access objective with data rather than anecdote"
"Day1_DataContracts_Ext_Perp","How would you adapt Valcon's principles to avoid dozens of overlapping ISO 20022 based marts at JP Morgan","Define an enterprise canonical model for payments and regulatory reporting and enforce that new regulatory projects source data from this model through shared warehouses and semantic layers Prohibit each project from building their own isolated tables for common metrics and require definitions to be centrally governed"
"Day1_DataContracts_Ext_Perp","After reading Diceus's case how would you present the risks of not integrating legacy systems before a major ISO 20022 reporting project","Explain that without integration each system will require bespoke mapping and reporting logic which increases complexity and leads to divergent figures whereas integrating them into a warehouse first allows ISO fields to be harmonised and reused across all reports and analytics"
"Day1_DataContracts_Ext_Perp","How could you combine consumer driven contracts and ISO 20022 schemas to improve testability of your reporting platform","Use ISO XSDs and CBPR plus EPC usage rules as baseline schemas and then have each downstream system such as AML or reporting define consumer contracts that assert required fields and invariants Run these contracts in automated pipelines to verify that new ISO message or mapping changes still satisfy every consumer's needs"
"Day1_DataContracts_Ext_Perp","Design a summary slide that ties together CBPR plus EPC FATF Wolfsberg and CPMI for a stakeholder presentation","Summarise that all five bodies push toward richer standardised data ISO 20022 as the foundation and better metrics on cross border flows compliance and risk Explain that your architecture uses ISO messages canonical models and contracts to satisfy those expectations and show one or two example dashboards for corridors sanctions and CPMI metrics"
"Day1_DataContracts_Ext_Perp","How would you align your canonical warehouse metrics with CPMI definitions to avoid rework later","Implement measures like cost speed transparency and access exactly as CPMI describes using ISO based columns and store them in dedicated facts with documented formulas so that when CPMI or a central bank asks for them you can generate required statistics without rewriting logic across multiple reports"
"Day1_DataContracts_Ext_Perp","After reading Martin Fowler's work how would you integrate contract testing into your CI CD pipelines for regulatory data","Add stages to your pipelines where sample messages and database schemas are validated against consumer driven contracts and structural schemas Any incompatible change fails the build requiring review by both producers and reporting owners so regulatory pipelines are protected from uncoordinated changes"
"Day1_DataContracts_Ext_Perp","How would you use insights from Valcon and Diceus to argue for investing in strong lineage tooling","Point out that both case studies show regulators increasingly expect traceability and consistency and that lineage tooling lets you click from a BOE or EBA number back through warehouse columns to ISO fields and source systems making audits faster and reducing manual effort in explaining figures"
"Day1_DataContracts_Ext_Perp","What concrete changes would you make to your data model after reading Wolfsberg and FATF papers around ultimate beneficial owners","Ensure canonical facts and dimensions carry explicit ultimate party fields from ISO such as UltmtDbtr and UltmtCdtr and link them to KYC data capturing ownership structures and risk ratings and then model reports that distinguish flows by UBO country and sector rather than just account holders"
"Day1_DataContracts_Ext_Perp","How would you design an internal training exercise using ISO 20022 for dummies and CBPR plus to upskill BAs and devs","Create a series of scenarios where participants must choose and model the right ISO message explain key fields for AML and reporting then verify their design against CBPR plus usage rules and canonical mapping specs reinforcing both conceptual understanding and practical constraints"
"Day1_DataContracts_Ext_Perp","How could you benchmark your bank against CPMI goals using your ISO 20022 data","Compute CPMI metrics such as average cost and speed per corridor using ISO derived fields compare them with CPMI target benchmarks or industry reports and identify corridors or products where your bank underperforms making this the basis for improvement initiatives"
"Day1_DataContracts_Ext_Perp","Describe how you would leverage Valcon's central model idea for non payments regulatory domains such as credit or market risk","Adopt the same pattern by building shared canonical models and warehouses for credit exposures and trades rather than report specific data silos ensuring new regulations like AnaCredit or FRTB can be met through new views and lineage rather than new source specific pipelines"
"Day1_DataContracts_Ext_Perp","Using Diceus's case how would you prioritise source system onboarding into your warehouse for a JP Morgan role","Prioritise the core payment engines and AML systems that most directly affect high priority regulatory reports then gradually onboard less critical systems ensuring each integration is aligned to the canonical model and governed so future projects can reuse the data"
"Day1_DataContracts_Ext_Perp","How would you structure documentation so that external references like CBPR plus EPC FATF and CPMI are clearly linked to internal data contracts","Maintain a documentation hub where for each important external document you list its key data requirements and map them to internal contract artefacts such as schema definitions code lists and DQ rules and keep both sets of documents version aligned so changes in external guidance are reflected in internal contracts"
Day1_DataContracts_Gemi,"Design a 'Canonical Fact Table' for Regulatory Reporting that supports both legacy CHAPS (MT) and New CHAPS (ISO 20022). Explain your column strategy.","Table: `Fact_Canonical_Payment`. Columns: `UETR` (Primary), `Canonical_Amount`, `Canonical_Currency`. For Parties: `Debtor_Name`, `Debtor_ID`. Strategy: Use 'Super-Type' columns. If Source is MT, map Field 50K to `Debtor_Name`. If Source is ISO, map `<Dbtr><Nm>` to `Debtor_Name`. Keep source-specific columns (e.g., `Raw_MT_Payload`) in separate JSON/XML columns for audit, but drive reporting off the standardized columns."
Day1_DataContracts_Gemi,"Describe how you would implement 'Contract Testing' between the Payment Engine (Producer) and the Regulatory Warehouse (Consumer).","Implement a CI/CD step. Consumers define a 'Contract Test' (e.g., PACT file) specifying expected fields (`UETR`, `Amount`) and formats. When the Producer commits code, the pipeline runs these tests. If the Producer changed a field type or removed a mandatory field, the build fails, preventing the breaking change from reaching Production."
Day1_DataContracts_Gemi,"How do you handle 'Schema Evolution' in a Kafka pipeline when the ISO 20022 standard upgrades (e.g., adding a new mandatory field)?","Use a Schema Registry with 'Forward Transitive' compatibility. 1. Update the Schema Registry with the new definition. 2. Update Consumers to handle the new field (or ignore it). 3. Update Producers to start sending the new field. This ensures no downtime. For mandatory new fields, existing consumers must be updated *before* producers."
Day1_DataContracts_Gemi,"Design a partitioning strategy for a `Fact_Payment_Transaction` table with 50TB of data to optimize for Regulatory Reporting queries.","Partition by `Settlement_Date` (Monthly or Daily). Most regulatory queries filter by a specific time period (e.g., 'Last Quarter'). This allows the engine to 'Prune' partitions. Within partitions, use 'Clustered Columnstore Indexing' ordered by `Scheme_ID` or `Country_Code` to speed up categorical filtering."
Day1_DataContracts_Gemi,"Explain the 'Data Quality Firewall' concept in the context of ISO 20022 ingestion.","It is a validation layer at the entry of the Data Lake. It checks incoming `pacs.008` messages against defined rules (e.g., 'UETR is not null', 'Country Code is valid ISO 3166'). Valid records pass to the 'Clean' zone. Invalid records are tagged and routed to a 'Quarantine' zone for manual repair, ensuring the reporting layer is not polluted."
Day1_DataContracts_Gemi,"How would you model 'Slowly Changing Dimensions' (SCD) for `Dim_Customer` risk ratings in a regulatory mart?","Use SCD Type 2. Columns: `Customer_Key` (Surrogate), `Customer_Business_ID`, `Risk_Rating`, `Effective_From_Date`, `Effective_To_Date`, `Is_Current_Flag`. When a rating changes, close the old record (update `Effective_To_Date`) and insert a new record. Fact tables link to `Customer_Key` to preserve the history of the rating *at that time*."
Day1_DataContracts_Gemi,"Design the data lineage strategy for a 'Sanctions Hit' report to satisfy an external auditor.","Automated Lineage: Use a tool (e.g., Spline, Atlas) to capture metadata from ETL jobs (Spark/Airflow). The Lineage Graph must show: Report Column `Hit_Reason` <-- `Fact_Sanctions.Reason` <-- `Transformation_Logic (SQL)` <-- `Raw_Sanctions_Log.HitInfo`. Manually verify and tag 'Critical Data Elements' (CDEs) for higher scrutiny."
Day1_DataContracts_Gemi,"A new regulation requires reporting on 'Ultimate Debtor', but this field is only present in 60% of your data (ISO flows). How do you handle this in the Data Contract and Report?","Contract: Make `Ultimate_Debtor` an OPTIONAL field (Union: Null, String). Reporting Logic: Use `COALESCE(Ultimate_Debtor, Debtor)`. Create a separate 'Data Quality Report' tracking the % of NULL Ultimate Debtors to monitor the migration progress and chase upstream banks for better data."
Day1_DataContracts_Gemi,"Describe the trade-offs between 'Star Schema' and 'One Big Table' (OBT) for high-performance dashboards.","Star Schema: Storage efficient, easier to update dimensions (SCD), but requires Joins at query time. OBT: Zero joins (fastest read performance), but massive data redundancy (storage cost) and complex updates (if a customer name changes, you must update millions of Fact rows). For 2200 reports, OBT is often preferred for the most critical/frequent dashboards."
Day1_DataContracts_Gemi,"How do you ensure 'Idempotency' when loading `pacs.008` files into the warehouse to prevent duplicate reporting?","Use the `UETR` or `Instruction_ID` as a deduplication key. 1. Staging: Load all data. 2. Merge/Upsert: `MERGE INTO Target T USING Staging S ON T.UETR = S.UETR`. IF MATCHED THEN UPDATE (or Ignore). IF NOT MATCHED THEN INSERT. Ensure the Source System sends distinct identifiers."
Day1_DataContracts_Gemi,"Design a solution for 'Late Arriving Data' where a payment settles at 23:59 on Jan 31st but arrives in the warehouse on Feb 1st.","Use two date columns: `Event_Time` (Transaction Time) and `Processing_Time` (Ingestion Time). The 'January Report' query must filter by `Event_Time`. If the report was generated at 00:05 Feb 1st, and the data arrives at 00:10, you need a 'Restatement Process' to re-run the Jan 31st report and identify the delta."
Day1_DataContracts_Gemi,"How would you secure PII data (Names, Accounts) in the Data Lake while allowing Data Scientists to build Fraud Models?","Use 'Column-Level Encryption' or 'Tokenization'. Store the keys in a secure KMS (Key Management System). Data Scientists access views with Tokenized columns (e.g., `Tok_Debtor_Name`). They can build graph networks using the tokens. Only authorized Compliance Officers have the key to detokenize for investigations."
Day1_DataContracts_Gemi,"Explain how you would handle 'Reference Data Management' (e.g., BIC Directory updates) in your data pipeline.","Centralized Reference Service: Master the BIC directory in one place. Publish updates as a 'CDC Stream' (Change Data Capture). The Warehouse subscribes to this stream to update `Dim_Bank`. Pipelines looking up BICs should cache the directory or join to the Dimension, ensuring they use the version valid *at the transaction date*."
Day1_DataContracts_Gemi,"Design a 'Data Observability' framework for the payments pipeline.","Implement checks at 3 stages: 1. Ingestion: Freshness (Is data arriving?), Volume (Did we drop 50% rows?). 2. Transformation: Distribution (Did `NULL` rate spike?). 3. Output: Schema (Did columns change?). Use tools like Great Expectations or Monte Carlo to alert via Slack/PagerDuty if thresholds are breached."
Day1_DataContracts_Gemi,"Describe the process of 'Backfilling' a new calculated column (e.g., `derived_corridor`) for 5 years of history.","1. Optimize code: Ensure the calculation logic is efficient. 2. Batching: Process by Partition (Month by Month). 3. Automation: Use an Orchestrator (Airflow) to loop through dates. 4. Isolation: Write to a new column/table version to avoid impacting live reports. 5. Validation: Compare aggregates before and after."
Day1_DataContracts_Gemi,"How do you map the 'Payment Purpose' code when the source provides generic codes but the regulator requires granular codes?","Implement a 'Enrichment Logic' in the ETL. 1. Check Source Code. 2. If Generic, analyze `Remittance_Info` text using Keywords/NLP (e.g., 'Salary', 'Rent'). 3. Map to Regulatory Code. 4. Flag as 'Inferred' in the database so auditors know it wasn't explicit in the source."
Day1_DataContracts_Gemi,"Design a strategy for 'Cross-Border' reporting where the Exchange Rate changes between Initiation and Settlement.","Store both `Instructed_Amount` (Source Currency) and `Settlement_Amount` (Target Currency). Store `Exchange_Rate` and `Rate_Date`. The report must explicitly state which rate is used. For 'Normalized' reporting (e.g., all in USD), use a standard 'End-of-Day Mid Market Rate' table joined on `Settlement_Date`."
Day1_DataContracts_Gemi,"How would you architect a 'Dual-Schema' approach for the Reporting Database during a major ISO 20022 upgrade?","Create a new Schema `V2` (ISO Native). Maintain `V1` (Legacy) using Views that map `V2` columns back to `V1` names/formats on the fly. This allows existing `V1` reports to keep running without changes while new `V2` reports are built. Deprecate `V1` after transition."
Day1_DataContracts_Gemi,"Explain the role of a 'Dead Letter Queue' (DLQ) in a Payments Streaming architecture.","It captures messages that fail serialization or validation. Instead of crashing the consumer application, the bad message is routed to the DLQ. A support process monitors the DLQ, allowing engineers to inspect the payload, fix the bug/data, and 'Replay' the message into the main topic."
Day1_DataContracts_Gemi,"Design the 'Access Control' model for the Regulatory Data Warehouse (RBAC).","Roles: `Data_Eng` (RW on all), `Report_Bot` (RO on Marts), `Compliance_Officer` (RO on PII), `Analyst` (RO on Anonymized). Implementation: Use Database Roles + Active Directory Groups. Apply Row-Level Security on `Fact_Payment` linked to `User_Country` so French analysts only see French payments."
Day1_DataContracts_Gemi,"How do you ensure 'Consistency' between the OLTP Payment Engine and the OLAP Warehouse?","Reconciliation Process: Run an hourly 'Count & Sum' query on the Source (OLTP) and Target (OLAP) grouped by `Scheme`. Compare the results. If variance exists, trigger a 'Data Gap' alert. Do not rely solely on pipeline success logs."
Day1_DataContracts_Gemi,"Describe a 'Versioning Strategy' for your Data Contracts (e.g., JSON Schemas).","Use Semantic Versioning (MAJOR.MINOR.PATCH). PATCH: Description change. MINOR: Backward-compatible change (add optional field). MAJOR: Breaking change (remove field). Store schemas in Git. The Schema Registry enforces that producers increment version numbers correctly based on the change type."
Day1_DataContracts_Gemi,"How would you model 'Complex Hierarchies' (e.g., Parent Company -> Subsidiary) in `Dim_Customer`?","Use a 'Bridge Table' or 'Path Enumeration' if depth is variable. For standard reporting, 'Flatten' the hierarchy into fixed columns in the Dimension: `Level1_Parent`, `Level2_Parent`, `Ultimate_Parent`. This simplifies SQL aggregation (e.g., 'Sum by Ultimate Parent')."
Day1_DataContracts_Gemi,"What is your strategy for handling 'Test Data' in the Production Warehouse?","Strict Separation. Test data should never enter Production pipelines. If synthetic data is needed for Prod testing, use a specific `Test_Flag` column or a separate `Test_Partition`. Ideally, use a dedicated `Pre-Prod` environment that mirrors Prod for testing."
Day1_DataContracts_Gemi,"How do you handle 'Text Encoding' issues (e.g., Emojis, Chinese characters) in ISO 20022 text fields for legacy reports?","Contract: Define allowed character sets (e.g., Latin-1). Transformation: Cleanse/Strip unsupported characters during ingestion. Store the original UTF-8 in a separate `Raw_Text` column. For legacy reports, provide the 'Sanitized' version to prevent report crashes."
Day1_DataContracts_Gemi,"Design a query optimization strategy for a report that aggregates 5 years of data by `Customer_Sector`.","1. Ensure `Fact_Payment` is partitioned by Date. 2. Ensure `Dim_Customer` contains `Sector`. 3. Create a 'Bitmap Index' on `Sector` (if cardinality is low). 4. Use a 'Columnstore Index' on the Fact table. 5. Consider a pre-aggregated 'Aggregate Table' (`Agg_Sector_Monthly`)."
Day1_DataContracts_Gemi,"Explain 'Event Time' vs 'Processing Time' in the context of Regulatory SLA reporting.","`Event_Time`: When the payment actually happened. `Processing_Time`: When the system processed it. Regulatory SLAs usually apply to `Event_Time`. If your pipeline lags (high processing time), you might miss the reporting deadline even if the data is eventually correct. Monitor 'Pipeline Lag'."
Day1_DataContracts_Gemi,"How would you implement 'Soft Deletes' in the Data Warehouse for cancelled payments?","Add a `Is_Deleted` flag and `Deleted_Date` column. Do not physically `DELETE` rows (expensive, loses audit trail). Update the row to set `Is_Deleted = True`. Reporting Views should filter `WHERE Is_Deleted = False` by default."
Day1_DataContracts_Gemi,"Design a 'Metadata' strategy to help analysts understand the 2200 columns in the warehouse.","Implement a Data Catalog (e.g., Alation/Collibra). Automate metadata ingestion: Extract comments from DB columns. Link Business Glossary terms (e.g., 'Settlement Amount') to Physical Columns. Show Lineage. Require Data Stewards to verify definitions quarterly."
Day1_DataContracts_Gemi,"Describe how to handle 'Multi-Value Fields' (e.g., a payment has 3 distinct `Service_Codes`) in a Star Schema.","Option 1: Bridge Table (`Fact_Payment` <-> `Bridge_Service` <-> `Dim_Service`). Option 2: Array type (if DB supports it). Option 3 (if fixed max): `Service_Code_1`, `Service_Code_2`. Bridge table is best for flexibility and querying 'Any payment with Service X'."
Day1_DataContracts_Gemi,"How do you manage 'Database Schema Changes' (DDL) in a CI/CD pipeline?","Use Migration Tools (e.g., Flyway, Liquibase). Store DDL in version control. Pipeline: 1. Spin up ephemeral DB. 2. Apply current migrations. 3. Apply new migration. 4. Run tests. 5. If pass, apply to Dev/Test/Prod. Never run manual DDL."
Day1_DataContracts_Gemi,"Explain the 'Data Mesh' principle of 'Federated Computational Governance' in your context.","Instead of a central team bottlenecking all decisions, embed governance rules (e.g., 'All PII must be encrypted', 'All Tables must have Owners') into the Platform/Infrastructure code. Teams can build what they want as long as their code passes these automated platform checks."
Day1_DataContracts_Gemi,"Design a logic to handle 'Currency Conversion' for reporting in a base currency (e.g., GBP).","Table `Fact_Exchange_Rates` (Date, From_Ccy, To_Ccy, Rate). Query: Join `Fact_Payment` to `Fact_Exchange_Rates` on `Settlement_Date` and `Currency`. Logic: `Report_Amt_GBP = Local_Amt * Rate`. Handle missing rates (e.g., weekend) by using 'Last Available Rate'."
Day1_DataContracts_Gemi,"How would you handle a requirement to 'Re-state' regulatory reports from 6 months ago due to a logic error?","Since the Warehouse uses SCD2 and Partitioning: 1. Identify affected partitions. 2. Correct the Logic. 3. Re-process the data for those partitions into a `V2` table or `Restatement` partition. 4. Generate new reports. 5. Keep original reports for audit trail (do not overwrite files, issue 'Correction')."
Day1_DataContracts_Gemi,"Describe the architecture for 'Real-Time Fraud Detection' vs 'Batch Regulatory Reporting'.","Fraud: Stream Processing (Kafka + Flink/Kinesis) on raw events. Low latency, looking for patterns. State stored in fast KV store (Redis). Reporting: Batch Ingestion (Spark/Snowflake) into Warehouse. High latency, high accuracy, complex joins. Do not use the Warehouse for Real-time blocking."
Day1_DataContracts_Gemi,"How do you map the `CategoryPurpose` vs `Purpose` codes in ISO 20022 to the warehouse?","Store both. `CategoryPurpose` is high-level (e.g., `SALA` - Salary). `Purpose` is granular. Create `Dim_Purpose` hierarchy. If `Purpose` is missing, use `CategoryPurpose` to infer the general bucket. Ensure the Hierarchy handles the 'Parent-Child' relationship defined in ISO."
Day1_DataContracts_Gemi,"Design a strategy for 'Data Retention' and 'Purging' (GDPR) in a massive Fact Table.","Partition by Date. When partition age > Retention Policy (e.g., 7 years), drop the partition (Instant operation). For 'Right to be Forgotten' (User deletion), use the 'Surrogate Key' update method in Dimension, or run a 'Compaction' job to scrub PII from old partitions (expensive)."
Day1_DataContracts_Gemi,"Explain 'Write-Read Conflict' when the Reporting Tool queries a table while ETL is loading it.","The report sees partial data (Inconsistent state). Solution: 1. Load into a Staging/Swap table. 2. Use `SWAP PARTITION` or `RENAME TABLE` (Metadata operation) to switch instantly. 3. Use Database Isolation `READ COMMITTED SNAPSHOT` or `MVCC` to ensure readers see the 'Old' state until the 'New' transaction commits."
Day1_DataContracts_Gemi,"How would you use 'Data Profiling' stats to improve Data Contracts?","Run profiling (Min, Max, Null%, Unique%) on every batch. If `Null%` for a mandatory column shifts from 0% to 5%, fail the contract. If `Max Amount` jumps from 1M to 100B, trigger an anomaly alert. Use these stats to refine the contract constraints dynamically."
Day1_DataContracts_Gemi,"Design a 'Self-Service' reporting layer for business users.","Build 'Curated Views' or 'Data Marts' on top of the raw warehouse. Use a Semantic Layer (e.g., LookerML, dbt semantic layer) to define metrics (`Total_Volume`) centrally. Users drag-and-drop pre-defined metrics. Do not expose raw Fact tables with cryptic column names."
Day1_DataContracts_Ext_Gemi,"Explain the 'Medallion Architecture' in the context of ISO 20022 Data Lake.","Bronze: Raw ingestion of XML/JSON files (immutable, append-only). Silver: Cleaned, parsed tables (e.g., `pacs008_parsed`) with correct data types, deduplicated by UETR. Gold: Business-level aggregates (e.g., `Daily_Liquidity_Report`, `AML_Alerts`) ready for BI dashboards."
Day1_DataContracts_Ext_Gemi,"Design a strategy to handle 'Schema Drift' in a 'Bronze' layer ingestion pipeline without crashing.","Use 'Schema Evolution' or 'Merge Schema' features (e.g., in Spark/Delta). Allow new columns to be added automatically to the Bronze table. Rely on the Silver layer ETL to explicitly select/validate only the columns needed for downstream, ignoring the 'Drifted' columns until they are formally added to the contract."
Day1_DataContracts_Ext_Gemi,"How would you model a 'Payment Status' lifecycle (Pending -> Cleared -> Settled) in a Kimball Mart to analyze latency?","Use an 'Accumulating Snapshot Fact Table'. Columns: `PaymentID`, `Status_Pending_Time`, `Status_Cleared_Time`, `Status_Settled_Time`. As the payment moves, update the specific timestamp column for that row. This allows easy calculation of `Cleared_Time - Pending_Time`."
Day1_DataContracts_Ext_Gemi,"Design a 'Data Contract' clause for 'Data Freshness' (SLA).","Define an SLA property: `max_latency_ms: 5000`. The consumer monitors the difference between `event_timestamp` (Producer time) and `ingestion_timestamp`. If the 99th percentile exceeds 5 seconds, an alert fires to the Producer team indicating a breach of contract."
Day1_DataContracts_Ext_Gemi,"Explain how to use 'Z-Ordering' to optimize a Data Lake table for Regulatory Queries filtering by `Corridor` and `Date`.","Run `OPTIMIZE table_name ZORDER BY (SettlementDate, Corridor)`. This physically rearranges the data in the Parquet files so that payments with the same Date and Corridor are co-located. This allows the engine to skip vast amounts of data when querying specific corridors/dates."
Day1_DataContracts_Ext_Gemi,"How would you implement 'Row-Level Security' (RLS) in a Data Warehouse for a global team?","Create a mapping table `User_Access` (`User_Email`, `Allowed_Country`). Create a Security Policy on the `Fact_Payment` table that automatically appends `WHERE Country IN (SELECT Allowed_Country FROM User_Access WHERE User_Email = CURRENT_USER())` to every query."
Day1_DataContracts_Ext_Gemi,"Design a solution for 'Replaying' 1 year of payments through a new 'Fraud Model' without affecting live reporting.","Create a new 'Experiment' pipeline reading from the Bronze/Silver layer (same source). Write to a separate `Fraud_Scores_V2_Test` table. Do not overwrite the live `Fraud_Scores` table. Once backfill is complete and verified, swap the pointer or promote V2 to Production."
Day1_DataContracts_Ext_Gemi,"Explain the 'Outbox Pattern' for reliable event publishing from the Payment Engine.","Instead of publishing to Kafka directly (which might fail after DB commit), write the event to an `Outbox` table in the same DB transaction as the Payment. A separate 'Connector' process reads the `Outbox` table and pushes to Kafka, ensuring consistency between DB and Stream."
Day1_DataContracts_Ext_Gemi,"How would you handle 'GDPR Right to Erasure' in an immutable Data Lake (Bronze Layer)?","Since Bronze is immutable, you typically 'Crypto-Shred'. Encrypt PII columns with a unique key per user. Store keys in a separate Key Store. To 'Erase', delete the User's Key. The data remains in Bronze but is mathematically unreadable (garbage)."
Day1_DataContracts_Ext_Gemi,"Design a 'SCD Type 2' dimension for `Dim_Sanctions_List`.","Columns: `SanctionKey` (Surrogate), `EntityID` (Natural), `Status` (Active/Blocked), `ValidFrom`, `ValidTo`, `IsCurrent`. When a person is added to a list: Insert new row with `ValidFrom = Today`, `IsCurrent = True`. Update old row `ValidTo = Yesterday`, `IsCurrent = False`. Facts link to `SanctionKey`."
Day1_DataContracts_Ext_Gemi,"Explain 'Predicate Pushdown' and how it influences your file format choice.","It is the ability of the query engine to pass 'WHERE' clauses to the storage layer. Formats like Parquet/ORC have metadata (Min/Max values per block). If query is `WHERE Amount > 1000`, the reader skips blocks where Max < 1000. Text/CSV/JSON do not support this efficiently."
Day1_DataContracts_Ext_Gemi,"How would you architect a 'Real-Time Liquidity Dashboard' using Kafka and a Database?","Kafka (Ingest Payments) -> Stream Processor (Flink/Kafka Streams) aggregates by Currency/Bank (Window: 1 min) -> Upsert results into a fast KV Store (Redis) or OLAP DB (Pinot/ClickHouse) -> Dashboard (Grafana/Superset) polls the DB every second."
Day1_DataContracts_Ext_Gemi,"Design a 'Dead Letter Queue' (DLQ) strategy for schema validation failures.","If a message fails validation: 1. Do not crash. 2. Wrap the original message + Error Reason + Timestamp in a new JSON. 3. Publish to `payment_dlq` topic. 4. Support Team subscribes to DLQ, fixes the data/schema, and publishes back to `payment_retry` topic."
Day1_DataContracts_Ext_Gemi,"Explain the difference between 'Event Time' and 'Processing Time' windowing in stream aggregations.","'Event Time' uses the timestamp generated by the Payment Engine. 'Processing Time' uses the clock of the Stream Processor. For accurate regulatory reporting, ALWAYS use 'Event Time' to handle network delays/late data correctly, ensuring a payment made at 23:59 counts for that day."
Day1_DataContracts_Ext_Gemi,"How would you use 'dbt' (Data Build Tool) to implement Data Contracts in the warehouse?","Define `contracts` in the `schema.yml` file for your models. Specify constraints (e.g., `is_null: false`, `type: string`). dbt will enforce these constraints during the build process, failing the run if the data produced does not match the contract."
Day1_DataContracts_Ext_Gemi,"Design a 'Bridge Table' solution for payments that can belong to multiple 'Campaigns'.","Table `Fact_Payment` (PaymentKey...). Table `Dim_Campaign` (CampaignKey, Name...). Table `Bridge_Payment_Campaign` (`PaymentKey`, `CampaignKey`). To report, join Fact -> Bridge -> Dimension. Note: Be careful of 'Double Counting' when summing amounts."
Day1_DataContracts_Ext_Gemi,"Explain the 'Lambda Architecture' vs 'Kappa Architecture' for Payments.","Lambda: Two paths. Batch (Accurate, High Latency) + Speed (Approx, Low Latency). Merged at query time. Complex code duplication. Kappa: One path (Stream). Stream handles both real-time and reprocessing (replay). Simpler, but requires a robust streaming store (e.g., Kafka with long retention)."
Day1_DataContracts_Ext_Gemi,"How would you validate 'Referential Integrity' in a distributed Data Mesh (different databases)?","You cannot use Foreign Keys. Implement 'Soft' integrity checks: 1. Async reconciliation jobs checking IDs across domains. 2. Data Contracts requiring the Producer to validate IDs against a shared 'Reference Data Service' API before emitting the event."
Day1_DataContracts_Ext_Gemi,"Design a 'Partitioning Strategy' for a Data Lake storing 10 years of payments.","Partition by `Year`, then `Month`, then `Day`. Structure: `/data/year=2023/month=01/day=15/file.parquet`. This allows efficient queries for daily, monthly, or yearly ranges. Do not partition by `PaymentID` (too many partitions)."
Day1_DataContracts_Ext_Gemi,"Explain 'Compaction' (Small File Problem solution) in a Streaming Ingestion scenario.","Streaming creates many small files (latency). Run a scheduled 'Compaction Job' (e.g., Delta Optimize) that reads small files and rewrites them into larger (1GB) files. Ensure this job uses transaction isolation to not block readers."
Day1_DataContracts_Ext_Gemi,"How would you handle 'Duplicate Events' in an 'Exactly-Once' pipeline?","1. Use an idempotent storage (e.g., Database with Primary Key). Insert attempts with same PK will fail/ignore. 2. Use framework features (e.g., Kafka Streams EOS) which track sequence numbers and drop duplicates automatically before processing."
Day1_DataContracts_Ext_Gemi,"Design a 'Data Catalog' entry for the `pacs.008` table. What metadata is essential?","1. Business Name ('Customer Credit Transfer'). 2. Owner ('Payments Team'). 3. Refresh Schedule ('Streaming'). 4. Schema (Columns/Types). 5. Lineage (Source System). 6. Data Quality Score. 7. Classification ('Confidential/PII')."
Day1_DataContracts_Ext_Gemi,"Explain the use of 'Bloom Filters' in query optimization.","A probabilistic data structure that tells you if a key *might* be in a file or *definitely isn't*. The query engine checks the Bloom Filter first. If 'Definitely Isn't', it skips reading the file entirely. Great for 'Point Lookups' (Find Payment ID X)."
Day1_DataContracts_Ext_Gemi,"How would you model 'Currency' in a Multi-Currency Warehouse?","Use a `Dim_Currency` (ISO Code, Name, Symbol). In `Fact_Payment`, store `Original_Currency_Key`, `Original_Amount`. Also store `Reporting_Currency_Amount` (e.g., converted to USD) and `Exchange_Rate_Key` linking to the rate used. This allows aggregation in both local and global currency."
Day1_DataContracts_Ext_Gemi,"Design a strategy for 'Backfilling' a new metric (e.g., 'Processing_Time') for historical data.","1. Modify ETL logic to calculate metric. 2. Identify historical partitions. 3. Spin up a separate 'Backfill Cluster' (to save prod resources). 4. Reprocess history partition-by-partition, overwriting the data (idempotent). 5. Validate totals match before/after."
Day1_DataContracts_Ext_Gemi,"Explain 'Data Skew' mitigation in a Join operation (Spark).","If one key (e.g., 'NULL') has massive volume, the join hangs. Mitigation: 1. Filter out NULLs if not needed. 2. 'Salting': Add a random number (0-9) to the skewed key in both tables to split it into 10 smaller buckets, perform join, then remove salt."
Day1_DataContracts_Ext_Gemi,"How would you handle 'Schema Evolution' where a column type changes (String -> Int)?","This is a breaking change. 1. Create a NEW column `Amount_Int`. 2. Dual-write to both old and new columns. 3. Backfill new column. 4. Migrate consumers to new column. 5. Deprecate and remove old column. (Direct type cast often fails or corrupts data)."
Day1_DataContracts_Ext_Gemi,"Design a 'Semantic Layer' view for 'Churned Customers'.","Define 'Churn' logic (e.g., No payment in 90 days). View: `SELECT CustomerID FROM Fact_Payment GROUP BY CustomerID HAVING MAX(Date) < DATEADD(day, -90, CURRENT_DATE)`. Expose this as a boolean dimension `Is_Churned` in the BI tool."
Day1_DataContracts_Ext_Gemi,"Explain 'Snapshot Isolation' in the context of reading a Data Lake while it is being written to.","Readers view a 'Snapshot' of the files as they existed at query start time. Writers create new files. The storage log (Delta Log/Hudi Timeline) manages which files belong to which snapshot. This ensures readers never see partial/corrupt data."
Day1_DataContracts_Ext_Gemi,"How would you implement 'Data Quality as Code' in the pipeline?","Use libraries like 'Great Expectations' or 'Deequ'. Write assertions in the ETL script: `expect_column_values_to_not_be_null('UETR')`. If assertion fails -> Stop pipeline or Alert. Commit these checks to Git alongside the code."
Day1_DataContracts_Ext_Gemi,"Design a 'Cross-Region Replication' strategy for Disaster Recovery of the Regulatory Warehouse.","Enable automated storage replication (e.g., S3 CRR) to a secondary region. For the Compute/Catalog (Snowflake/Databricks), enable 'Database Replication' features to keep metadata in sync. In DR event, flip the DNS/Connection String to the secondary region URL."
Day1_DataContracts_Ext_Gemi,"Explain the concept of 'Governance Domains' in a Data Mesh.","Grouping Data Products by business function (e.g., 'Payments Domain', 'Risk Domain'). Each Domain has a 'Domain Data Owner' responsible for the governance, quality, and contracts of their data, rather than a central IT team doing it all."
Day1_DataContracts_Ext_Gemi,"How would you handle 'Reference Data' (e.g., Country Codes) that changes over time in the Warehouse?","Treat it as a Type 2 Dimension. If 'Yugoslavia' becomes 'Serbia', keep the old code for historical payments linked to the old timeframe, and use the new code for new payments. Do not overwrite, or history is lost."
Day1_DataContracts_Ext_Gemi,"Design a 'Cost Attribution' model for Data Usage.","Tag resources (Compute Clusters, Storage Buckets) with 'Cost Center' tags. Aggregate logs: 'Team Risk used 1000 DB Credits'. 'Team Payments stored 50TB'. Generate a monthly 'Bill' to show internal chargebacks, driving efficiency."
Day1_DataContracts_Ext_Gemi,"Explain 'Polyglot Persistence' for a Payment System.","Payment Engine uses RDBMS (Postgres) for ACID transaction. Search Service uses ElasticSearch for text search. Fraud uses Graph DB (Neo4j) for relationships. Reporting uses Columnar (Snowflake) for aggregation. Each tool fits a specific access pattern."
Day1_DataContracts_Ext_Gemi,"How would you optimize a 'Distinct Count' query (e.g., Unique Users) on a massive dataset?","Use 'HyperLogLog' (HLL) approximation algorithm if 99% accuracy is acceptable. It uses constant memory vs linear memory for exact count. If exact is required, ensure data is pre-aggregated or use Bitmap indexes."
Day1_DataContracts_Ext_Gemi,"Design a strategy to handle 'Test Data' pollution in Production reports.","Mandate a `is_test` flag in the Data Contract. In the Bronze layer, filter `WHERE is_test = false` before loading to Silver. Alternatively, segregate Test traffic to a completely different Topic/Bucket upstream."
Day1_DataContracts_Ext_Gemi,"Explain the risk of 'Data Silos' when using Microservices databases.","Each service has its own DB. Reporting needs joined data. Risk: Extracting data from 50 services is complex (ETL hell). Solution: CDC pipelines streaming all DB changes into a centralized Data Lake/Mesh for analytical joining."
Day1_DataContracts_Ext_Gemi,"How would you implement 'Tag-based Masking' for PII?","Tag columns in the Data Catalog (e.g., `Tag: PII`). Define a Policy: 'If User Group = Analyst and Tag = PII, show Masked'. The Database engine applies this policy dynamically at query time based on the tags."
Day1_DataContracts_Ext_Gemi,"Design a 'Data Retention' policy automation.","Use 'Lifecycle Management' rules on the Object Storage (S3/Azure). 'Rule: If file age > 7 years, move to Glacier'. 'Rule: If file age > 10 years, Delete'. For Database tables, run a monthly `DELETE WHERE Date < X` job (and optimize storage afterwards)."
Day0_BasicsPayments_Perp,"Design the end to end flow of a CNP card payment with 3DS highlighting authorization clearing and settlement","Describe the customer entering card details and the merchant triggering a 3DS authentication request through the card network to the issuer where SCA is performed via OTP or biometrics After successful 3DS the merchant sends an authorization request through the acquirer and network to the issuer which approves and places a hold Later the acquirer sends the transaction in a clearing file to the scheme the scheme delivers clearing records to issuers and calculates net positions and finally issuers and acquirers settle via RTGS or scheme settlement accounts and merchants are credited"
Day0_BasicsPayments_Perp,"Compare using Faster Payments versus ACH for a UK corporate payroll scenario and discuss risk timing and operational implications","Explain that ACH or BACS is optimised for bulk payroll with scheduled D plus one or D plus two settlement predictable cut offs and often lower unit cost while FPS gives real time posting but may require handling many individual payments higher liquidity demands and potentially higher fees Conclude that for payroll where value date is known in advance batch ACH style rails are usually more efficient while FPS is better reserved for urgent exceptions or corrections"
Day0_BasicsPayments_Perp,"Explain how you would design a P2P service on top of UPI including fraud controls and SCA","Outline a mobile app where users register and link bank accounts via UPI handle KYC and device binding and then send P2P payments by selecting contacts or using virtual payment addresses Use UPI PIN plus device security as SCA and add transaction risk scoring device fingerprinting and behavioural analytics to detect anomalies such as new devices sudden large transfers or mule behaviour and integrate strong customer education about scams and simple in app flows for dispute and block requests"
Day0_BasicsPayments_Perp,"Describe how you would introduce tokenization and biometrics into a mobile payment app while keeping SCA compliance","Propose storing card details as network or vault tokens rather than PANs and relying on the device secure element or platform token services for storage Use app registration to bind the device as the possession factor and enable biometric login and transaction signing as the inherence factor so most transactions meet SCA through device plus biometric Provide PIN or password fallback and avoid ever exposing PAN to app code or logs"
Day0_BasicsPayments_Perp,"Design a monitoring approach for authorized push payment fraud in a Faster Payments based P2P service","Explain that you would use behavioural and contextual analytics to look for unusual payment patterns such as new beneficiaries large first payments changes in device or location and high risk narratives along with sanctions and watchlist checks Implement real time scoring at initiation with the ability to delay or step up authentication for high risk transactions and provide in app warnings for known scam typologies plus post event review of complaints and chargeback like claims to refine rules"
Day0_BasicsPayments_Perp,"Explain the differences between authorization clearing and settlement for an ACH credit transfer from a corporate to a supplier","Describe that authorization is largely the internal approval and file submission by the corporate and its bank followed by format and basic risk checks rather than interactive card style auth Clearing occurs when the ACH operator receives the file distributes transactions to receiving banks and computes net obligations Settlement then happens when net positions are posted between participant banks at the central bank or settlement agent and only then are accounts finally updated"
Day0_BasicsPayments_Perp,"Outline how a UK ecommerce merchant can add Pay by bank via FPS and what benefits and risks this brings","Suggest integrating with an open banking or PIS provider that redirects customers to their banks for SCA and then triggers an FPS payment directly from customer to merchant account Benefits include lower card interchange costs faster settlement and fewer chargebacks while risks include push payment fraud limited refund tooling compared with cards and reliance on strong bank side SCA and consent flows"
Day0_BasicsPayments_Perp,"Design a high level flow for a cross border USD wire from a UK customer to a US beneficiary including key actors and accounts","Describe the UK customer instructing a USD payment at their UK bank which debits their GBP or USD account performs SCA and screening and sends a SWIFT message to its US correspondent bank The UK bank funds its USD Nostro at the correspondent if needed the correspondent credits the US beneficiary bank account and the beneficiary bank credits the customer Settlement takes place via debits and credits to Nostro or RTGS accounts between the banks and reconciliation ties messages to ledger entries"
Day0_BasicsPayments_Perp,"Explain how you would decide whether to route a GBP payment via FPS or CHAPS in an internal payment routing engine","State that you would use business rules based on amount urgency and use case for example amounts above a certain threshold or transactions needing guaranteed intraday finality such as property completions use CHAPS while routine lower value retail and SME payments use FPS Also factor in cut off times customer instructions and scheme cost and embed these rules in a routing table that operations can adjust under governance"
Day0_BasicsPayments_Perp,"Describe how settlement risk manifests in card payments and how scheme rules mitigate it","Explain that settlement risk arises because merchants deliver goods after authorization but before final settlement and issuers may fail or cardholders may not fund their accounts Scheme rules mitigate this through settlement cycles collateral requirements and risk sharing plus chargeback mechanisms that allow disputes to be resolved among issuer acquirer and merchant with clear responsibilities for fraud and non receipt of goods"
Day0_BasicsPayments_Perp,"Outline a design for handling ACH return codes in a bank operations and analytics stack","Describe ingesting all ACH returns into a dedicated table capturing codes original transaction references and amounts then routing operational items to case management for resolution such as contacting customers or correcting details Aggregate return data in a warehouse to monitor return rates by originator product and reason and feed rules back into onboarding and mandate management to reduce future exceptions"
Day0_BasicsPayments_Perp,"Explain how you would build a reporting view that compares performance of real time rails versus batch rails for retail transfers","Propose a unified fact table of payments with fields for rail type timing and outcome then create a view that groups by rail and period computing metrics such as average delivery time success rate value bands and fraud or dispute rates Use this to show management and product teams where real time rails are creating value versus where batch remains efficient"
Day0_BasicsPayments_Perp,"Design a checkout flow that supports both card with 3DS and account to account Pay by bank options while keeping UX coherent","Suggest a single payment selection page where customers choose card or bank transfer Card flows go through 3DS2 with risk based authentication while Pay by bank launches an open banking redirect for SCA but both share consistent messaging around security and confirmation The back end keeps a unified order state and records payment method details for reporting and reconciliation"
Day0_BasicsPayments_Perp,"Explain how real time rails change the requirements for bank liquidity management compared with batch systems","Describe that in batch systems banks can net positions and fund settlement at predictable times while real time rails cause continuous intraday movements so banks must monitor settlement accounts closely maintain buffers or prefunding and possibly use intraday credit or sweeps to avoid failed payments or overdrafts This increases the importance of real time liquidity dashboards and alerts"
Day0_BasicsPayments_Perp,"Describe how tokenization interacts with recurring CNP transactions such as subscriptions","Explain that instead of storing PANs the merchant or PSP stores tokens often network or vault tokens and uses them for subsequent recurring charges so that exposure of card data is limited When tokens are bound to merchant or device context they also reduce fraud risk and when cards are updated the token can often be refreshed automatically avoiding customer friction"
Day0_BasicsPayments_Perp,"Design a fraud control strategy for POS chip and PIN transactions compared to contactless","State that chip and PIN relies on strong card authentication and offline and online risk parameters with relatively low fraud so thresholds for additional checks can be higher For contactless you would set transaction and cumulative value limits trigger online authorization more often and occasionally request PIN entry after certain usage to manage risk while keeping speed at POS"
Day0_BasicsPayments_Perp,"Explain how you would integrate device fingerprinting into a CNP card gateway","Describe capturing device attributes such as browser fingerprint operating system identifiers and behavioural patterns at checkout and generating a device id that is passed to the issuer or fraud engine along with transaction data Use this id to track risky devices such as those associated with many cards or chargebacks and factor it into risk scores that determine when to challenge via 3DS or decline"
Day0_BasicsPayments_Perp,"Outline a design for an FPS based bill payment product for UK consumers","Propose an interface where consumers register billers and reference details then schedule or trigger payments that send FPS instructions from their bank to the biller bank Use confirmation of payee checks to reduce misdirected payments and add reminders and recurring options The product must integrate with risk and sanctions screening and provide notifications and simple dispute flows"
Day0_BasicsPayments_Perp,"Describe how settlement works for a domestic ACH debit between two banks","Explain that the originator bank submits a batch file with debit entries to the ACH operator which forwards items to receiving banks After all entries are processed the operator calculates net positions and instructs settlement at the central bank so that participant accounts are adjusted and each bank updates customer accounts according to scheme rules"
Day0_BasicsPayments_Perp,"Explain the lifecycle of a UPI merchant QR payment including customer experience and behind the scenes flows","Describe the customer scanning a QR code which encodes the merchant virtual payment address and amount then approving the payment with UPI PIN in their app The payer PSP sends the request to the UPI switch which routes to payer and payee banks for debit and credit decision and once approved both customers see confirmations while backend settlement between banks is handled by the UPI infrastructure on a frequent or near real time basis"
Day0_BasicsPayments_Perp,"Design a simple data model to capture key fields for analyzing performance of different rails","Suggest a fact table with payment id rail type origin and destination countries amount currency timestamps for initiation and completion and outcome plus dimensions for customer segment and product This model allows computation of metrics such as average completion time success rate and value by rail corridor and customer type"
Day0_BasicsPayments_Perp,"Explain how you would introduce SCA compliant flows for online card payments in a market newly adopting PSD2 rules","Outline that you would upgrade to 3DS2 integrate issuer ACS flows into web and app journeys implement logic to identify SCA exemptions such as low value or corporate and pass appropriate indicators to acquirers while defaulting to full SCA when uncertain Provide clear error handling and fallback to challenge flows and work with acquirers to ensure accurate flagging of exemptions"
Day0_BasicsPayments_Perp,"Describe a method to reconcile card clearing files with internal authorization logs","Propose matching on card number or token merchant id date time and amount tolerances linking clearing records to original auths and flagging mismatches for investigation For each batch compute counts and totals and reconcile to expected scheme settlement amounts and ensure unmatched items generate cases for operations"
Day0_BasicsPayments_Perp,"Explain why real time sanctions screening can be challenging on high volume instant payment rails and how you would address it","State that instant rails require sub second decisions so name and address matching must be highly optimised and tuned to control false positives without delaying payments Solutions include pre screening counterparties using whitelists and reference data using efficient matching algorithms and scaling infrastructure horizontally while defining clear rules for auto release and manual review of borderline hits"
Day0_BasicsPayments_Perp,"Design an approach for allowing customers to save card details safely for one click e commerce payments","Suggest using network or PSP tokenization storing tokens not PANs and binding tokens to the merchant and device Use SCA with 3DS for initial tokenization then allow subsequent low risk transactions to be processed in a frictionless manner where exemptions are allowed while maintaining strong fraud monitoring"
Day0_BasicsPayments_Perp,"Explain how you would design customer notifications across the payment lifecycle to reduce disputes and fraud","Propose sending real time alerts on authorizations and postings especially for CNP and international transactions providing clear merchant and amount details Offer easy options to report unknown transactions freeze cards or accounts and summarise daily activity so that suspicious activity is spotted quickly and disputes can be raised early"
Day0_BasicsPayments_Perp,"Describe a design for handling dual rails where card and real time account to account are both offered at an ecommerce checkout","Explain offering a choice of card including 3DS and account to account using open banking payments and implementing routing rules for merchant priorities and customer preferences Store the outcome and costs of each method for analytics and design reconciliation and refund processes suited to each rail while giving merchants dashboard level visibility of method mix and performance"
Day0_BasicsPayments_Perp,"Explain how ACH batch processing shapes the daily rhythm of bank operations","Describe that operations teams work around file cut offs submission windows and settlement cycles scheduling tasks like payroll file acceptance exception handling and funding Nostro or settlement accounts at specific times This creates peaks of processing and affects staffing cash management and reporting cycles"
Day0_BasicsPayments_Perp,"Design a high level migration plan from legacy card auth only to full 3DS2 support","Outline assessing current gateway capabilities integrating a 3DS server or ACS selecting acquirer and scheme integrations updating checkout flows adding mobile SDKs for in app 3DS2 and performing testing with issuers Start with soft launch and risk based routing then ramp up coverage while monitoring friction abandonment and fraud and adjusting rules and UX accordingly"
Day0_BasicsPayments_Perp,"Explain how you would use data from different rails to build a unified customer payment behaviour view","Propose consolidating payments from cards ACH instant rails and wires into a common fact table with normalized customer ids merchant or counterparty identifiers categories and amounts then using this view to analyze patterns such as preferred channels frequency and cross border behaviour supporting both product design and AML"
Day0_BasicsPayments_Perp,"Describe how you would design an FPS based instant payout feature for card refunds that posts back to customer accounts","Explain that instead of refunding via card schemes which can be slow the merchant or PSP could request bank account details or use open banking to identify the account then send an FPS credit matching the card refund amount The system must link the payout with the original card transaction for accounting and ensure appropriate fraud checks before sending money to the claimed account"
Day0_BasicsPayments_Perp,"Explain how you would integrate behavioural biometrics into a mobile banking app without degrading UX","Suggest collecting passive signals such as typing rhythm swipe patterns and motion data during normal use and feeding them into a risk engine which scores sessions and transactions Only when scores indicate risk would you add explicit step up such as PIN or biometric challenge so most users experience no extra friction while suspicious activity is challenged"
Day0_BasicsPayments_Perp,"Design an internal education module to teach business stakeholders the differences between push and pull payments","Propose a workshop using scenarios like payroll versus direct debit utility bills and card subscriptions explaining who initiates the payment typical protections fraud patterns and dispute rights Use diagrams of message and account flows and short quizzes so stakeholders can correctly select appropriate rails and risk controls for new products"
Day0_BasicsPayments_Perp,"Describe how you would build KPIs for success of a new real time rail integration from a bank perspective","Define metrics such as adoption rate by customer segment percentage of eligible payments routed to the rail average delivery time failure and return rates fraud loss rate per value and customer satisfaction measures Compare these to baseline batch rails and use dashboards to inform product adjustments and operational improvements"
Day0_BasicsPayments_Perp,"Explain the main architectural challenges of supporting twenty four by seven instant payments across multiple regions","State that always on services demand highly available multi region infrastructure zero or near zero downtime deployments real time monitoring and robust failover strategies and synchronization with batch based systems like GL and AML plus cross time zone operations staffing This requires decoupling processing from reporting clear data contracts and strong operational readiness"
Day0_BasicsPayments_Perp,"Design a flow explaining to a regulator how your bank ensures SCA compliance for mobile initiated payments across cards and account to account rails","Outline that your mobile app binds devices and uses secure elements or platform security then uses biometrics and or PIN as second factor for login and payment initiation For cards you use 3DS2 integrated into the app and for account payments you use strong login and payment signing flows all logged with evidence of factor use and exemptions and monitored through regular control testing and audits"
Day0_BasicsPayments_Gemi,"Design the end-to-end flow of a CNP card payment with 3DS, highlighting authorization, clearing, and settlement steps.","1. Auth: Customer enters card details -> Merchant Server -> 3DS MPI (Directory Server) -> Issuer (ACS). 2. Challenge: Issuer performs Risk Analysis. If High Risk, user does Biometric/OTP challenge. 3. Result: Issuer returns Authentication Value (CAVV). 4. Authorization: Merchant sends Auth Request (0100) + CAVV to Acquirer -> Scheme -> Issuer. Issuer verifies CAVV, checks balance, approves (0110). 5. Clearing: End of day, Merchant sends Capture Batch to Acquirer. Acquirer submits to Scheme. 6. Settlement: Scheme nets positions. Acquirer receives funds, Merchant receives funds (T+1/2)."
Day0_BasicsPayments_Gemi,"Compare using Faster Payments (FPS) vs ACH (BACS) for a UK corporate payroll scenario; discuss risk, timing, and operational implications.","FPS: Instant (seconds), 24/7. High limit (£1M). Risk: Irrevocable (mistakes hard to fix), requires liquidity pre-funding. Ops: 'Just-in-time' payroll possible. BACS: 3-day cycle (Input Day, Processing Day, Entry Day). Low limit constraints. Risk: Revocable (Recall possible within windows). Ops: Requires submitting file 2 days early. FPS preferred for gig-economy/urgent; BACS for standard monthly salary due to robustness and cost."
Day0_BasicsPayments_Gemi,"Explain how you would design a P2P service on top of UPI, including fraud controls and SCA.","Architecture: Mobile App acts as PSP (Payment Service Provider). 1. Registration: Bind Device ID + SIM to User. 2. Addressing: Create VPA (user@app). 3. Auth: Use UPI Common Library (CLM) for SCA (MPIN entered on secure screen). 4. Fraud Controls: Velocity checks (max 10 tx/day), Geo-fencing, 'New Device' cooling period (24h low limit), Beneficiary Trust Score (warn if paying unknown VPA)."
Day0_BasicsPayments_Gemi,"Describe how you would introduce tokenization and biometrics into a mobile payment app while keeping SCA compliance.","1. Tokenization: Integrate with VTS/MDES (Scheme Token Services). When user adds card, request Token. Store Token + Cryptogram, not PAN. 2. Biometrics: Use Device Biometrics (FaceID) to unlock the App KeyStore. 3. SCA: Design the flow so Biometric Unlock = 'Inherence' and Device Binding = 'Possession'. 4. Transaction: Pass Device Authentication Data (CD-CVM) in the auth message so the Issuer knows SCA was performed on-device."
Day0_BasicsPayments_Gemi,"Explain the role of the 'Acquirer' when a Merchant faces a Chargeback, and design a process to handle it.","Role: Acquirer receives the Chargeback from Scheme, debits Merchant's account, and notifies Merchant. Process Design: 1. Ingestion: Automated parsing of Scheme Dispute files. 2. Notification: Webhook to Merchant 'Dispute Opened'. 3. Evidence Collection: Portal for Merchant to upload Proof of Delivery/Logs within time limit (e.g., 20 days). 4. Representation: Acquirer reviews evidence and submits 'Representment' to Scheme. 5. Outcome: If won, credit Merchant. If lost, write off."
Day0_BasicsPayments_Gemi,"Design an 'Idempotency' strategy for a Payment API to prevent double-charging during network timeouts.","1. Client Requirement: Client must send a unique `Idempotency-Key` header (UUID) with every POST request. 2. API Layer: Check Key + Payload hash in Redis cache. 3. Logic: If Key exists and status is 'Processing', return 409 (Conflict/Wait). If Key exists and status is 'Success', return stored 200 OK response (do not re-process). If Key new, process payment and store result. 4. Retention: Keep keys for 24-48 hours."
Day0_BasicsPayments_Gemi,"Compare 'Network Tokenization' vs 'PCI Proxy' (Vault) approaches for a recurring billing merchant.","Network Tokenization: Tokens issued by Visa/MC. Pros: Auto-updates (lifecycle management) if card lost. Lower Interchange rates sometimes. Cons: Complex integration, dependent on Issuer readiness. PCI Proxy: Tokens issued by Gateway/Vendor. Pros: Gateway agnostic (if independent proxy), works with all cards immediately. Cons: Tokens break if card number changes (declines), no interchange benefit, vendor lock-in."
Day0_BasicsPayments_Gemi,"How would you architect a 'Least Cost Routing' (LCR) engine for a merchant accepting Debit Cards?","Data Inputs: BIN Table (identifies Card Type/Network), Interchange Rate Table, Scheme Fee Table, Acquirer Per-Transaction Fees. Logic: 1. Lookup BIN -> Identify if 'Dual Network' (e.g., local EFT vs Visa Debit). 2. Calculate Total Cost for Route A vs Route B. 3. Dynamic Selection: Send auth request to cheaper network. 4. Fallback: If cheaper route fails/declines, retry on premium route."
Day0_BasicsPayments_Gemi,"Describe the 'Settlement Risk' differences between RTGS (CHAPS) and DNS (BACS) and how central banks mitigate DNS risk.","RTGS: Zero settlement risk. Funds move line-by-line. If Sender has no funds, txn fails immediately. DNS: High settlement risk. Participants accumulate debt during the cycle. If Bank A fails before 4pm settlement, Bank B is exposed. Mitigation: 1. Caps (Net Debit Caps). 2. Collateral (posting bonds/cash at Central Bank to cover max exposure). 3. Loss Sharing Agreements (surviving banks cover the default)."
Day0_BasicsPayments_Gemi,"Design a mechanism to handle 'Late Returns' in ACH (e.g., Unauthorized Return received 60 days later).","Challenge: Funds likely already paid out to user. Design: 1. Reserve/Hold: Hold % of high-risk funds (Rolling Reserve). 2. Negative Balance: Allow User Wallet to go negative. 3. Recovery: Trigger auto-debit from backup source or collections workflow. 4. Ban: Add bank account fingerprint to 'Negative File' to prevent re-use. 5. Monitoring: Alert if Return Rate > 0.5% (NACHA threshold)."
Day0_BasicsPayments_Gemi,"Explain 'Dynamic Linking' in the context of PSD2 SCA and how it prevents 'Man-in-the-Middle' attacks.","Concept: The Authentication Code (OTP/Cryptogram) must be cryptographically bound to the 'Amount' and 'Payee'. Implementation: 1. App generates data blob: `Hash(Amount + Currency + IBAN)`. 2. User signs blob with Private Key (triggered by Biometric). 3. Verification: Server verifies signature against Public Key and checks if Hashed details match the actual transaction request. If Hacker intercepts and changes Amount, signature fails."
Day0_BasicsPayments_Gemi,"How would you manage 'FX Risk' in a Cross-Border remittance service that offers 'Guaranteed Rates' to customers?","Risk: Rate changes between 'Customer Quote' and 'Interbank Settlement'. Mitigation: 1. Real-time sourcing: Get streaming rates from Liquidity Provider + Margin. 2. Short Validity: Quote valid for only 30-60 seconds. 3. Back-to-Back Hedging: Immediately execute a swap/forward with LP as soon as Customer commits (Micro-hedging). 4. Wallets: Pre-fund currency accounts (Nostro) to avoid on-the-fly conversion for popular corridors."
Day0_BasicsPayments_Gemi,"Design a 'Retry Strategy' for a Payment Gateway interacting with an unstable Acquirer.","1. Categorize Errors: Distinguish 'Soft Declines' (Insufficient Funds, Timeout, System Error) from 'Hard Declines' (Stolen Card, Closed Account). 2. Instant Retry: For Timeouts, retry immediately (once) to alternate endpoint. 3. Intelligent Retry: For Insufficient Funds, schedule retry on 'Pay Day' (End of month/Friday). 4. Circuit Breaker: If Acquirer failure rate > 50%, trip breaker and route traffic to Backup Acquirer."
Day0_BasicsPayments_Gemi,"Explain the concept of 'Interchange++' pricing vs 'Blended' pricing for a merchant.","Blended: Merchant pays flat rate (e.g., 2.9%). Simple, predictable, but opaque. Merchant overpays on cheap cards (Debit). IC++: Merchant pays Interchange (Pass-through to Issuer) + Scheme Fees (Pass-through to Network) + Acquirer Markup (fixed). Complex reconciliation, fluctuating costs, but transparent and cheaper for large volume/safe cards."
Day0_BasicsPayments_Gemi,"Design a solution for 'Split Payments' in a Marketplace (e.g., Uber: Driver + Uber Fee) while avoiding being a regulated body.","Challenge: Touching funds requires license. Solution: Use a Licensed PSP's 'Marketplace Engine'. 1. Pay-in: Customer pays full amount to PSP's Escrow/Client Money Account. 2. Allocation: Marketplace sends instruction 'Give $80 to Driver, $20 to Platform'. 3. Pay-out: PSP settles directly to Driver and Platform separately. Marketplace never 'holds' the funds."
Day0_BasicsPayments_Gemi,"What is 'Level 3 Data' in card processing, and why/how would you implement it for a B2B supplier?","Definition: Line-item detail (Product Code, Qty, Tax, Freight). Why: Lowers Interchange fees (Data reduces risk for Issuer) and helps Corporate Buyer reconciliation. Implementation: 1. Gateway: Update API to accept L3 JSON objects. 2. Mapping: Map Merchant ERP data to Scheme L3 Fields. 3. Transmission: Send extra data in the Clearing/Settlement batch file."
Day0_BasicsPayments_Gemi,"Design a fraud rule set for a 'Digital Goods' merchant (High risk, instant delivery).","1. Velocity: Max 2 txns per card per 24h. 2. Geo: IP Country must match Card Issuing Country. 3. Email: Block disposable domains. 4. Device: Block known bad Device IDs. 5. 3DS: Force 3DS on ALL first-time transactions. 6. Bin Check: Reject Prepaid Cards (high fraud correlation). 7. Manual Review: Hold if Amount > $100."
Day0_BasicsPayments_Gemi,"Explain how 'Instant Payment' schemes handle 'Liquidity Management' for banks on weekends.","Challenge: RTGS (Central Bank) might be closed. Solution: 1. Pre-funding: Banks must deposit cash in a dedicated 'Settlement Account' or 'Technical Account' at the Central Bank before the weekend. 2. Caps: Payment engine strictly rejects outbound transfers if the pre-funded balance hits zero. 3. Liquidity Bridge: Mechanisms to move collateral during off-hours (if supported)."
Day0_BasicsPayments_Gemi,"Design a 'Reconciliation System' for a PSP handling Card and APM (Alternative Payment Methods) settlements.","1. Standardization: Normalize settlement reports from Visa, PayPal, Klarna into a 'Canonical' schema. 2. 3-Way Match: Match Internal Order (DB) vs Gateway Transaction Log vs Acquirer Settlement Report. 3. Logic: Match on `Order_ID` or `Acquirer_Ref`. Check `Gross`, `Fee`, `Net`. 4. Exception Queue: Flag 'Settled but not Ordered' (Ghost txn) or 'Ordered but not Settled' (Missing funds). 5. Output: Ledger entries."
Day0_BasicsPayments_Gemi,"How does 'Open Banking' (PISP) payment flow differ from a Card flow? Design the user journey.","Diff: Push (Bank Transfer) vs Pull (Card). No Chargebacks. Lower fees. Journey: 1. Checkout: User selects 'Pay by Bank'. 2. Consent: Merchant redirects user to their Bank App (Deep Link). 3. Auth: User logs in (Biometric) and confirms Amount/Payee. 4. Execution: Bank sends Faster Payment to Merchant. 5. Redirect: Bank sends user back to Merchant Success Page."
Day0_BasicsPayments_Gemi,"Explain the 'Dual Message' vs 'Single Message' distinction and its impact on 'Open-to-Buy' balances.","Dual (Credit): 1. Auth (Hold): Reduces 'Available Balance', leaves 'Ledger Balance'. 2. Clearing (Post): Updates 'Ledger Balance' days later. Single (Debit/ATM): Auth & Clearing combined. Reduces both balances immediately. Impact: Dual allows 'Tips/Gratuity' adjustment (Restaurant auths $100, clears $120). Single requires exact final amount upfront."
Day0_BasicsPayments_Gemi,"Design a 'Stand-In Processing' (STIP) logic for a Payment Processor when the Upstream Bank is slow.","Trigger: If Bank Latency > 5s for 10% of txns. Action: 1. Low Value: Approve transactions < $20 locally (Risk: Processor eats loss). 2. VIPs: Approve known good shoppers. 3. Decline: Decline high-value/risky txns. 4. Queue: Store 'Advice' messages to update Bank when back online."
Day0_BasicsPayments_Gemi,"How would you implement 'Sanctions Screening' for a Real-Time Payment system without adding latency?","1. Async Loading: Pre-load Sanctions Lists into memory (not DB lookup). 2. Fuzzy Matching: Use optimized algorithms (e.g., Levenshtein) with strict timeout (e.g., 50ms). 3. Parallelism: Run screening in parallel with Schema Validation. 4. Fail-Open/Close: Define policy (Usually Fail-Close: if Screening times out, Reject payment). 5. Strip False Positives: Whitelist known false positives."
Day0_BasicsPayments_Gemi,"Describe the 'Soft Descriptor' on a bank statement and how dynamic descriptors reduce chargebacks.","Soft Descriptor: The text 'PAYPAL *MERCHANTNAME' on the statement. Dynamic: Changing the text per transaction. Usage: 1. Clarity: Put the specific 'Brand Name' or 'Product' (e.g., 'NYTimes Sub' vs 'NYTimes Store'). 2. Support: Add phone number 'Help: 555-0199'. Benefit: Customer recognizes the charge, reducing 'Did not recognize' disputes."
Day0_BasicsPayments_Gemi,"Design a 'Virtual IBAN' architecture for a Neo-Bank.","Concept: Many Virtual IBANs (vIBAN) map to one Physical Master IBAN. 1. Master Account: Held at Clearing Bank. 2. vIBAN Generation: Neo-Bank generates valid IBANs (Country + Bank Code + Unique Customer Ref). 3. Ingestion: Clearing Bank receives payment to Master IBAN. 4. Virtualization: Clearing Bank passes 'Remittance Information' or 'Virtual Account Number' in the file. 5. Ledger: Neo-Bank parses file, identifies Customer by Virtual Number, credits Customer Wallet."
Day0_BasicsPayments_Gemi,"Explain the impact of 'ISO 20022' migration on Data Truncation risks in AML.","Risk: ISO 20022 allows long/structured data (e.g., 140 char Name). Legacy MT103 allows short/unstructured (4x35 chars). Mitigation: 1. Mapping: Define strict truncation rules (keep first 35 chars vs smart trim). 2. Structured vs Unstructured: Prioritize Structured tags. 3. Golden Copy: Always screen against the full ISO original, never the truncated MT version."
Day0_BasicsPayments_Gemi,"Design a 'Pre-paid Card' Issuing platform flow for Authorization.","1. Network sends 0100 message to Processor. 2. Processor parses PAN, maps to Wallet ID. 3. Balance Check: Query Ledger (Atomic). If Balance >= Amount, Reserve Funds, Return 0110 (Approve). 4. If Balance < Amount, Return 0105 (Decline). 5. Update 'Open-to-Buy'. 6. Log 'Auth Hold'."
Day0_BasicsPayments_Gemi,"How would you handle 'Merchant Initiated Transactions' (MIT) authentication under SCA rules?","Requirement: First txn (CIT) needs SCA. Subsequent (MIT) are exempt if linked. Design: 1. Setup: Perform CIT with 3DS. Flag `challengeIndicator=01`. 2. Save Credential: Store `networkTransactionId` (Trace ID) from CIT. 3. MIT Execution: Send auth with `dsTransID` linking to original. Flag `posEntryMode=CredentialOnFile`. Scheme recognizes link and waives SCA."
Day0_BasicsPayments_Gemi,"Compare 'Direct Integration' vs 'Hosted Fields' for card acceptance from a Security/PCI perspective.","Direct (API): Merchant server receives PAN. High Risk. Scope: SAQ-D (Audits, Pen Tests). Full Control of UI. Hosted Fields (iFrame): Scripts load input fields from Gateway domain. PAN bypasses Merchant server. Low Risk. Scope: SAQ-A (Self-assessment). Good UI control. Preferred for modern e-com."
Day0_BasicsPayments_Gemi,"Design a 'Refund' logic for a transaction that settled 6 months ago (Archived).","Challenge: Original auth token might be expired; Card might be closed. Logic: 1. Lookup: Retrieve original ARN (Acquirer Reference Number). 2. Attempt Referenced Refund: Send refund msg linked to ARN. 3. Fallback: If declined (Card Closed), prompt Merchant for 'Unreferenced Refund' (OCT/Push to Card) or Bank Transfer. 4. Risk: Flag Unreferenced Refund for manual approval (high fraud risk)."
Day0_BasicsPayments_Gemi,"Explain the 'Zero Trust' security model implementation for a Payment Switch.","1. mTLS: Mutual TLS for all microservice communication. 2. Least Privilege: Services only access DBs they own. 3. Tokenization: No cleartext PANs in logs/internal messages. 4. Network Segmentation: Payment CDE (Cardholder Data Environment) isolated from Corporate Network. 5. Immutable Logs: Ship logs to WORM (Write Once Read Many) storage."
Day0_BasicsPayments_Gemi,"Design a 'Payment Orchestration Layer' to optimize for 'Availability' (Uptime).","1. Multi-Acquirer: Integrate 3+ Acquirers. 2. Health Checks: Active probing of Acquirer APIs. 3. Failover Logic: If Acquirer A fails 3x, route to B. 4. Queueing: If all down, queue non-interactive txns (e.g., subscriptions) for later. 5. Circuit Breaking: Stop sending to failing provider to prevent cascading latency."
Day0_BasicsPayments_Gemi,"How would you handle 'Double Spending' risks in an offline digital cash system?","Challenge: Device is offline, can't check ledger. Solution: 1. Secure Element: Hardware enforcement (TEEs) prevents cloning cash tokens. 2. Counter: Monotonic counters signed by issuer. 3. Deferred Sync: When device goes online, sync logs. 4. Risk Limit: Low limits ($50) for offline mode. 5. Blacklist: If double spend detected later, brick the device certificate."
Day0_BasicsPayments_Gemi,"Design a 'Credit Push' fraud detection model vs a 'Debit Pull' model.","Push (Authorized Push Payment Fraud): Focus on 'Social Engineering'. Detect: New Beneficiary, Abnormal Amount, User hesitating (biometrics), Remote Access Tool active. Pull (Card Fraud): Focus on 'Stolen Credential'. Detect: Velocity, Distance from home, Mismatching Device ID, Incorrect CVV."
Day0_BasicsPayments_Gemi,"Explain the mechanism of 'Settlement Finality' in Blockchain vs RTGS.","RTGS: Finality is legal and immediate upon posting to Central Bank ledger. Probabilistic? No. Blockchain (PoW): Probabilistic Finality. Valid after X confirmations (blocks mined on top). Risk of 'Re-org'. Design implication: Wait 6 blocks (Bitcoin) before releasing goods, or use L2 instant finality solutions."
Day0_BasicsPayments_Gemi,"Design a 'Multi-Currency' wallet ledger to handle FX conversion.","Table Structure: `Wallet_Balance` (User_ID, Currency, Amount). Logic: 1. User pays USD with GBP wallet. 2. Lock Quote: Get FX Rate R. 3. Debit: `UPDATE Wallet_Balance SET Amount = Amount - (USD_Amt / R) WHERE Ccy='GBP'`. 4. Internal Transfer: Move GBP to 'FX_Holding'. 5. Credit: Move USD from 'FX_Holding' to Nostro. 6. Audit: Record Rate and Timestamp."
Day0_BasicsPayments_Gemi,"How do you ensure 'Data Consistency' across Microservices (Order Svc -> Payment Svc) without 2PC?","Pattern: Saga / Outbox Pattern. 1. Order Svc: Create Order (Pending). Insert 'PaymentRequired' event to Outbox table (Atomic). 2. Relay: CDC/Worker pushes event to Kafka. 3. Payment Svc: Consumes event. Processes Payment. Publishes 'PaymentSuccess'. 4. Order Svc: Consumes Success. Updates Order to 'Confirmed'. Handle 'Compensating Transaction' (Refund) if Order fails after Payment."
Day0_BasicsPayments_Gemi,"Design a 'Chargeback Prevention' alert system using 'Ethoca' or 'Verifi'.","Concept: Issuers alert Merchants of confirmed fraud *before* processing Chargeback. Design: 1. Subscribe to Alert Network. 2. Ingest Alert (Webhook). 3. Auto-Action: If status is 'Settled', issue Refund immediately. 4. Outcome: Refund prevents Chargeback fee and Strike against merchant ratio. 5. Stop Future: Blacklist the card/user."
Day0_BasicsPayments_Gemi,"Explain the role of 'HSM' (Hardware Security Module) in Payment Processing.","Role: Manage Keys and Perform Crypto operations inside a tamper-resistant hardware. Usage: 1. PIN Translation: Decrypt PIN block from ATM key, re-encrypt with Network key. 2. CVV Validation: Calculate CVV based on PAN+Exp+ServiceCode+Key. 3. Key Generation: Generate session keys. *Never* expose Master Keys in software/memory."
Day0_BasicsPayments_Gemi,"Design a 'Subscription Billing' engine logic for handling 'Expiration Date' updates.","Problem: Card expires, recurring payment fails. Solution: 1. Account Updater: Subscribe to Visa VAU / MasterCard ABU. 2. Batch: Send file of stored cards monthly. 3. Update: Scheme returns new Expiry/PAN. 4. Apply: Update DB. 5. Retry: If Updater fails, Email Customer 'Update Payment Method'. 6. Grace Period: Allow 7 days service continuity before churn."
"Day0_CorrBanking_Perp","Describe the full payment and account flow when a UK bank sends USD to a Brazilian beneficiary via a US correspondent including Nostro Vostro impacts","Explain that the UK bank debits its customer's account and sends a SWIFT payment message such as pacs.008 or MT103 to its chosen US correspondent according to SSIs The UK bank's USD Nostro with the US correspondent is debited while on the US bank's books the UK bank's Vostro is reduced The US bank either credits the Brazilian bank's USD account held with it or forwards the payment through another correspondent and once Bank C in Brazil receives funds it credits the beneficiary in local currency using its BRL Nostro at the domestic RTGS while each step is confirmed via SWIFT messages and reconciled against Nostro statements"
"Day0_CorrBanking_Perp","Design a Nostro reconciliation process using SWIFT statements MT950 or camt.053 and internal ledgers and explain how you handle timing differences","Describe ingesting external statements into a structured store then matching entries to internal GL and transaction tables using amount currency value date and references Apply matching rules with tolerances and reason codes Flag unmatched items as breaks and categorise them as timing FX fees or potential errors For timing differences recognise that some transactions appear externally before internal posting or vice versa and maintain an aging view so genuine timing breaks are cleared on subsequent days while unexplained breaks trigger investigation and possible manual adjustments"
"Day0_CorrBanking_Perp","Explain how the migration from MT202 COV to pacs.009 COV changes transparency and data modelling for FI to FI payments","State that pacs.009 COV provides richer structured fields for underlying customer parties addresses and remittance than MT202 COV so data models must include explicit ultimate debtor creditor and structured address attributes in FI payment facts rather than just storing free text reference fields This transparency allows correspondent banks and regulators to see which customers are behind FI transfers and supports better sanctions and AML analytics so warehouses and screening tools must be upgraded to consume these new ISO elements as first class fields"
"Day0_CorrBanking_Perp","Describe how you would monitor and manage intraday USD Nostro liquidity across multiple correspondents","Propose aggregating real time balances from camt.052 or intraday feeds and internal pending payment queues into a central liquidity dashboard showing current positions expected inflows and outflows and bilateral limits For each correspondent set target and minimum balances and automated alerts when projected balances approach thresholds Trigger sweeps or RTGS transfers from central bank or other sources to top up deficient accounts and throttle or reroute large outgoing payments when funding is tight while keeping treasury and operations aligned on cut offs and FX requirements"
"Day0_CorrBanking_Perp","Design a central SSI repository and routing engine for a global transaction bank","Explain that you would maintain a controlled database keyed by currency corridor and beneficiary attributes that stores preferred correspondents account numbers BICs and routing chains Expose this via an internal API or rules engine that payment systems call when constructing SWIFT or ISO messages Enforce version control approval workflows and four eyes checks for SSI changes and log every routing decision so that liquidity risk cost and performance can be analysed per corridor and adjustments can be made centrally"
"Day0_CorrBanking_Perp","Explain how you would integrate UETR based tracking into operations for cross border payments","Describe assigning or reading UETR for each eligible cross border payment storing it in internal payment tables and exposing it in operations dashboards Link every SWIFT message pacs.008 pacs.009 pacs.002 to the same UETR and use SWIFT gpi or equivalent tracking APIs to fetch status from the network Provide search by UETR in operations and customer service tools so staff can see each hop times and statuses and feed this data into SLA and investigation workflows"
"Day0_CorrBanking_Perp","Describe how you would use ISO 20022 data to build a correspondent banking risk dashboard for senior management","Propose a warehouse fact table capturing pacs.008 and pacs.009 flows with correspondent and respondent BICs amounts currencies corridors and KYC derived attributes such as sector and risk rating Then build dashboards that show volumes and values by correspondent country and risk band top high risk corridors sanctions hit rates and trends in payments routed through each partner so management can see concentration risk and the impact of any de risking or remediation initiatives"
"Day0_CorrBanking_Perp","Design an escalation process for significant Nostro reconciliation breaks identified at end of day","Explain that large or aged breaks should trigger automatic alerts to operations treasury and risk teams categorised by type such as missing incoming credit or unexpected debit The process should define ownership timelines and actions such as contacting the correspondent checking SWIFT messages adjusting internal postings or temporarily restricting outgoing flows Through regular governance reviews persistent break patterns should drive fixes in upstream processes SSIs or data mappings"
"Day0_CorrBanking_Perp","Explain how you would incorporate FX rate differences into Nostro reconciliation logic","Describe matching Nostro movements that include FX conversion by comparing the booked foreign currency movements with internal local currency postings and agreed FX rates from the treasury rate table Use tolerance bands to accommodate minor rounding differences and flag any entries where applied rates are outside allowed ranges for investigation adjusting internal FX entries as necessary to align the two sets of books"
"Day0_CorrBanking_Perp","Describe how you would architect systems to support both MT and ISO 20022 messages during SWIFT migration","Suggest building a canonical internal payment model that is ISO centric and mapping both MT and ISO messages into that model Use translation layers at the edge MT to ISO and ISO to MT as needed for external connectivity while treating ISO based facts as golden source for screening liquidity and reporting Provide tools to ensure no loss of key information such as ultimate parties during translation and plan phased decommissioning of MT once all counterparts support ISO"
"Day0_CorrBanking_Perp","Explain how you would use ISO 20022 structured address fields to improve sanctions screening on correspondent flows","Propose mapping structured postal address elements such as country city and street name from pacs messages into screening engines rather than free form address lines Configure matching algorithms to weight structured fields appropriately and implement DQ checks to ensure these fields are populated for high risk corridors This reduces ambiguous matches and improves detection of sanctioned locations compared with text based MT data"
"Day0_CorrBanking_Perp","Design a control framework around bilateral limits with correspondents using transaction data","Describe storing bilateral credit and volume limits per correspondent and corridor then tracking utilisation in real time as payments are processed When utilisation exceeds a defined threshold generate alerts or block further flows until treasury approves additional limits or reroutes payments Regularly back test limits using historical ISO transaction data to ensure they are consistent with actual volumes and risk appetite documenting all breaches and responses for regulators"
"Day0_CorrBanking_Perp","Explain how you would support regulatory reporting on cross border flows using a Nostro based data model","Recommend creating a fact table of cross border payments linked to Nostro movements capturing payer and beneficiary countries currencies amounts sectors and correspondent BICs From this model derive reports that aggregate flows by corridor counterparties and sectors as required by central banks and international bodies while linking every reported value back to underlying ISO messages and Nostro statement lines for auditability"
"Day0_CorrBanking_Perp","Describe how you would implement intraday alerts for potential Nostro overdrafts","Explain that you would forecast end of hour or end of day positions based on current balances booked and queued payments and expected incoming flows If projected balances fall below thresholds or become negative trigger alerts to liquidity desks and optionally automatically halt or reroute low priority outgoing payments Provide visual dashboards highlighting accounts at risk and integrate with funding tools to initiate top up transfers"
"Day0_CorrBanking_Perp","Design a process to onboard a new correspondent including technical and risk aspects","Outline that first compliance and risk teams perform due diligence on the prospective correspondent then legal teams agree terms including AML responsibilities and reporting Next technical teams exchange BICs keys and message formats and set up test payment flows and statement exchanges SSIs and bilateral limits are configured in routing engines and finally controlled pilot volumes are processed and monitored before full scale use"
"Day0_CorrBanking_Perp","Explain how you would handle a situation where a key correspondent announces exit from a region you rely on","Propose rapidly assessing impacted corridors and volumes identifying alternative correspondents or direct memberships in clearing systems negotiating new relationships and updating SSIs and routing rules Plan data migration and Nostro account closure activities and communicate with clients about potential changes in cut offs or fees while monitoring operational and liquidity impacts of the transition"
"Day0_CorrBanking_Perp","Describe how you would design reports to measure the impact of de risking on customer access","Suggest measuring number of active corridors and correspondents per country volumes and values of payments before and after de risking decisions and numbers of rejected or rerouted payments Analyse changes by customer segment and product to show which clients or regions lost access and present this to risk committees and potentially supervisors along with compensating measures for legitimate customers"
"Day0_CorrBanking_Perp","Explain how ISO 20022 pacs.008 and pacs.009 messages can feed both operational and risk analytics in correspondent banking","State that pacs.008 provides structured data on cross border customer credits while pacs.009 describes FI to FI transfers including cover payments Mapping their party agent currency amount and purpose fields into a warehouse enables operations to track flows and breaks while AML and risk teams can analyse corridors high risk sectors sanctions hits and correspondent exposures from the same consistent dataset"
"Day0_CorrBanking_Perp","Design a model for capturing and analysing correspondent fees and charges from Nostro statements","Describe parsing fee line items from MT950 or camt.053 statements and mapping them into a fee fact table with attributes such as correspondent currency product corridor and fee type Link these to underlying payments where possible and create reports showing effective per transaction and per value fees over time enabling negotiation of better pricing and routing optimisation"
"Day0_CorrBanking_Perp","Explain how you would architect a multi bank tracking dashboard using UETR for operations teams","Propose a central service that ingests SWIFT tracking updates and internal status messages keyed by UETR and stores them in a tracking table For each UETR the dashboard shows all hops banks timestamps and statuses providing filters by corridor correspondent and problem codes Allow operators to drill from a customer inquiry to the underlying chain of events and escalations to counterparties when delays occur"
"Day0_CorrBanking_Perp","Describe how you would align RTGS ISO migration with Nostro based settlement in your internal data model","Explain that high value RTGS payments and Nostro based settlements should both feed a unified settlement fact table keyed by account and date containing amounts currencies and references RTGS messages provide central bank account movements while Nostro statements reflect correspondent flows and both map back to customer payments through UETR or internal ids enabling reconciled views of settlement activity across both channels"
"Day0_CorrBanking_Perp","Explain how you would design a data quality monitoring framework for correspondent banking ISO 20022 fields","Recommend defining critical fields for each AML and reporting use case such as debtor country ultimate debtor and intermediary agents then setting completeness and validity thresholds for each corridor Build daily DQ reports showing population rates outliers and mapping errors for these fields and link persistent issues back to specific channels or systems so remediation is targeted and progress can be tracked"
"Day0_CorrBanking_Perp","Design a high level architecture for integrating camt.053 statements into liquidity management tools","Propose a feed that transforms camt.053 XML into structured tables capturing opening balances transactions and closing balances per account and value date Provide APIs or direct views for liquidity engines to query latest balances and cash flow history and combine this with internal forecasts from payments pipelines so liquidity decisions are based on reconciled data rather than manual spreadsheets"
"Day0_CorrBanking_Perp","Describe how you would use graph analytics on ISO 20022 data to improve correspondent banking AML controls","Explain building a graph where nodes represent banks accounts and customers and edges represent payments derived from pacs.008 or pacs.009 fields Then run algorithms to identify clusters with high exposure to sanctioned entities long chains of intermediaries or unusual patterns across corridors Use these insights to adjust risk ratings of correspondents refine transaction monitoring rules and support de risking decisions grounded in granular data"
"Day0_CorrBanking_Perp","Explain how changes to SSIs should be governed to avoid operational incidents","Describe requiring formal change requests with impact analysis and approvals from operations treasury and risk before SSI modification Apply four eyes control enforce dual control for key activities and schedule changes outside peak times with rollback plans Log all changes in a central repository and run validation by sending low value test payments before routing production volumes through new paths"
"Day0_CorrBanking_Perp","Design a process and data model for tracking failed or returned cross border payments by corridor and reason","Suggest a fact table for payment returns capturing original payment id corridor correspondent reason codes values and timestamps sourced from pacs.002 or MT199 messages Then build dashboards to show return rates per corridor and correspondent helping to identify operational or data quality issues and common failure drivers such as incorrect account details sanctions hits or insufficient funds"
"Day0_CorrBanking_Perp","Explain how you would support multi currency liquidity optimisation across several Nostro accounts per currency","Propose aggregating balances and projected flows per currency and account and defining optimisation rules that hold minimal operating balances at correspondents and concentrate surplus cash at central bank or main treasury accounts Use automated sweeps and FX trades triggered by thresholds to balance positions and incorporate regulatory liquidity metrics such as LCR and NSFR in decision making"
"Day0_CorrBanking_Perp","Describe how ISO 20022 migration affects testing and certification for correspondent payment flows","Explain that new pacs and camt messages require end to end schema validation mapping verification and scenario testing with correspondents under CBPR plus or RTGS guidelines Banks must update test harnesses for various use cases including straight through and exception flows and retest sanctions screening reconciliation and liquidity tooling ensuring that fields critical for compliance are populated and consumed correctly"
"Day0_CorrBanking_Perp","Explain how you would present Nostro and correspondent risk to a board or risk committee","Describe summarising exposures by correspondent country and currency across Nostro balances pending payments and credit lines showing trends and stress scenarios Include metrics on sanctions hits operational incidents and returns and present mitigation actions such as diversification of correspondents improved screening or strategic de risking where needed supported by ISO based data and graphical views"
"Day0_CorrBanking_Perp","Design an onboarding questionnaire for potential respondents focusing on AML and sanctions controls","Propose questions about regulatory status licensing sanctions policies customer risk assessments ongoing monitoring use of ISO data KYC practices for high risk clients and cooperation in information sharing Request documentation of policies and independent audit results and use answers to score the respondent against your bank's risk appetite and regulatory expectations"
"Day0_CorrBanking_Perp","Explain how you would ensure that correspondent payment processing remains resilient during SWIFT outages","Suggest designing failover paths such as alternative network providers or contingency arrangements like manual messages for critical payments while keeping clear procedures and limits for their use Store SSIs and routing logic locally so routing decisions do not depend on network availability and maintain regular tests of contingency plans with operations and counterparties"
"Day0_CorrBanking_Perp","Describe how you would integrate regulatory expectations from Wolfsberg FATF and CPMI into correspondent banking architecture","Explain that these bodies emphasise transparency AML effectiveness and accurate cross border data so architecture should preserve ultimate party details and structured addresses in ISO format leverage them in screening and risk models and feed aggregated metrics on cost speed transparency and access into reporting tools Governance should ensure that these requirements are baked into design standards and documentation"
"Day0_CorrBanking_Perp","Design a training session to explain Nostro Vostro and Loro to non technical stakeholders using examples","Propose using simple three bank scenarios showing how a UK bank's USD account at a US bank is its Nostro and the US bank's Vostro and introducing a third bank that calls it their account with you Use diagrams to show debits and credits on each ledger when a payment is sent and complement this with exercises where participants label accounts correctly to reinforce understanding"
"Day0_CorrBanking_Perp","Explain how you would structure documentation so that every correspondent payment can be traced from SWIFT message to ledger and report","Recommend capturing message ids UETR and internal transaction ids in payment and ledger tables and maintaining lineage metadata that maps each GL and reporting entry back to source messages and accounts Provide tools that allow auditors or investigators to input a payment reference and retrieve all related messages Nostro movements and report entries in one view"
"Day0_CorrBanking_Perp","Describe how you would build KPIs to monitor operational health of correspondent flows","Suggest KPIs such as percentage of payments settled on value date return and investigation rates by corridor average time to resolve Nostro breaks sanctions hit rates settlement failures and average processing times by correspondent Monitor these on dashboards segmented by currency customer segment and corridor and use deviations from thresholds to trigger problem solving and continuous improvement"
"Day0_CorrBanking_Perp","Summarise how you would explain the strategic importance of correspondent banking architecture to a senior executive","State that robust correspondent architecture and data models underpin the bank's ability to move money globally safely and profitably enabling cross border trade while meeting demanding sanctions AML and reporting requirements Emphasise that investments in ISO 20022 data quality routing liquidity tools and reconciliation reduce risk improve customer experience and position the bank well for evolving regulatory and market expectations"
Day0_CorrBanking_Gemi,"Describe the full payment and account flow when a UK bank sends USD to a Brazilian beneficiary via a US correspondent, including Nostro/Vostro impacts.","1. UK Bank (Barclays) debits Client GBP, converts to USD. 2. Barclays sends `pacs.008` (Customer) to Brazil Bank (Bradesco) and `pacs.009 COV` (Cover) to US Correspondent (JPMC). 3. Barclays credits `JPMC Vostro` (Internal Mirror). 4. JPMC receives `pacs.009`, debits `Barclays Nostro`, credits `Citibank Nostro` (Bradesco's corr) via CHIPS. 5. Citibank credits `Bradesco Nostro`. 6. Bradesco receives `pacs.008`, sees cover at Citibank, credits Beneficiary."
Day0_CorrBanking_Gemi,"Design a Nostro reconciliation process using SWIFT statements (MT950 / camt.053) and internal ledgers; how do you handle timing differences?","1. Ingest: Parse `camt.053` (Bank View) and Internal GL Extract (Book View) daily. 2. Normalize: Map codes (e.g., `NTRF` -> Wire) and amounts. 3. Auto-Match: Rules engine matches on `Amount` + `Reference` + `Value Date`. 4. Timing Logic: If unmatched, check `Value Date`. If GL date < Stmt Date, mark 'In Transit'. If Stmt > GL, mark 'Unadvised'. 5. Exception: Manual queue for 'Fee Deductions' or 'FX Variances'."
Day0_CorrBanking_Gemi,"Explain how the migration from MT202COV to pacs.009 COV changes transparency and data modelling for FI-to-FI payments.","MT202COV was a workaround with size limits. `pacs.009 COV` is native XML. Transparency: Allows full structured data for Originator/Beneficiary (Address, ID) in the nested 'Underlying Customer Credit Transfer' block. Modeling: Database must store the FI-to-FI transaction (Header) AND the full Customer details (Nested) separately but linked, ensuring Sanctions Screening can access the nested granular data without truncation."
Day0_CorrBanking_Gemi,"Describe how you would monitor and manage intraday USD Nostro liquidity across multiple correspondents.","1. Dashboard: Aggregate real-time balances from `camt.052` (Intraday updates) across all USD Nostros. 2. Forecasting: Overlay scheduled outgoing payments (Queued) vs expected incoming covers. 3. Alerting: Trigger if Balance < Limit. 4. Action: If short, execute Intraday Swap (Swap EUR for USD) or 'Sweep' excess cash from a long account (e.g., JPMC) to the short account (e.g., BofA) via CHIPS."
Day0_CorrBanking_Gemi,"Explain the role of 'RMA' (Relationship Management Application) in SWIFT security and how it impacts new corridor setup.","RMA is the authorization layer. Bank A cannot send a `pacs.008` to Bank B unless Bank B has accepted an RMA Key from A. Impact: Setting up a new corridor requires exchanging RMA keys first. If RMA is 'Revoked' (e.g., due to Sanctions risk), the messaging pipe is hard-blocked at the SWIFT network level, preventing any payment flow."
Day0_CorrBanking_Gemi,"Design a 'De-risking' strategy that minimizes client impact while satisfying compliance.","Instead of blanket exiting a region (De-risking): 1. Enhanced Data: Demand ISO 20022 `Structured Data` from Respondent to enable better screening. 2. Segmentation: Analyze Respondent's client base. Allow 'Low Risk' flows (Trade, Gov) but block 'High Risk' (Casino, Crypto) using Policy rules. 3. Pricing: Increase fees for high-risk corridors to cover enhanced manual diligence costs."
Day0_CorrBanking_Gemi,"How does 'Settlement Risk' differ in a Serial vs Cover payment method?","Serial: Bank A pays B, B pays C. Each hop is funded. If B fails, C never gets the msg or funds. Settlement risk is sequential. Cover: Bank A sends msg to C, funds go A->Corr->C. Risk: Bank C might credit the Beneficiary upon receipt of `pacs.008` (advice) *before* the Cover Funds (`pacs.009`) actually arrive/settle in their Nostro. If Cover fails, Bank C is out of pocket."
Day0_CorrBanking_Gemi,"Explain how 'Standard Settlement Instructions' (SSIs) are maintained and the risk of 'Ad-hoc' instructions.","Maintenance: Stored in a 'Static Data' master (e.g., Bankers Almanac/SWIFT Ref). Automatic updates via SWIFT broadcast. Risk: Ad-hoc instructions (typed manually in a payment) bypass validation. High risk of fraud (Man-in-the-Middle changing Acct #) or error (Stuck funds). Policy: Always prioritize stored SSIs over ad-hoc unless validated by callback."
Day0_CorrBanking_Gemi,"Design a workflow for handling 'Unadvised Credits' on a Nostro account.","1. Detection: `camt.053` shows Credit, no matching GL entry. 2. Investigation: Parsing the Narrative/Remittance info in the statement. 3. Search: Scan 'Expected Receipts' database. 4. Posting: If owner found, book GL credit + Notify Client. 5. Suspense: If unknown, book to 'Suspense/Investigations' GL. 6. Return: If unclaimed after 30 days, return funds to Sender."
Day0_CorrBanking_Gemi,"How would you calculate the 'Cost of Funds' for a Nostro account that is persistently long?","Cost = (Average Daily Excess Balance) * (Internal Cost of Capital rate OR Opportunity Cost rate e.g., Overnight LIBOR/SOFR). If Nostro pays 0% interest but Cost of Capital is 5%, holding $10M excess costs $500k/year. Strategy: Minimize excess buffer, sweep to interest-bearing accounts overnight."
Day0_CorrBanking_Gemi,"Explain the architectural difference between 'Nostro-Vostro' clearing vs 'RTGS' clearing.","Nostro-Vostro: Bilateral. Book entry transfer. Bank A debits Bank B's account. No Central Bank involved. Trust based. Fast but creates credit risk. RTGS: Multilateral. Funds move across Central Bank accounts (Fed/BoE). Finality is immediate and legal. Zero credit risk. Used for high value/settlement of net positions."
Day0_CorrBanking_Gemi,"Design a control framework to prevent 'Nesting' abuse by a Respondent Bank.","1. Contract: Clause prohibiting 'Downstream Clearing' for unauthorized 3rd party banks. 2. Monitoring: Analyze transaction data. Look for 'Indeterminate' originators or patterns where the Respondent acts as a pass-through (High volume, low value, rapid turnover). 3. RFI: Request Information on specific transactions. If Respondent refuses/delays, it suggests nesting."
Day0_CorrBanking_Gemi,"How do you handle 'Sanctions Screening' on a `pacs.009 COV` message?","Screening is dual-layer. 1. Header: Screen the Financial Institutions (Sender, Receiver, Intermediary) against Bank Blocklists. 2. Nested Block: Extract `UnderlyingCustomerCreditTransfer`. Screen Originator and Beneficiary against Person/Entity Blocklists. Both must pass. A hit on the nested customer freezes the cover payment."
Day0_CorrBanking_Gemi,"Explain the impact of 'Value Dating' on Reconciliation breaks.","Scenario: You send payment on Day 1. Correspondent processes it Day 1 but applies Value Date Day 2 (due to cut-off). Ledger = Day 1. Statement = Day 2. Result: Timing break. Mitigation: Configure Rec tool to match on `Value Date` tolerance (+/- 1 day) or `Entry Date`, but flag 'Value Date Loss' (Interest claim) if they delayed value unreasonably."
Day0_CorrBanking_Gemi,"Design a 'Payment Repair' queue for SWIFT messages with invalid Routing.","1. Validation: Check `Intermediary_BIC` against SWIFT Directory. 2. Failure: Route to 'Repair Queue'. 3. GUI: Operator views message. System suggests 'Correct BIC' based on fuzzy match or historical learning. 4. Action: Operator updates BIC -> Re-validate -> Release. 5. Feedback: Update Master Data to prevent recurrence."
Day0_CorrBanking_Gemi,"How does 'CLS' (Continuous Linked Settlement) mitigate Herstatt Risk?","CLS acts as a trusted third party. Bank A pays Currency X to CLS. Bank B pays Currency Y to CLS. CLS waits until *both* have paid. Then CLS pays out the swaps simultaneously. If one fails, CLS returns the funds to the payer. Net result: No party is left without principal."
Day0_CorrBanking_Gemi,"Describe the 'Investigation' lifecycle for a Beneficiary Claiming Non-Receipt.","1. Trigger: Client email. 2. Trace: Check internal status -> 'Sent'. 3. Tracker: Check SWIFT gpi Tracker. Is it stuck at Intermediary? 4. Proof: Generate MT103 copy / gpi printout for client. 5. RFI: Send MT199/camt.030 to Correspondent: 'Please advise status/credit date'. 6. Resolution: Corresp confirms credit or returns funds."
Day0_CorrBanking_Gemi,"What is the role of the 'Account Servicing Institution' in the ISO 20022 chain?","It is the bank *holding* the account being debited or credited. In a direct flow, the Correspondent is the Account Servicing Institution for the Respondent (Account Owner). They act as the 'ledger keeper' and generator of the statement."
Day0_CorrBanking_Gemi,"How would you implement 'Principal Protection' (OUR charges) logic in a payment engine?","1. User selects 'OUR'. 2. Engine checks Correspondent Tariff. 3. Engine calculates 'Cover Fee' (e.g., $20). 4. Debit Client: Principal + $20. 5. Send SWIFT: Field `71A` = `OUR`. 6. Send Cover: `pacs.009`. Note: Correspondent will deduct their fee from the *Cover* (or bill separately), but they must pass the full Principal to the next bank."
Day0_CorrBanking_Gemi,"Design a strategy to migrate Legacy MT103 templates to pacs.008 for corporate clients.","1. Audit: Analyze past 12 months of MT103s. 2. Gap Analysis: Identify missing ISO fields (e.g., Structured Address, Purpose Code, Ultimate Debtor). 3. Outreach: Contact clients with 'Data Remediation' packs. 4. Channel Upgrade: Update e-Banking portal to capture new fields. 5. Defaulting: For low-risk missing fields, apply default values temporarily (e.g., 'Unstructured' address) but flag for future fix."
Day0_CorrBanking_Gemi,"Explain 'Mirror Accounting' vs 'Real Accounting' in Nostro management.","Real Accounting: The Correspondent's books (Legal truth). Mirror Accounting: Your internal shadow copy. You debit your Mirror when you send a payment instructions. You credit Mirror when you receive funds. Recs compare Mirror vs Real. Discrepancies = Unreconciled Items."
Day0_CorrBanking_Gemi,"How do you handle 'Duplicate UETRs' in your ingestion pipeline?","1. Check: Is the payload identical? Yes -> Ignore (Re-transmission). No -> Reject (Integrity Error). Unique Constraint: Database `Payment` table must have Unique Index on `UETR`. If `INSERT` fails, catch error and trigger check logic."
Day0_CorrBanking_Gemi,"Design a report for 'Nostro Funding Cost Analysis'.","Metrics: 1. Avg Intraday Overdraft amount. 2. Overdraft Interest Paid. 3. Avg Long Balance (Idle cash). 4. Opportunity Cost (Long Balance * Risk Free Rate). 5. Transaction Fees. Goal: Optimize liquidity to keep balance near zero without failing payments."
Day0_CorrBanking_Gemi,"Explain the 'Three-Corner Model' vs 'Four-Corner Model' in payment clearing.","3-Corner: Sender and Receiver use the *same* Correspondent (Book Transfer). Fast, cheap. 4-Corner: Sender Bank -> Clearing System/Correspondent -> Receiver Bank. Standard model. Correspondent banking is often a chain of 4-corner models linked together."
Day0_CorrBanking_Gemi,"How would you manage 'Currency Holidays' (e.g., USD holiday, EUR open) in cross-currency payments?","Logic: 1. Check Calendar for Ccy Pair. 2. If USD is closed, Spot Date moves to T+3. 3. Execution: You can book the deal, but Settlement instructions must date-roll to the next valid business day for *both* currencies (or split settlement if using non-CLS)."
Day0_CorrBanking_Gemi,"Design an 'Auto-Repair' logic for SWIFT messages with missing 'Bank Operation Codes'.","1. Analyze: If `pacs.008`, check 'Category Purpose'. 2. Rule: If missing, default to `CRED` (Standard Credit Transfer) for retail, or `SPAY` for salary if keywords found. 3. Apply: Update message. 4. Log: 'Auto-repaired Op Code'. Caution: Do not auto-repair critical parties or amounts."
Day0_CorrBanking_Gemi,"What is the impact of 'PSD2' on Correspondent Banking operations outside the EU?","Transparency: Payments *from* EU to Non-EU (One-leg out) now require full transparency on charges/deductions. Speed: Value dating rules apply. Impact: Non-EU correspondents interacting with EU banks must provide clearer fee schedules and adhere to stricter data passing (Travel Rule)."
Day0_CorrBanking_Gemi,"How do you handle 'Sanctioned Country' addresses appearing in `Unstructured` fields during migration?","Risk: Legacy data often hides 'Cuba' or 'Iran' in free text. Migration: 1. Parse Unstructured text. 2. Attempt to structure (City, Country). 3. Screen: Run rigorous screening on the parsed text. 4. Fail-Safe: If structuring fails, treat the whole blob as high-risk and manual review. Do not just truncate."
Day0_CorrBanking_Gemi,"Explain the role of 'RMA Plus' in SWIFT.","Granular Control: Standard RMA is 'Yes/No' for messages. RMA Plus allows restricting *which* messages (e.g., Accept MT103 but Block MT202) and *currencies* can be exchanged. Use: Restrict a Respondent to only Customer Payments (Low risk) and block Bank-to-Bank payments (High risk)."
Day0_CorrBanking_Gemi,"Design a 'Liquidity Buffer' policy for a volatile Nostro account.","Policy: Target Balance = Forecasted Outflows + Buffer. Buffer Calc: `StdDev(Daily_Outflows) * 2` (95% confidence). Review: Monthly. Logic: If Balance < Buffer, auto-trigger funding request from Treasury. Reduces risk of failed payments due to unexpected spikes."
Day0_CorrBanking_Gemi,"How would you process a 'Return' (pacs.004) where the FX rate has moved significantly?","Original: Sent $100 -> £80. Return: £80 -> $95 (USD stronger). Loss: $5. Policy: 1. Return the foreign amount (£80). 2. Apply *current* spot rate. 3. Credit Originator $95. 4. Notify: 'Less amount returned due to FX movement'. Do not absorb FX loss unless it was a Bank Error."
Day0_CorrBanking_Gemi,"Explain the 'Dedication' of a Nostro account for 'Client Money' (CASS rules).","Segregation: Account is named 'Bank ABC - Client Segregated'. Usage: Only client funds, no house funds. Liquidity: Cannot be used to offset house overdrafts. Reporting: Strict daily reconciliation required by regulator (FCA). insolvency: Protected from bank creditors."
Day0_CorrBanking_Gemi,"Design a 'Reference Data' architecture for routing payments.","1. Sources: SWIFT Ref, Bankers Almanac, Local Clearing Directories. 2. Master: 'Golden Copy' routing DB. 3. Logic: `Country` + `Currency` -> Priority List of Correspondents. 4. Attributes: Cut-off time, Cost, Quality Score. 5. API: Payment Engine calls `GetRoute(USD, BR)` -> Returns `JPMC NY`."
Day0_CorrBanking_Gemi,"How does 'Blockchain/DLT' (Ripple/Partior) attempt to solve Correspondent Banking inefficiencies?","1. Ledger: Shared ledger replaces bilateral Nostro/Vostro recs (Single source of truth). 2. Liquidity: Atomic Settlement (Tokenized cash) eliminates pre-funding/trapped cash. 3. Speed: 24/7 vs Banking Hours. Challenge: Liquidity fragmentation and adoption by incumbent banks."
Day0_CorrBanking_Gemi,"Explain the 'Non-STP' fee charged by correspondents.","Fee applied when a payment requires manual repair (e.g., invalid format, missing IBAN). Strategy to avoid: Implement strict Validation Rules at the Point of Capture (e-Banking) to ensure clients cannot send non-compliant instructions."
Day0_CorrBanking_Gemi,"Design a 'Payment Status Tracker' for corporate clients using SWIFT gpi data.","1. Subscribe: Listen to gpi Tracker API / MT199 updates. 2. Store: DB table `Tx_Status_Log` (UETR, Bank, Status, Time, Deductions). 3. UI: Timeline view 'Sent -> Intermediary (Fee $5) -> Credited'. 4. Push: Webhook to Client ERP 'Payment Credited'."
Day0_CorrBanking_Gemi,"How do you handle 'Case Management' for inquiries when the Correspondent uses a different Time Zone?","Async Workflow. 1. Create Case. 2. Send SWIFT Inquiry. 3. SLA Timer: Pause (awaiting external). 4. Ops Handoff: 'Follow-the-Sun' model (London hands off to Singapore) to chase responses during the Correspondent's working hours."
Day0_CorrBanking_Gemi,"Explain the risk of 'Split Settlement' (sending 2 payments to settle 1 deal).","Ops Risk: Double work. Cost: Double fees. Recs: Harder to match (1 deal = 2 cash lines). Fraud: One might be genuine, one fake. Policy: Enforce Netting or Single Settlement instructions where possible."
Day0_CorrBanking_Gemi,"Design a 'Compliance Check' pre-processor for Nostro transfers.","Before generating `pacs.009`: 1. Check Balance. 2. Check Limits (Counterparty Credit Limit). 3. Check Sanctions (Beneficiary Bank). 4. Check Internal Blacklist. 5. If Pass -> Sign & Send. If Fail -> Queue for Compliance Review."
Day0_CorrBanking_Gemi,"What is the impact of 'Instant Payments' on Nostro Liquidity Management?","Volatility: Outflows happen 24/7, not just 9-5. Buffers: Need higher buffers for weekends/nights when funding markets are closed. Forecasting: Needs AI/ML to predict off-hours flows. Funding: Need access to 24/7 Liquidity sources (or Central Bank facilities)."
Day0_CoreOpenBank_Gemi,"Design a 'Sidecar' payments engine architecture around a legacy core. Explain the data flow for an inbound payment.","Architecture: Deploy modern Payment Engine (Sidecar) alongside Legacy Core. Use an API Gateway for ingestion. Data Flow: 1. Gateway receives payment. 2. Sidecar validates, performs sanction check, and updates 'Shadow Ledger' (real-time availability). 3. Sidecar emits 'PaymentPosted' event. 4. CDC/Integration Layer consumes event and updates Legacy Core (System of Record) via async queue or batch file (depending on legacy constraints). 5. Reconciliation job runs EOD to ensure Sidecar and Legacy balances match."
Day0_CoreOpenBank_Gemi,"Describe how you would evolve an existing Open Banking (PSD2) platform into an Open Finance platform under FiDA.","1. Data Layer: Implement Data Virtualization/ODS to aggregate non-payment data (Pensions, Insurance) from diverse legacy backends which lack APIs. 2. API Layer: Extend API Gateway to support new FiDA resource endpoints. 3. Consent: Upgrade Consent Management System to handle 'Coarse' vs 'Fine-grained' scopes suitable for complex financial products. 4. Analytics: Add capabilities to serve real-time valuation of volatile assets (investments) rather than just static balances."
Day0_CoreOpenBank_Gemi,"Explain how you would design an Embedded Finance integration for an e-commerce platform offering 'White-label Accounts', focusing on KYC and Settlement.","KYC: Implement 'Shadow KYC'. Platform collects docs via their UI and sends to Bank API. Bank verifies and returns a 'KYC_Cleared' token. No account is active without this token. Settlement: Use FBO Model. Platform has one Master Account. Bank issues vIBANs for Platform's users. Incoming funds hit Master Account; Bank webhook notifies Platform to update user's UI balance. Outgoing funds triggered by Platform API call, authorized by Platform's credentials."
Day0_CoreOpenBank_Gemi,"Design the 'Strangler Fig' migration strategy for moving 'Customer Data' from a Monolith to a Microservice.","1. Build 'Customer Microservice' with modern DB. 2. Implement 'Dual Write': Any change in Monolith is written to Microservice (via CDC). 3. Implement 'Read Routing': Point API Gateway to read from Microservice for 1% of traffic, falling back to Monolith on failure. 4. Ramp up Read traffic to 100%. 5. Switch 'Write Master': Point writes to Microservice, sync back to Monolith (for safety). 6. Decommission Monolith Customer module."
Day0_CoreOpenBank_Gemi,"Compare 'BaaS via API' vs 'BaaS via Core Access' for a Fintech partner. Discuss trade-offs in risk and speed.","BaaS via API: Bank exposes clean, limited endpoints (Accounts, Payments). High Speed, Low Risk (Bank controls logic/compliance). Limited flexibility for Fintech. BaaS via Core Access: Bank gives Fintech direct access to Core configuration (e.g., via a sandbox instance). Max Flexibility for Fintech to build novel products. High Risk (Bank must audit Fintech's configurations/code). Slower implementation due to compliance checks."
Day0_CoreOpenBank_Gemi,"How would you handle 'Token Revocation' in an Open Banking architecture where multiple TPPs hold long-lived tokens?","Implement a centralized 'Consent Dashboard' for the customer. When a user revokes consent for TPP A: 1. Dashboard calls Identity Provider (IdP). 2. IdP invalidates the specific Refresh Token associated with TPP A. 3. Publish 'TokenRevoked' event. 4. API Gateway subscribes to event and blocks future calls from TPP A for that user immediately (Blacklist cache)."
Day0_CoreOpenBank_Gemi,"Design a 'Real-time Ledger' for a neo-bank sidecar that needs to handle high-frequency transaction spikes.","Use an 'Event Sourced' architecture. 1. Ingest transactions as immutable events into a log (e.g., Kafka). 2. 'Projections' consume the log to update an in-memory or KV-store database (e.g., Redis/Cassandra) for read-heavy 'Current Balance' queries. 3. Consistency is 'Eventual' (milliseconds). 4. Use optimistic locking or single-writer principle per account to handle concurrency."
Day0_CoreOpenBank_Gemi,"Explain the 'Data Ownership' and 'Liability' model when a SaaS platform offers Embedded Lending powered by your bank.","Data Ownership: Bank owns the Credit Decisioning data and Regulatory Reporting data (legal requirement). Platform owns the Customer Relationship/UX data. Liability: Bank is liable for regulatory compliance (Fair Lending, AML). Platform is liable for 'First Line' fraud detection (e.g., account takeover) and potentially shares credit risk (First Loss Default Guarantee) via contractual indemnification."
Day0_CoreOpenBank_Gemi,"How would you architect a solution to handle 'Variable Recurring Payments' (VRP) mandates in the payment engine?","1. Mandate Store: Create a database for long-lived consents (Amount Limit, Frequency, Expiry). 2. Auth Logic: When a VRP payment request arrives, API Gateway validates the TPP's Access Token against the Mandate Store. 3. Limit Check: Ensure cumulative amount < Limit. 4. Execution: If valid, route to Payment Engine as an internal transfer. 5. Notification: Push alert to customer app."
Day0_CoreOpenBank_Gemi,"Design a 'Hybrid Cloud' networking strategy for a 4th Gen Core (Cloud) connecting to a Legacy Mainframe (On-Prem).","1. Connectivity: Establishing AWS Direct Connect / Azure ExpressRoute for low-latency, private, dedicated bandwidth. 2. Security: Mutual TLS (mTLS) for all traffic. 3. API Gateway: Deploy a Gateway in the Cloud DMZ to terminate external traffic. 4. Reverse Proxy: Deploy an Internal Gateway On-Prem to police traffic entering the Mainframe zone. 5. Latency Management: Use caching at the Cloud edge to minimize Mainframe round-trips."
Day0_CoreOpenBank_Gemi,"Describe the 'Change Data Capture' (CDC) pipeline to sync a Legacy Core DB (DB2) with a Modern Data Lake (Snowflake) for Analytics.","1. Source: Enable transaction logging on DB2. 2. Ingest: Use a CDC tool (e.g., Qlik, Debezium) to tail the logs. 3. Stream: Push changes to Kafka topics (one per table). 4. Sink: Use a Kafka Connect sink to write micro-batches to Cloud Object Storage (Bronze). 5. Transform: Spark/Snowpipe merges changes into the Snowflake Data Warehouse tables (Silver/Gold), handling updates/deletes."
Day0_CoreOpenBank_Gemi,"Explain how you would ensure 'Idempotency' in a Payment Initiation API exposed to Third Parties.","1. Requirement: TPP must send unique `Idempotency-Key` header. 2. Gateway Layer: Hash the Key + Payload. Check Distributed Cache (Redis). 3. Logic: If Key exists and processing is active -> Return 429/Wait. If Key exists and processing complete -> Return saved 200 Response. If Key new -> Process. 4. Persistence: Store keys for 24-48 hours."
Day0_CoreOpenBank_Gemi,"Design the 'Compliance Firehose' for an Embedded Finance partner who manages their own customer UI.","Since the Partner manages UI, they see the IP/Device data. 1. API Contract: Require Partner to pass 'Risk Context' headers (IP, DeviceID, Geo) with every API call. 2. Validation: If Context missing, reject call. 3. Ingestion: Bank feeds this data into the Transaction Monitoring System (TMS) in real-time. 4. Feedback: If TMS flags fraud, Bank pushes a webhook to Partner to freeze the user."
Day0_CoreOpenBank_Gemi,"How would you handle 'API Versioning' for a public Open Banking API to avoid breaking TPP integrations?","1. Strategy: Use URI Versioning (`/v1/payments`, `/v2/payments`). 2. Policy: Support N-1 versions. 3. Communication: Announce deprecation 6 months in advance. 4. Implementation: Use API Gateway to route `/v1` calls to a 'Legacy Adapter' service and `/v2` calls to the 'Modern Service'. 5. Sunset: Hard block old versions after grace period."
Day0_CoreOpenBank_Gemi,"Design a 'Disaster Recovery' plan for a Cloud-Native Core Banking System.","1. Architecture: Multi-Region Active-Passive. 2. Data: Real-time asynchronous replication of Database/Event Store to Region B. 3. Compute: Infrastructure-as-Code (Terraform) ready to spin up stateless services in Region B. 4. Traffic: DNS Failover (Route53) to switch traffic. 5. RPO/RTO: Target RPO < 5 mins (data loss), RTO < 1 hour (downtime). 6. Testing: Quarterly 'Game Days'."
Day0_CoreOpenBank_Gemi,"Explain the role of 'Micro-Frontends' in a modern banking portal architecture.","Decompose the Monolithic Web App into features (e.g., 'Payments Widget', 'Account List', 'Profile'). Each widget is owned/deployed by a separate squad. A 'Container App' composes them at runtime. Benefit: The Payments squad can release a new feature without coordinating with the Profile squad, increasing velocity."
Day0_CoreOpenBank_Gemi,"How would you secure 'Service-to-Service' communication within a Microservices Core?","1. Protocol: mTLS (Mutual TLS) for encryption and identity verification. 2. Management: Use a Service Mesh (e.g., Istio/Linkerd) to manage certificates and rotation automatically. 3. AuthZ: Implement 'Zero Trust' policies (OPA - Open Policy Agent) defining which service can call which (e.g., 'Only PaymentService can call LedgerService')."
Day0_CoreOpenBank_Gemi,"Design a 'Testing Strategy' for a Bank exposing APIs to external developers.","1. Sandbox: Provide a publicly accessible Sandbox with mock data. 2. Contract Testing: Publish Swagger/OpenAPI specs. 3. Certification: Require TPPs to pass a 'Conformance Suite' (automated tests running against the Sandbox) before granting Production access. 4. Production: Use Canary releases to test internal changes before they hit all TPPs."
Day0_CoreOpenBank_Gemi,"Explain how 'Smart Routing' in a Payments Sidecar can optimize costs.","Sidecar holds logic: 1. Analyze Payment (Amount, Dest, Currency, Urgency). 2. Look up Cost Table (SEPA Inst vs SWIFT vs Ripple). 3. Rule: If < €15k and within EU -> SEPA Inst (€0.02). If Urgent International -> SWIFT gpi. If non-urgent -> Batch SEPA. 4. Execution: Route payload to appropriate gateway adapter."
Day0_CoreOpenBank_Gemi,"Design the 'Account Number Generation' logic for a multi-tenant Core Banking System.","1. Structure: Country Code + Check Digits + Bank Code + Branch Code + Serial. 2. Partitioning: Pre-allocate ranges of Serial numbers to different 'Shards' or 'Tenants' to avoid contention. 3. Uniqueness: Use a central atomic counter service (e.g., Zookeeper/Database Sequence) or UUIDs mapped to short-codes. 4. Validation: Ensure check-digit algorithm (Mod97) passes."
Day0_CoreOpenBank_Gemi,"How would you implement 'Immutable Audit Logs' for a core banking ledger?","1. Storage: Write logs to WORM (Write Once Read Many) storage (e.g., S3 Object Lock). 2. Cryptography: Chain log entries with hashes (Merkle Chain/Blockchain logic) so modification breaks the chain. 3. Access: Restrict read access to Auditors/Security team. 4. Verification: Automated nightly jobs verify the hash chain integrity."
Day0_CoreOpenBank_Gemi,"Design a 'Consent Revocation' flow for an Open Finance aggregation use case.","1. User Action: User clicks 'Revoke' in Bank App. 2. Bank System: Updates Consent Database status to 'Revoked'. 3. Notification: Bank sends webhook to TPP 'Consent Revoked'. 4. Enforcement: API Gateway immediately rejects incoming calls using Tokens associated with that Consent ID. 5. TPP Action: TPP must delete cached data (regulatory requirement)."
Day0_CoreOpenBank_Gemi,"Explain the 'CQRS' (Command Query Responsibility Segregation) pattern in a high-volume core.","Split the application into two parts: 1. Command Side (Write): Optimized for transactional integrity (processing payments). Writes to Event Store. 2. Query Side (Read): Optimized for reading (viewing balance/history). Subscribes to events and updates a denormalized Read DB (e.g., Mongo/Elastic). Allows scaling reads independently of writes."
Day0_CoreOpenBank_Gemi,"How would you handle 'Schema Evolution' in an Event-Sourced Core?","Problem: Old events have `structure V1`, new code expects `V2`. Strategy: 1. Upcasting: When reading events, a translation layer converts V1 events to V2 format on-the-fly. 2. Versioning: Add version number to event payload. 3. Lazy Migration: Only rewrite events if absolutely necessary (avoid if possible). Code must handle multiple event versions."
Day0_CoreOpenBank_Gemi,"Design a 'Partner Portal' for an Embedded Finance bank.","Features: 1. API Key Management (Create/Rotate). 2. Webhook Configuration. 3. Sandbox Access. 4. Analytics Dashboard (API usage, Error rates). 5. Documentation Hub (Interactive OpenAPI specs). 6. Support Ticketing. Goal: Self-service onboarding to reduce Ops load."
Day0_CoreOpenBank_Gemi,"Explain 'Distributed Tracing' utility in a microservices banking architecture.","Problem: A payment fails, but logs are scattered across 20 services. Solution: Implement OpenTelemetry/Jaeger. 1. Ingress: Assign a unique `TraceID` at the Gateway. 2. Propagation: Pass `TraceID` in headers to every downstream service. 3. Visualization: Use a UI to see the full waterfall of the request, identifying exactly which microservice caused the latency or error."
Day0_CoreOpenBank_Gemi,"How would you implement 'Offline Capability' for a banking app while maintaining ledger integrity?","1. Read: Cache last known balance/txns locally. 2. Write: Queue actions (e.g., 'Pay Bill') in local outbox. 3. Sync: When online, replay actions to server. 4. Conflict: If server rejects (e.g., Insufficient Funds), push notification to user. *Never* calculate authoritative balance on device."
Day0_CoreOpenBank_Gemi,"Design a 'Rate Limiting' strategy for Open Banking APIs to prevent service degradation.","1. Levels: Global Limit (protect system), Per-TPP Limit (fairness), Per-User Limit (abuse). 2. Tech: Use Token Bucket algorithm in API Gateway (Redis backed). 3. Response: Return `429 Too Many Requests` with `Retry-After` header. 4. Monetization: Offer higher rate limits to Premium TPP partners."
Day0_CoreOpenBank_Gemi,"Explain the 'Backend-for-Frontend' (BFF) pattern for a bank supporting Web, iOS, and Android.","Instead of one generic API, build a specific API layer for each client. 1. iOS BFF: Formats data for iOS UI, handles iOS specific auth. 2. Web BFF: Optimize for browser caching. Benefit: Decouples UI requirements from Core Domain logic. Mobile team can iterate BFF without waiting for Core team."
Day0_CoreOpenBank_Gemi,"How would you architect a 'Data Lakehouse' for Regulatory Reporting?","1. Ingestion: Raw data (Bronze) landed in Cloud Storage. 2. Processing: Spark/Delta Lake jobs clean and validate (Silver). 3. Aggregation: Business logic applies regulatory rules (Gold). 4. Serving: BI Tools query Gold tables directly via SQL endpoint. Benefit: Combines low cost of Lake with ACID transactions/Schema enforcement of Warehouse."
Day0_CoreOpenBank_Gemi,"Design a 'Feature Flag' system for a core banking rollout.","1. Tool: Use LaunchDarkly/Split. 2. Implementation: Wrap new code (e.g., 'New Interest Calc') in conditional blocks. 3. Rollout: Enable for 'Internal Staff' -> 'Beta Customers' -> 'Region A' -> '100%'. 4. Kill Switch: If errors spike, instantly disable flag to revert to old logic without redeploying code."
Day0_CoreOpenBank_Gemi,"Explain 'Contract Testing' (PACT) for Microservices.","1. Consumer (e.g., Mobile App) defines expectations (I need field 'balance' as string). 2. Provider (Account Service) verifies it meets these expectations during its build pipeline. 3. Benefit: Prevents breaking changes deploying to Prod. If Account Service renames 'balance' to 'bal', the build fails."
Day0_CoreOpenBank_Gemi,"How would you handle 'Time Zones' in a Global Core Banking System?","1. Storage: Store all timestamps in UTC in the DB. 2. Processing: Core logic operates in UTC. 3. Product Config: Define 'Cut-off times' in local time but convert to UTC for execution. 4. Presentation: Convert to User's Local Time only at the UI/Statement rendering layer."
Day0_CoreOpenBank_Gemi,"Design an 'Alerting Strategy' for a Payment Gateway.","1. Metrics: Monitor 'Success Rate', 'Latency', 'Error Codes'. 2. Anomaly Detection: Alert if Success Rate drops > 5% vs same time last week. 3. Routing: Alert specific teams (e.g., 'Connection to Visa Down' -> Ops, '500 Errors' -> Dev). 4. Severity: P1 (Wake up), P3 (Next Business Day)."
Day0_CoreOpenBank_Gemi,"Explain the 'Saga Pattern' for a distributed money transfer (Debit Sender, Credit Receiver).","Transaction spans two services. 1. Choreography: Debit Service succeeds -> Publishes 'Debited' Event -> Credit Service consumes. 2. Compensation: If Credit Service fails -> Publishes 'CreditFailed' Event -> Debit Service consumes -> Executes 'Refund' (Compensating Transaction) to undo the debit. Ensures eventual consistency."
Day0_CoreOpenBank_Gemi,"How would you architect 'Search' for a banking app (Find transaction 'Coffee')?","Do not search the Core DB (slow). 1. Pipeline: CDC stream from Core DB -> Elasticsearch/OpenSearch. 2. Query: App calls Search Service -> Queries Elastic. 3. Security: Ensure Elastic index respects user isolation (only search own data)."
Day0_CoreOpenBank_Gemi,"Design a 'Customer 360' view aggregator.","1. Sources: Core (Accounts), CRM (Interactions), Cards (Txns), Mortgage (Loans). 2. Integration: GraphQL API Gateway. 3. Caching: Cache the aggregated view in Redis with short TTL. 4. Logic: GraphQL resolvers fetch data in parallel from sources. 5. Handling Failure: If CRM is down, return partial response (Accounts only) rather than error."
Day0_CoreOpenBank_Gemi,"Explain 'Secret Management' in a Kubernetes-based banking cluster.","1. Storage: Use Vault/AWS Secrets Manager. 2. Injection: Inject secrets (DB passwords, API keys) as environment variables or mounted volumes at runtime. 3. Rotation: Automate key rotation. Apps should reload config on change or restart. Never commit secrets to Git."
Day0_CoreOpenBank_Gemi,"How would you implement 'Predictive Scaling' for a core banking system?","1. Data: Analyze historical traffic patterns (e.g., Payday spikes). 2. Schedule: Pre-scale clusters (increase min instance count) 1 hour before expected spike. 3. Reactive: Configure HPA (Horizontal Pod Autoscaler) to scale based on CPU/Memory/Request Queue depth for unexpected loads."
Day0_CoreOpenBank_Gemi,"Design a 'Developer Portal' experience for Open Banking TPPs.","1. Onboarding: Automated Identity Verification. 2. App Creation: Generate Client ID/Secret. 3. Documentation: Interactive 'Try it out' API explorer. 4. Sandbox: One-click creation of test users/accounts. 5. Support: Chatbot/Community Forum. Goal: Reduce 'Time to First Hello World' to < 15 mins."
"Day0_CoreOpenBank_Perp","Design a sidecar payments engine around a legacy core and explain data contracts and reconciliation","Describe a payments service that receives payment requests from channels and first calls the legacy core via a reserve funds API using idempotent correlation ids then executes on the chosen rail and finally posts confirmed settlements back to the core or GL through a post transaction interface Define data contracts that specify required fields such as account ids currency and value date and publish events like PaymentExecuted to a bus Reconciliation compares sidecar ledgers core balances and scheme statements daily to ensure no divergence"
"Day0_CoreOpenBank_Perp","Explain how you would decide which products to move first when hollowing out a monolithic core","State that you would prioritise products with high change pressure or differentiation such as instant payments or digital lending but relatively clean boundaries and low entanglement Analyse dependencies and risk appetite then target those domains for new services while leaving deeply complex or low change areas like some legacy savings portfolios for later migration using metrics on incidents and change backlog to justify the sequence"
"Day0_CoreOpenBank_Perp","Describe how you would evolve an Open Banking AIS PIS platform into an Open Finance platform under FiDA","Explain that you would extend the API gateway and consent system to cover new product types beyond payment accounts add product specific schemas and endpoints for savings investments pensions and insurance and integrate with additional back end systems or third party providers while keeping common security and consent semantics Provide a unified developer experience and update customer UX so they can see and manage consents across all product categories"
"Day0_CoreOpenBank_Perp","How would you design a consent management system that supports multiple TPPs and scopes in Open Banking","Propose a central consent service that stores consents as records keyed by customer TPP and scope with expiry dates and revocation flags Expose APIs for creating revoking and querying consents integrate with SCA flows for initial grant and ensure API gateway and downstream services check consent state and scope on each request Log all access decisions for audit and enable self service revocation in digital channels"
"Day0_CoreOpenBank_Perp","Explain how you would structure API versioning for a public Open Banking API","Describe using semantic versioning with separate base paths for major versions such as v1 and v2 and clear deprecation policies minor changes that are backwards compatible are added within a version while breaking changes trigger a new major version Provide version negotiation documentation and timelines so TPPs can migrate safely without sudden disruptions"
"Day0_CoreOpenBank_Perp","Design an architecture that lets a fintech aggregator pull account data from multiple banks and also initiate payments via PIS","Suggest using a multi bank connectivity layer that integrates with various banks Open Banking APIs normalising AIS and PIS responses into a canonical model The aggregator stores only consented data with clear mappings and routes payment initiations through the relevant bank PIS endpoints with SCA handled at each bank All calls go through a secure API gateway enforcing consent scopes and logging for each bank relationship"
"Day0_CoreOpenBank_Perp","Describe how you would expose lending as building block APIs to an embedded finance partner","Propose APIs for prequalification and eligibility quoting scoring and final loan creation plus endpoints for disbursements and repayments Ensure each API has clear request response schemas including KYC and financial data and that approvals result in loans booked on the bank side with events published for servicing The partner receives tokens or ids referencing loans but does not manipulate internal ledgers directly"
"Day0_CoreOpenBank_Perp","Explain how you would manage risk and limits for an embedded BNPL product on an ecommerce site","Describe defining credit policies and risk models executed by the bank's BNPL engine on each checkout data request and setting per customer and per merchant limits using both platform and bank data Monitor performance by segment and adjust limits dynamically while ensuring that agreements terms and disclosures meet regulatory requirements and that defaults are visible in credit and collections systems"
"Day0_CoreOpenBank_Perp","Design data flows to ensure AML systems receive full visibility of embedded finance transactions","Explain that every embedded transaction should generate events carrying key AML attributes such as customer ids counterparties amounts and channels which flow into the bank's central monitoring systems regardless of which platform initiates them Ensure that BaaS or partner systems provide structured data feeds into the bank via events or APIs and that transactions are tagged with platform ids to support segmentation and typology design"
"Day0_CoreOpenBank_Perp","How would you enforce data minimisation for AIS data in a data lake used for analytics","Propose enforcing schema level controls that only ingest fields covered by consent and configured use cases applying tokenisation or aggregation where detailed data is not necessary Implement retention policies that purge data after consent expiry or defined windows and use access controls and column level security so analysts see only datasets relevant to their authorised use"
"Day0_CoreOpenBank_Perp","Describe a migration approach where new current accounts are opened on a cloud native core while old ones stay on mainframe","Explain that you would route new account opening flows via the routing layer to the new core and assign them a specific id pattern while existing accounts remain on mainframe Serving and posting for legacy accounts continues on mainframe while all operations on new accounts go to the new core Reporting and AML combine data from both using canonical models until legacy accounts are migrated or run off"
"Day0_CoreOpenBank_Perp","Explain how you would integrate a cloud native core with an existing GL system during a transition period","Describe using an integration layer that translates domain events and postings from the new core into GL journal entries matching existing chart of accounts and posting rules For the legacy core keep existing GL feeds running in parallel and ensure that postings from both cores reconcile to consolidated GL balances with clear tags indicating source system"
"Day0_CoreOpenBank_Perp","Design an approach for handling idempotency in payment initiation APIs used by embedded partners","Propose requiring partners to provide an idempotency key per operation and storing keys along with outcomes so that repeated requests with the same key return the original result and do not trigger duplicate payments Define key scopes such as per customer or per partner and document behaviours for retries and timeouts"
"Day0_CoreOpenBank_Perp","How would you segregate responsibilities between bank and platform for customer support in an embedded finance model","Explain that the platform would handle first line support and basic inquiries while the bank remains responsible for regulatory communications transactional disputes and formal complaints Include clear escalation paths SLAs and scripts so platform staff know when to hand off to the bank and ensure that both parties maintain consistent records for audit"
"Day0_CoreOpenBank_Perp","Describe how you would design APIs to surface pension data under an Open Finance regime while managing complexity","Suggest endpoints that present high level views of pension entitlements contributions and projections with underlying detailed data accessible through additional calls Use clear domain models for schemes contributions and benefits and avoid over exposing raw technical records while still enabling accurate projections and portability features"
"Day0_CoreOpenBank_Perp","Explain the trade offs between building your own Open Banking connectivity and using an aggregator","State that building direct connections offers more control and potentially lower dependency risks but requires significant maintenance across many banks and standards Aggregators provide faster time to market and unified APIs but add an extra layer of dependency and cost Architects must consider regulatory requirements resilience and vendor risk when choosing"
"Day0_CoreOpenBank_Perp","Design a monitoring dashboard for core modernisation progress when using strangler pattern","Describe showing metrics such as percentage of new accounts opened on new core share of payments processed through sidecars incident rates per domain performance comparisons and a view of remaining legacy functionality Provide drill downs per product and region plus risk indicators showing dependencies still tied to the monolith"
"Day0_CoreOpenBank_Perp","How would you ensure that new domain services remain product agnostic enough to support Open Finance","Argue that services should model generic capabilities such as holdings policies and accounts with product specialisations layered via configuration and reference data rather than hard coding a single scheme type This supports adding new products like different pension schemes without major code changes while still meeting regulatory nuances through parameterisation"
"Day0_CoreOpenBank_Perp","Describe a high level architecture for a developer portal exposing Open Banking and embedded finance APIs","Propose an API gateway fronting all services integrated with an identity provider and consent system and a portal providing documentation sandbox access keys and analytics Developers authenticate to the portal to obtain credentials and test endpoints while underlying routing sends calls to production or sandbox clusters with proper rate limiting and monitoring"
"Day0_CoreOpenBank_Perp","Explain how you would minimise customer impact when introducing SCA into existing mobile and web journeys","Suggest leveraging existing device binding and biometrics to fulfil SCA with minimal new steps and integrating 3DS2 or equivalent flows so challenges are contextual and in app Use exemptions intelligently based on risk and transaction type and phase rollout with clear communication and A B testing to reduce abandonment"
"Day0_CoreOpenBank_Perp","Design the data architecture required to support cross product analytics across accounts savings and investments under Open Finance","Recommend a canonical customer and product model in a central data platform where each transaction from accounts savings and investments is normalised and linked via stable customer and account ids Include dimensions for product type risk level and tax treatment so analytics can evaluate customer financial health and cross sell opportunities"
"Day0_CoreOpenBank_Perp","How would you control API abuse and rate limiting for third party access under Open Banking","Describe implementing per TPP rate limits and quotas at the API gateway with thresholds by endpoint and customer plus anomaly detection on traffic patterns Provide clear error messaging and a mechanism to request higher limits with justification and log all rate limiting events for review and potential blocking of abusive clients"
"Day0_CoreOpenBank_Perp","Explain how you would design an insurance embedded offering within an ecommerce checkout","Propose integrating an insurance quote API that uses basket details and customer data to return tailored offers The customer accepts coverage within the checkout and the platform posts a bind request to the insurer or bank insurance engine which creates a policy and provides confirmation Store policy ids in order records and ensure claims and regulatory documents are delivered to the customer under agreed responsibilities"
"Day0_CoreOpenBank_Perp","Describe an approach to keep legacy batch based reporting working while migrating to event driven architecture","Explain that you would capture events into a streaming platform and also write them into an operational data store or stage tables from which existing batch jobs can run unchanged while gradually refactoring reports to consume curated warehouse tables built from events Over time legacy extracts can be retired as confidence in event based reporting grows"
"Day0_CoreOpenBank_Perp","How would you test end to end flows that span legacy core and new sidecar services","Outline a testing strategy that includes contract tests for APIs integration tests with synthetic end to end transactions through routing layers and sidecars and reconciliation checks verifying that resulting postings match expected balances and GL entries Also include failure scenario tests such as timeouts and partial outages to ensure resilience and idempotency behaviour"
"Day0_CoreOpenBank_Perp","Design governance to stop teams bypassing the routing layer when integrating new products","Suggest enforcing architecture standards that require new integrations to use published APIs and routing components with reviews by an architecture board Monitor traffic patterns to detect direct core access and restrict direct database connections with technical controls and change management gates"
"Day0_CoreOpenBank_Perp","Explain how you would adapt SLOs for core services after moving to cloud native","Describe defining SLOs for availability latency and error rates at service and API levels and using monitoring tools to track them in real time Build error budgets into planning so teams can balance new features with stability work and adjust capacity based on scale events and regional variations"
"Day0_CoreOpenBank_Perp","Describe how you would handle customer data residency requirements when designing a global Open Finance platform","Propose deploying regional data clusters in jurisdictions with residency rules ensuring PII and account data remain within region while exposing controlled global aggregates or anonymised analysis Federate identity and consent across regions and route API calls to the appropriate regional backend based on the customer's residency and regulatory context"
"Day0_CoreOpenBank_Perp","How would you determine whether to place a new SME lending product on a sidecar or directly in the existing core","Explain that you would assess time to market integration complexity and long term strategy if the core is rigid and you plan broader modernisation a sidecar SME lending engine may be preferable while if the core supports modular add ons and servicing is tightly coupled it may be more efficient to implement within the core while still exposing it via APIs"
"Day0_CoreOpenBank_Perp","Design a scheme for tracking which platform initiated which embedded finance transactions for reporting and risk","Suggest adding a platform id or partner id field to all embedded transactions and including it in events and ledger postings Use this to segment reports sanctions and fraud metrics and to apply partner specific limits and pricing thereby enabling the bank to measure performance and risk per partner"
"Day0_CoreOpenBank_Perp","Explain how you would size and plan capacity for an event driven core that feeds many downstream consumers","Describe estimating base event volumes and peak factors then designing topics and partitions accordingly Plan consumer groups for AML reporting and analytics with clear SLAs and ensure horizontal scaling of brokers and consumers with back pressure handling so that spikes in one domain do not impact critical consumers like fraud or ledger updates"
"Day0_CoreOpenBank_Perp","Describe how you would handle revocation of Open Banking consents that are cached in third party systems","Explain that while the bank can immediately block API access upon revocation it should also provide webhook or polling mechanisms to notify TPPs of revoked consents so they stop storing or using cached data beyond what regulations allow and update their own views accordingly"
"Day0_CoreOpenBank_Perp","How would you justify investment in a unified product and consent layer to executives","Argue that a unified layer reduces duplication across channels and markets simplifies compliance with evolving regulations like FiDA and EU AI data rules and accelerates launch of new products and partnerships by providing consistent APIs and controls reducing integration cost and regulatory risk"
"Day0_CoreOpenBank_Perp","Design a roadmap to transition from screen scraping based fintech access to full Open Banking APIs","Outline steps including launching secure APIs implementing consent management working with major TPPs to migrate integrating SCA and gradually blocking screen scraping based on risk and contractual agreements while monitoring impact and providing support to developers"
"Day0_CoreOpenBank_Perp","Explain how you would structure documentation for external partners consuming your embedded finance APIs","Suggest providing OpenAPI specs domain guides sequence diagrams for key flows sandbox usage instructions and clear sections on authentication consent error codes and rate limits Also maintain a changelog and deprecation schedule so partners can plan upgrades and assign partner managers to support integration"
"Day0_CoreOpenBank_Perp","Summarise how core modernisation Open Finance and embedded finance mutually reinforce each other in a target architecture","Explain that a modular cloud native core and domain services provide the agility and APIs needed for Open Banking and Open Finance while those same APIs enable embedded finance partnerships Data streams from these services feed analytics and compliance platforms which in turn support safer product innovation and regulatory alignment creating a virtuous cycle of faster change with controlled risk"
"Day0_AML_Sanctions_Perp","Design an AML monitoring stack that supports Perpetual KYC for retail and SME customers","Describe a layered architecture where transactional and customer events flow into a streaming platform and are consumed by TM rules engines anomaly detection models and segmentation services Risk scores for customers are stored in a central risk profile service linked to KYC records and any material change from alerts list updates or external data triggers KYC reviews in case management Dashboards for AML and business lines show current risk ratings and recent triggers while periodic formal reviews become a check on a continuously updated profile rather than the only source of change"
"Day0_AML_Sanctions_Perp","Explain how you would integrate graph analytics into sanctions screening to detect indirect exposure","Explain that you would load customer counterparties corporate ownership and sanctions list entities into a graph database with nodes for people companies and accounts and edges for ownership and transactional links Define rules that flag nodes within certain path lengths or ownership thresholds from sanctioned nodes and expose these flags to the screening engine so hits are generated not only on direct name matches but also on indirectly controlled entities Investigators would see the relationship path in their tools for context"
"Day0_AML_Sanctions_Perp","Describe how you would source validate and use UBO data in transaction monitoring scenarios","Describe sourcing UBO data from customer declarations corporate registries and third party databases then representing ownership structures in a structured model with confidence scores Validate inconsistencies through enhanced due diligence and record final UBOs and their attributes such as nationality PEP status and risk country Then use these UBO fields as features in TM rules and models so transactions involving entities with high risk UBOs receive more scrutiny even if account holders appear benign"
"Day0_AML_Sanctions_Perp","Design a data model to support both sanctions screening and pKYC using ISO 20022 payments data","Propose a fact_payment table with payer and payee party ids amounts corridors and UETR plus dimensions for party customer and UBO that include names identifiers addresses risk ratings and sanctions flags Store sanctions screening results and TM alerts as separate fact tables linked via payment id and party id and ensure that updates to sanctions or KYC data can be re applied to historical payments when lists change while pKYC services read from these tables to refresh risk"
"Day0_AML_Sanctions_Perp","Explain how you would build feedback loops from investigators into AML models while maintaining governance","Propose capturing investigator decisions and key comments for each alert as structured labels in case management and exporting them regularly into a model training dataset Implement a model governance process where changes triggered by new labels are documented validated by independent teams and tested on out of time samples before deployment Maintain audit trails of model versions and performance so regulators can see how feedback has influenced detection over time"
"Day0_AML_Sanctions_Perp","Describe an approach to prioritise AML alerts using machine learning without removing investigator discretion","Explain that you would build a supervised model that assigns each alert a risk score based on past outcomes and features such as transaction history customer risk and typology indicators Alerts are still sent to investigators but queues are sorted so high score alerts are worked first The model does not auto close alerts instead it is used to optimise workload while investigators have the ability to override and their decisions feed back into model training"
"Day0_AML_Sanctions_Perp","How would you architect a system that supports both real time and batch sanctions screening on the same customer and transaction data","Explain that you would centralise customer and transaction data in a repository accessible to both a low latency screening component for real time checks and a batch engine for re screening when lists change The real time engine focuses on rapid name and context matching with limited graph depth while the batch engine can run deeper analyses and broad re screens Both share the same list management and configuration layer to maintain consistency"
"Day0_AML_Sanctions_Perp","Design a strategy to reduce sanctions false positives in a high volume retail payments environment while maintaining detection quality","Suggest improving data quality for names and addresses enabling structured fields in ISO 20022 using more sophisticated fuzzy matching tuned per language and adding contextual scoring based on country and customer risk Consider implementing list optimisation to remove non relevant entries and create white lists or reference data for low risk recurring counterparties while ensuring that any auto clear logic is well documented monitored and subject to periodic review"
"Day0_AML_Sanctions_Perp","Explain how you would integrate adverse media into Perpetual KYC workflows","Describe setting up a feed from adverse media providers that associates articles and risk scores with customer and UBO identifiers and routing this into the risk profile service When high severity hits appear they become KYC trigger events that prompt analysts to review the customer profile and potentially update risk rating or trigger account restrictions Make sure the integration includes deduplication and relevance filtering so that noise does not overwhelm staff"
"Day0_AML_Sanctions_Perp","Describe how you would use behavioural segmentation to improve AML model performance","Explain that you would group customers into segments based on features such as product usage geographical footprint transaction frequencies and counterparties then build or tune models separately for each segment This allows thresholds and features to reflect realistic behaviour for each group for example distinguishing typical salary earners from money service businesses so models are less likely to flag normal patterns as suspicious"
"Day0_AML_Sanctions_Perp","Design a target operating model where pKYC responsibilities are shared between AML and frontline relationship teams","Propose that AML owns the risk framework models and triggers while relationship teams are responsible for reviewing pKYC prompts for their customers updating business understanding and escalating concerns Create SLAs for response times and clear playbooks describing when front line input is sufficient and when full AML investigations are required and ensure both teams use a shared risk view and tooling"
"Day0_AML_Sanctions_Perp","Explain how you would handle regulatory expectations for explainability of an AI based AML transaction monitoring model","Describe choosing models and explanation tools that can produce human readable feature contributions for each alert and integrating these into investigator screens Document model purpose data sources features and training processes and maintain example cases where the model clearly aligned with typologies Provide regular reports to model risk and regulators showing global feature importance stability over time and how governance responds to drift"
"Day0_AML_Sanctions_Perp","Design a process for onboarding new sanctions lists and programmes into an existing screening system","Explain that you would implement a controlled pipeline where new lists are ingested validated and mapped into internal structures with clear tagging for issuing authority and programme Run impact assessments in a test environment measuring changes in hit volume and patterns then tune configurations such as matching rules and contextual scores before pushing to production with appropriate change control and communication to operations"
"Day0_AML_Sanctions_Perp","Describe how you would detect mule accounts using graph and behavioural analytics","Explain that you would look for accounts receiving funds from many unrelated sources and quickly passing them on particularly to high risk destinations or cash channels Graphs help identify clusters of accounts acting as hubs for such flows and time series analysis flags unusual spikes Investigators can then examine KYC data for inconsistencies and features such as device or IP sharing across apparently distinct customers"
"Day0_AML_Sanctions_Perp","Explain how you would manage model drift in an AML anomaly detection system","Describe monitoring performance metrics such as alert conversion rates and stability of feature distributions over time and implementing alerts when these deviate from baselines Periodically retrain models on more recent data if drift is confirmed following governance steps and compare new models against old on holdout samples to ensure improvements before deployment"
"Day0_AML_Sanctions_Perp","Design an intake and triage process for sanctions alerts that leverages both rules and ML scoring","Propose that real time screening creates initial hits based on tuned rules and fuzzy matching then a scoring layer ranks hits using contextual ML features such as corridor customer risk and history Low scoring hits may be auto cleared under strict rules or routed to low priority queues while high scoring hits go to specialised sanctions investigators with SLAs for quick decisions"
"Day0_AML_Sanctions_Perp","Describe how you would reconcile sanctions screening results from multiple systems after a merger","Explain that you would create a unified view of customers and counterparties mapping identifiers across both banks then run a consolidated screen using a single harmonised engine on the merged portfolio Compare historical hits and outcomes to identify gaps in either system and plan migration of all screening to a single platform with common lists and policies while maintaining parallel runs during transition for assurance"
"Day0_AML_Sanctions_Perp","Explain how you would build a cross border corridor risk model for AML using transaction data","Describe using historical ISO 20022 or SWIFT data to compute features such as typical amounts counterparties and customer segments per corridor and label cases with SAR outcomes Train models that predict riskiness of flows by corridor controlling for customer type and product Use outputs to refine rules set higher scrutiny for corridors with elevated risk and support management reporting on corridor level exposure"
"Day0_AML_Sanctions_Perp","Design an architecture to support multi jurisdictional AML and sanctions requirements in a global bank","Suggest a central data and analytics platform that harmonises core transaction and KYC data but supports jurisdiction specific rules configurations and thresholds per booking entity Implement local policy layers that reflect national regulations while sharing global risk indicators and sanctions lists where permitted ensuring that models and rules can be tuned per jurisdiction without fragmenting data lineage"
"Day0_AML_Sanctions_Perp","Explain how you would incorporate payment channel information into AML models","Describe adding features for channel type such as branch online mobile API along with device fingerprint IP and authentication method into models so they can learn that certain risk patterns such as sudden high risk transfers from new devices or APIs may be more indicative of laundering or account takeover than transactions initiated through long standing branch relationships"
"Day0_AML_Sanctions_Perp","Describe how Perpetual KYC can be applied to small retail accounts without overwhelming resources","Explain that pKYC can be tailored so low risk retail customers are monitored with light weight behavioural triggers and model based scanning while only a subset that exhibit changes such as unusual foreign transactions or adverse media are escalated to human review This focuses resources on customers whose risk actually changes rather than treating all accounts the same"
"Day0_AML_Sanctions_Perp","Design controls to ensure sanctions list updates are correctly applied in all relevant systems","Propose centralising list ingestion in a list management service that pushes updates via APIs or feeds to screening engines maintaining version numbers and timestamps Implement monitoring that confirms each engine uses the latest version and run automated smoke tests with known test cases after each update logging results for audit"
"Day0_AML_Sanctions_Perp","Explain how you would assess whether graph analytics brings incremental value over existing AML tools","Describe running proof of concept pilots comparing graph based alerts with existing rule and model outputs on historical data measuring additional true positives unique typologies discovered and manageable changes in false positives Document findings costs and operational impact and use this evidence to decide whether to scale"
"Day0_AML_Sanctions_Perp","Design a framework for resolving conflicting UBO information across multiple data sources","Propose assigning confidence scores to each source based on reliability and freshness and defining rules for reconciling discrepancies such as preferring official registries then high quality vendors while always recording customer attestations Investigators review complex cases with a structured checklist documenting decisions and the chosen ownership tree which is stored for reuse and audit"
"Day0_AML_Sanctions_Perp","Explain how you would structure training for investigators on using AI supported AML tools","Describe developing modules that explain model concepts alert scoring and explanation outputs using concrete case examples Provide guidance on how to interpret scores how to challenge or override them and how to give structured feedback that improves models Over time include results on how model aided triage improved outcomes to build trust"
"Day0_AML_Sanctions_Perp","Design a KPI set to measure effectiveness of a new pKYC programme","Suggest KPIs such as number of risk rating changes triggered by behaviour rather than calendar time reduction in time between risk change and KYC update conversion rate of pKYC triggered investigations into SARs and resource usage compared with periodic reviews plus measures of customer impact and regulatory feedback"
"Day0_AML_Sanctions_Perp","Explain how you would combine rules and AI to detect trade based money laundering","Describe keeping typology aligned rules that look at invoice values goods types shipment routes and counterparties while adding models that learn from legitimate trade patterns to identify anomalies Cross referencing customs data and shipping information can enhance both rules and models and graph analysis of counterparties helps detect circular trade flows indicating possible laundering"
"Day0_AML_Sanctions_Perp","Describe how you would use ISO 20022 fields to improve sanctions screening quality","Explain that you would use structured party names and postal addresses plus identifiers like LEIs from ISO messages instead of relying on free text and ensure that fields such as ultimate debtor creditor and intermediary agents are available to the screening engine This richer structure allows more precise matching and better context for scoring hits"
"Day0_AML_Sanctions_Perp","Design an end to end process for handling a high priority sanctions hit on an instant payment","Propose that real time screening flags the payment and routes it into a high priority queue holding the transaction pending decision Investigators receive full context including counterparties corridor and historical interactions and must decide within strict SLAs whether to release block or reject according to policy All actions and rationales are logged and if blocked relevant reporting and customer communication processes are triggered"
"Day0_AML_Sanctions_Perp","Explain how you would integrate KYC utilities into your pKYC process","Describe connecting to utilities via APIs to pull updated KYC profiles documents and risk indicators for shared customers on a regular schedule and at trigger events This data is merged with internal behaviour and used to refresh risk scores and KYC records while governance ensures that responsibilities between the bank and utility are clearly understood"
"Day0_AML_Sanctions_Perp","Design a controls environment to ensure models are not changed by developers directly in production","Explain that all model changes must go through a controlled pipeline including code and configuration in source control peer review automated testing and approval by model risk or governance committees Deployments are automated and audited and any emergency changes are documented and retrospectively reviewed"
"Day0_AML_Sanctions_Perp","Describe how you would support regulators requests for examples of how AI models improved AML detection","Explain that you would maintain case studies where AI identified typologies that rules missed with anonymised but detailed transaction graphs and model explanations Provide metrics on incremental SARs and improved time to detect and share documentation on governance validation and monitoring showing that use of AI is controlled"
"Day0_AML_Sanctions_Perp","Explain how pKYC supports early exit decisions for high risk customers","Describe that continuous monitoring allows the bank to spot deteriorating behaviour or ownership links more quickly and to reassess whether the relationship remains within risk appetite When thresholds are breached pKYC workflows route the case to relationship and risk committees who can decide on remediation or exit well before the next scheduled review"
"Day0_AML_Sanctions_Perp","Design a multi layer sanctions control approach for correspondent banking flows","Propose screening at onboarding of respondent banks plus ongoing due diligence screening of customer and FI parties in every cross border message pre and post transaction and network analytics to identify risky correspondent chains Combine this with periodic re screening when lists change and targeted reviews of corridors with heightened sanctions risk"
"Day0_AML_Sanctions_Perp","How would you integrate pKYC with credit risk for SMEs","Explain that behavioural and ownership signals from pKYC such as sudden high risk payments or adverse media can be fed into credit monitoring so that deteriorating AML risk prompts reviews of facilities and covenants Conversely credit events like defaults can trigger AML reviews to check for financial crime links aligning both disciplines"
"Day0_AML_Sanctions_Perp","Summarise how you would present the value of AI driven AML and pKYC to a senior executive","State that AI and pKYC allow the bank to detect riskier behaviours and hidden networks faster with fewer false positives while demonstrating to regulators a more dynamic risk based approach They also enable prioritisation of investigator effort and better alignment between AML customer risk and business decisions helping avoid fines and reputational damage without excessive manual cost"
Day0_AML_Sanctions_Gemi,"Design an AML monitoring stack that supports Perpetual KYC for retail customers. Explain the data flow.","1. Data Ingestion: Real-time event stream (Kafka) captures Transactions, Address Changes, and Login Data. 2. Profile Builder: Updates 'Customer 360' feature store dynamically. 3. Risk Engine: Runs 'Trigger Rules' (e.g., High-risk country transaction) and 'ML Models' (Anomaly detection) against the updated profile. 4. Decision: If risk score > threshold, trigger 'KYC Refresh Case'. 5. Feedback: Analyst outcome updates the Customer Risk Rating (CRR) immediately, resetting the monitoring baseline."
Day0_AML_Sanctions_Gemi,"Explain how you would integrate graph analytics into sanctions screening to detect indirect exposure (50% rule).","1. Data Lake: Ingest corporate registry data (e.g., Dun & Bradstreet, Moody's) and internal customer data. 2. Graph DB: Build a graph where Nodes = Entities/People and Edges = 'Owns' (with % weight). 3. Enrichment: Flag Sanctioned Nodes. 4. Traversal Algorithm: For every customer, run a traversal: 'Does any path exist to a Sanctioned Node where aggregate ownership > 50%?'. 5. Alerting: If yes, flag as 'Indirectly Sanctioned'."
Day0_AML_Sanctions_Gemi,"Describe how you would source, validate, and use UBO data in your transaction monitoring scenarios.","Source: Primary: Customer declaration (KYC). Secondary: API calls to Corporate Registries (Companies House) and Data Vendors (Orbis). Validation: Fuzzy match declared UBO vs Registry UBO. Resolve discrepancies. Usage: In TM, link accounts sharing the same UBO. Scenario: 'Cluster Monitoring' – detect if multiple unrelated companies owned by the same UBO are sending funds to each other (Circular Trading)."
Day0_AML_Sanctions_Gemi,"Design a 'Feedback Loop' mechanism for an AI-driven Transaction Monitoring model to prevent model drift.","1. Capture: Record Model Prediction (Score) and Analyst Decision (SAR / False Positive). 2. Labeling: Tag data points as 'Effective' or 'Ineffective'. 3. Retraining: Scheduled monthly pipeline retrains the Supervised Model using the new labeled dataset. 4. Champion/Challenger: Deploy new model in 'Shadow Mode'. 5. Evaluation: Compare performance. If new model has higher recall/precision, promote to Production."
Day0_AML_Sanctions_Gemi,"Explain how you would handle 'Name Transliteration' issues in Sanctions Screening for cross-border payments.","Implement a fuzzy matching engine with specific 'Script Support'. 1. Standardization: Convert input (e.g., Cyrillic, Arabic) to Latin characters using standard ISO schemas. 2. Phonetic Algorithms: Use Soundex or Metaphone to match 'sound-alikes' (Mohammad vs Muhamad). 3. Edit Distance: Use Levenshtein distance with weights (penalize start-of-name differences more). 4. Cultural Dictionaries: Use lookup tables for common name variations."
Day0_AML_Sanctions_Gemi,"Describe a control framework for 'Real-Time Payment' sanctions screening where latency must be <100ms.","1. In-Memory Computing: Load Sanctions Lists into RAM (Redis/GridGain), not disk DB. 2. Optimized Matching: Run 'Exact Match' and 'High Confidence Fuzzy' first. 3. Timeout Logic: If fuzzy logic takes >80ms, 'Fail Open' (release) or 'Fail Closed' (reject) based on risk appetite (usually Fail Closed for sanctions). 4. Strip Noise: Pre-filter stop words ('Limited', 'The'). 5. Asynchronous Logging: Write logs after decision to save I/O."
Day0_AML_Sanctions_Gemi,"How would you detect 'Structuring' (Smurfing) using ML instead of basic rules?","Basic rules fail if the user varies amounts ($9k, $8k, $9.5k). ML Approach: Use Clustering (e.g., DBSCAN) on the feature vector `[Time_Interval, Amount_Variance, Location]`. Train a model to identify 'High Frequency, Just-Below-Threshold' clusters over a sliding window (e.g., 7 days). This detects the *intent* to evade, even if amounts vary."
Day0_AML_Sanctions_Gemi,"Design a pKYC trigger framework for 'Adverse Media'.","1. Source: API integration with Adverse Media vendor (e.g., Refinitiv/LexisNexis). 2. Filter: Configure filters for 'Financial Crime' and 'Certainty' (exclude gossip). 3. Matching: Fuzzy match news subject against Customer Base. 4. Trigger: If Match Confidence > 90%, create 'Event-Driven Review' case. 5. AI Summary: Use LLM to summarize the article and extract 'Risk Keywords' for the analyst."
Day0_AML_Sanctions_Gemi,"Explain how you would use 'Network Analysis' to detect a 'Mule Account' ring.","1. Build Graph: Nodes = Accounts, IP Addresses, Device IDs. Edges = Transactions, Logins. 2. Pattern: Look for 'Fan-In' (many small credits) followed by 'Fan-Out' (one large debit) or 'Pass-Through'. 3. Shared Attributes: Identify distinct accounts sharing the same Device ID or IP Subnet. 4. Centrality: Calculate 'Betweenness Centrality' to find the 'Herder' account controlling the mules."
Day0_AML_Sanctions_Gemi,"Describe the data lineage requirements for a Sanctions Screening model to satisfy a regulator.","1. Raw Data: Trace the payment message (SWIFT MT103) from entry. 2. Transformation: Document parsing logic (e.g., extraction of Field 50K). 3. Normalization: Show how special characters were handled. 4. List Version: Log exactly which version of the OFAC list was active at the microsecond of screening. 5. Decision: Log the score and the threshold used. Proof that 'Input A' + 'List B' deterministicly = 'Alert'."
Day0_AML_Sanctions_Gemi,"How would you manage 'List Management' updates (e.g., new OFAC designations) in a 24/7 payment system?","1. Subscription: Automated feed from Regulatory Body. 2. Delta Processing: Identify only new/changed entities. 3. Hot Deployment: Update the in-memory screening lists without restarting the engine (Zero Downtime). 4. Retro-Screening: Immediately trigger a batch scan of the entire Customer Base against the *new* names to catch existing accounts. 5. Audit: Log timestamp of list activation."
Day0_AML_Sanctions_Gemi,"Design a solution to reduce 'False Positives' caused by common names (e.g., 'John Smith') in screening.","1. Secondary Identifiers: Require Year of Birth or City in the search. 2. Contextual Scoring: If Name Match = 100% but Country = 'USA' and List = 'North Korea', downgrade score. 3. Good Guy List: Maintain a 'White List' of previously investigated false positives (hashed). 4. ML Classifier: Train a model on analyst dispositions to predict 'False Positive' probability based on metadata."
Day0_AML_Sanctions_Gemi,"Explain the concept of 'Dynamic Risk Rating' in pKYC.","Instead of a static 'High/Medium/Low' set at onboarding, the Risk Rating is a live score (0-100). Inputs: Transaction/Alert history, Profile changes, Geo-risk. Logic: `Score = w1*CountryRisk + w2*AlertCount + w3*ProductRisk`. If Score crosses a threshold (e.g., 80), the Customer Tier upgrades to 'High', triggering EDD immediately."
Day0_AML_Sanctions_Gemi,"How would you handle 'Unstructured Data' (e.g., trade documents) in AML monitoring?","1. OCR: Digitize paper Invoices/Bills of Lading. 2. NLP/NER (Named Entity Recognition): Extract 'Shipper', 'Consignee', 'Goods Description', 'Value'. 3. Validation: Compare extracted Value against market prices (Price Check). 4. Sanctions: Screen extracted names. 5. Alerting: Flag if Goods Description is vague ('Merchandise') or high-risk ('Chemicals')."
Day0_AML_Sanctions_Gemi,"Design a 'Testing & Tuning' methodology for Transaction Monitoring rules.","1. Baseline: Measure current Alert Rate and SAR conversion. 2. Backtesting: Run modified rule logic (e.g., change threshold $10k -> $15k) on 6 months of historical data. 3. Impact Analysis: Calculate 'Alert Volume Reduction' vs 'Missed SARs' (using known SARs). 4. Above-the-Line/Below-the-Line testing: Sample transactions just below the threshold to ensure no leakage. 5. Governance: Approval for change."
Day0_AML_Sanctions_Gemi,"Explain how 'ISO 20022' data fields improve 'Payment Transparency' and AML detection.","ISO 20022 introduces structured fields (e.g., `UltimateDebtor`, `PurposeCode`, `CategoryPurpose`). Improvement: 1. Precision: Can monitor specifically for 'Salary' or 'Gambling' codes. 2. Ownership: `UltimateDebtor` reveals the true payer, preventing shell company masking. 3. Address: Structured address fields allow targeted screening of 'Town' or 'Zip' without fuzzy text errors."
Day0_AML_Sanctions_Gemi,"Design an 'Alert Hibernation' logic for repeated false positives.","1. Identification: Analyst marks alert 'False Positive - Business Justification Accepted'. 2. Fingerprinting: Create a hash of the activity pattern (e.g., 'Monthly Transfer to Wife < $5k'). 3. Hibernation: If identical pattern recurs within X months, auto-close or suppress alert. 4. Safety Valve: If amount deviates by >20% or counterparty changes, break hibernation and alert."
Day0_AML_Sanctions_Gemi,"How would you implement 'Explainable AI' (XAI) for an AML neural network model?","Use SHAP (SHapley Additive exPlanations) or LIME values. For every high-risk score, generate a 'Reason Code' list. Example: 'Score 0.95. Top Factors: 1. Burst of high-value transactions (+0.4), 2. Interaction with High-Risk Geo (+0.3)'. Display these factors to the investigator UI so they understand *why* the 'Black Box' flagged it."
Day0_AML_Sanctions_Gemi,"Describe a strategy for 'Dual-Use Goods' screening in trade finance.","1. Lookup Table: Maintain database of HS Codes (Harmonized System) linked to Dual-Use lists. 2. Extraction: Pull HS Codes from trade docs. 3. Matching: Alert if HS Code matches restricted list. 4. Narrative Analysis: Use NLP to scan 'Goods Description' for synonyms of restricted items (in case HS code is misdeclared). 5. Corridor Check: Elevate risk if Destination is a sanctioned-adjacent country."
Day0_AML_Sanctions_Gemi,"Design a 'Customer Segmentation' model using Unsupervised Learning.","1. Feature Engineering: Create features like 'Avg Txn Size', 'Intl Txn %', 'Cash %', 'Time of Day'. 2. Algorithm: Apply K-Means or DBSCAN clustering. 3. Analysis: Identify clusters (e.g., 'Night-time Crypto Traders', 'Cash-heavy Retail'). 4. Application: Apply specific TM thresholds to each cluster (e.g., tighter Cash limits for the Crypto cluster, looser for Retail)."
Day0_AML_Sanctions_Gemi,"Explain the operational workflow for a 'Material UBO Discrepancy' discovered during pKYC.","1. Detection: Automated check finds Registry UBO != Bank UBO. 2. Block: Place 'Post No Debit' marker on account. 3. Outreach: Automated email to client 'Please update ownership details'. 4. Remediation: Client submits new docs. 5. Verification: Analyst validates docs. 6. Update: Update Core Banking UBO data. 7. Unblock: Remove restriction. 8. Report: File discrepancy report to Company Registry if required by law."
Day0_AML_Sanctions_Gemi,"How would you handle 'Crypto-Asset' AML monitoring in a traditional bank?","1. On/Off Ramp Monitoring: Identify transactions to/from known Crypto Exchanges (using BIC/Account lists). 2. Risk Rating: High risk score for customers frequently interacting with exchanges. 3. Blockchain Analytics: For direct crypto custody, integrate with Chainalysis/Elliptic to screen the *wallet address* for taint (links to Darknet/Sanctions) before processing the transaction."
Day0_AML_Sanctions_Gemi,"Design a 'Lookback' process for a newly sanctioned jurisdiction.","1. Scope: Identify all payments involving the new jurisdiction (Country Code, Free Text search) for the past 5 years. 2. Data Retrieval: Pull historical transaction logs from Data Warehouse. 3. Re-Screening: Screen historical counterparties against the *current* sanctions list. 4. Investigation: Manually review hits. 5. Reporting: If a match is found (meaning we traded with them before they were listed, or during a window), assess legal obligation to report VSD."
Day0_AML_Sanctions_Gemi,"Explain the role of 'Synthetic Data' in training AML models where real fraud data is scarce.","1. Generation: Use GANs (Generative Adversarial Networks) to create fake transaction data that statistically mimics real behavior. 2. Injection: Inject known 'Fraud Patterns' (e.g., Smurfing) into the synthetic dataset. 3. Training: Train the model on this balanced dataset. 4. Benefit: Overcomes the 'Class Imbalance' problem (real fraud is <0.1%) and avoids privacy issues with using real customer PII."
Day0_AML_Sanctions_Gemi,"How would you secure the 'Sanctions List' database to prevent internal tampering?","1. Access Control: Read-only for 99% of staff. Write access restricted to 'List Management' team. 2. 4-Eyes Principle: Updates require Maker/Checker approval. 3. Integrity Check: Hash the list file after every update. The Screening Engine verifies the hash before loading. 4. Audit Log: Immutable log of who changed what rule/name and when."
Day0_AML_Sanctions_Gemi,"Design a 'Governance Framework' for a third-party KYC utility integration.","1. Due Diligence: Audit the Utility's data sources and refresh frequency. 2. SLA: Define 'Uptime' and 'Data Freshness' (e.g., updates within 24h of Registry change). 3. Fallback: If Utility is down, revert to Manual Document Upload. 4. Testing: periodic 'Mystery Shopper' tests—compare Utility data against source Registry to check accuracy. 5. Liability: Contractual definition of who is liable for bad data."
Day0_AML_Sanctions_Gemi,"Explain how 'Containerization' (Docker) aids in deploying AML models.","1. Consistency: Bundles the Model + Dependencies (Python libs) into one image. 2. Scalability: Kubernetes can auto-scale the number of 'Screening Pods' based on transaction volume (e.g., Black Friday). 3. Isolation: Different models (Sanctions vs AML) run in isolated containers, preventing conflicts. 4. Versioning: Easy rollback to previous model version if the new one underperforms."
Day0_AML_Sanctions_Gemi,"Design a 'SAR Filing' workflow automation.","1. Trigger: Investigator clicks 'File SAR'. 2. Auto-Populate: System pulls Customer Name, Address, DoB, and Transaction History into the SAR template. 3. Narrative: GenAI drafts the 'Suspicious Activity Narrative' based on case notes. 4. Review: MLRO reviews and edits. 5. Submission: Secure API submission to the FIU (Financial Intelligence Unit). 6. Record Keeping: Save specific SAR version and ack receipt."
Day0_AML_Sanctions_Gemi,"How would you use 'Biometrics' in pKYC to prevent Account Takeover (ATO)?","1. Baseline: Capture 'Behavioral Biometrics' (typing speed, device angle, swipe gestures) during normal usage. 2. Monitoring: Continuous scoring during every session. 3. Trigger: If behavior score drops (e.g., typing is too fast = bot, or too slow = fraudster), trigger 'Step-Up Auth' (FaceID). 4. pKYC: Update 'Last Verified' timestamp upon successful biometric challenge."
Day0_AML_Sanctions_Gemi,"Explain 'Model Drift' in AML and how to detect it.","Drift: The model's performance degrades because real-world behavior changes (criminals change tactics). Detection: Monitor 'PSI' (Population Stability Index). Compare the distribution of input scores today vs training data. If distribution shifts significantly, the model is drifting. Also monitor SAR conversion rate—if it drops, the model is losing relevance."
Day0_AML_Sanctions_Gemi,"Design a strategy for monitoring 'Politically Exposed Persons' (PEPs) in pKYC.","1. Screening: Daily screening against PEP lists. 2. Risk Scoring: Differentiate Domestic vs Foreign PEPs, and Role (Head of State vs Local Councillor). 3. Transaction Monitoring: Apply stricter thresholds for 'Cash' and 'International Wires'. 4. Annual Review: Mandatory manual review by Senior Management to approve continuing the relationship (EDD requirement)."
Day0_AML_Sanctions_Gemi,"How would you handle 'False Negatives' (Missed Fraud) discovery in your audit?","1. Root Cause Analysis: Why did the rule/model miss it? (Data quality? Threshold too high?). 2. Backtesting: Run a new potential rule on historical data to see if it would have caught it without excessive False Positives. 3. Tuning: Adjust the model/rules. 4. Reporting: Disclose the gap and the fix to the Audit Committee/Regulator."
Day0_AML_Sanctions_Gemi,"Design a 'Data Quality Dashboard' for AML.","Metrics: 1. 'Completeness': % of customers with missing DoB/Address. 2. 'Validity': % of invalid Country Codes. 3. 'Timeliness': % of KYC reviews overdue. 4. 'Consistency': % of UBO data matching external registry. Visual: Red/Amber/Green status. Action: Auto-assign data remediation tasks to Front Office if quality drops."
Day0_AML_Sanctions_Gemi,"Explain the use of 'Federated Learning' for collaborative AML between banks.","Problem: Banks can't share customer data (Privacy). Solution: Federated Learning. 1. A central 'Global Model' is sent to each Bank. 2. Bank trains model locally on its own data. 3. Bank sends only the 'Model Weights' (math updates) back to the center (no PII). 4. Center aggregates weights and improves the Global Model. 5. Result: Collective intelligence on fraud patterns without sharing data."
Day0_AML_Sanctions_Gemi,"Design a 'Whistleblowing' channel integration with AML.","1. Channel: Secure, anonymous web portal/hotline. 2. Triage: Case assigned to Special Investigations Unit. 3. Cross-Ref: Analyst checks 'Subject' against AML alerts. 4. Protection: Ensure 'Subject' is not tipped off. 5. Outcome: If validated, manually trigger a high-priority AML Case/SAR and potentially freeze assets."
Day0_AML_Sanctions_Gemi,"How would you implement 'Geo-Fencing' rules for Sanctions?","1. Input: IP Address and GPS data from Mobile App/Browser. 2. Enrichment: Resolve IP to Geo-location. 3. Policy: IF Country = 'North Korea/Iran/Syria' -> Block Access immediately. 4. VPN Detection: Use proxy detection services to identify if user is masking location. If VPN detected -> Require Step-Up Auth or Block."
Day0_AML_Sanctions_Gemi,"Design a 'Pre-Trade' screening flow for Trade Finance.","1. Input: Letter of Credit details (Parties, Ports, Vessel, Goods). 2. Screen: Check Vessel against IMO blacklist. Check Ports against Sanctioned Ports list. Check Goods against Dual-Use list. 3. Outcome: If all Clean -> Auto-Approve. If any Hit -> Hold for Trade Specialist review. 4. SLA: Must complete within 2 hours to avoid trade delays."
Day0_AML_Sanctions_Gemi,"Explain 'Risk Appetite Statement' and how it links to AML configurations.","Statement: Board-level document defining acceptable risk (e.g., 'Zero tolerance for Sanctions breaches', 'Low tolerance for Cash businesses'). Link: The AML configurations (Thresholds, Country Risk Scores) must be mathematically derived from this statement. If Appetite = Low, Country Risk Weights must be High, and Alert Thresholds must be Low."
Day0_AML_Sanctions_Gemi,"Design a 'Source of Wealth' (SoW) collection workflow for High Net Worth Individuals.","1. Trigger: Total Assets > $1M or High Risk rating. 2. Collection: Digital form asking 'Origin of Wealth' (Inheritance, Business, Salary). 3. Evidence: Upload support (Will, Business Accounts). 4. Validation: Wealth Analyst reviews consistency (Does Business Revenue match Wealth?). 5. Storage: Document stored in immutable archive for regulator inspection."
Day0_AML_Sanctions_Gemi,"How would you optimize the 'alert review' UI for analysts to improve efficiency?","1. Context: Display 'Customer Journey' timeline (Past cases, recent changes). 2. Highlight: Use color-coding to highlight the *exact* transaction/field that triggered the alert. 3. Network: Show a mini-graph of related entities. 4. One-Click: Integration with external registries for quick lookups. 5. Templates: Pre-filled narrative templates for closing cases."
Day2_ISO20022_Extraction_Gemi,"Describe how to extract all information needed for a cross-border payments volume report from `pacs.008` and `pain.001` tables.","Join `pacs.008` and `pain.001` on UETR (if available) or EndToEndId. Filter by Date. Extract `<InstgAgt>` (Sender BIC) and `<InstdAgt>` (Receiver BIC). Substring BICs to get Country Codes. Filter where SenderCountry != ReceiverCountry. Sum `<InstdAmt>` group by Currency and Corridor (SenderCountry-ReceiverCountry)."
Day2_ISO20022_Extraction_Gemi,"Design a table structure for `stg_pacs009_cov` to support 'Stripping' detection.","Table: `stg_pacs009_cov`. Columns: `message_id` (PK), `uetr`, `settlement_amt`, `settlement_ccy`, `settlement_method` (Index), `nested_ordering_customer_name` (extracted from `<UndrlygCstmrCdtTrf/Dbtr/Nm>`), `nested_beneficiary_name`, `xml_blob` (Golden Copy). Detection Logic: Select * where `settlement_method` = 'COV' and (`nested_ordering_customer_name` IS NULL)."
Day2_ISO20022_Extraction_Gemi,"Explain how to extract the 'Ultimate Debtor' from a `pacs.008` XML using SQL pseudo-code, handling the case where it might be missing.","`COALESCE(XMLGET(xml_col, 'GrpHdr/UltmtDbtr/Nm'), XMLGET(xml_col, 'TxInf/UltmtDbtr/Nm'), 'Not Provided')`. Logic: Check Group Header first (uncommon), then Transaction level. If both null, the field is missing (implies Debtor = Ultimate Debtor, or data gap)."
Day2_ISO20022_Extraction_Gemi,"Write the logic to map ISO 20022 `Structured Address` fields to a legacy 4-line address format.","Line 1: `<StrtNm>` + ' ' + `<BldgNb>`. Line 2: `<BldgNm>` (if exists). Line 3: `<TwnNm>` + ' ' + `<PstCd>`. Line 4: `<Ctry>`. Note: Check for length limits (e.g., 35 chars). If Line 1 > 35, truncate or move spillover to Line 2. Append '...' if truncated."
Day2_ISO20022_Extraction_Gemi,"How would you calculate the 'FX Spread' earned on a payment using `pacs.008` fields?","Extract `<InstdAmt>` (Client Ccy) and `<IntrBkSttlmAmt>` (Settlement Ccy). Extract `<XchgRate>` (Rate applied). Calculate `Implied Rate` = `IntrBkSttlmAmt` / `InstdAmt`. Compare `Implied Rate` vs `Market Mid-Rate` at `<CreDtTm>`. Spread = (Implied - Market) * Amount."
Day2_ISO20022_Extraction_Gemi,"Design a query to identify 'High Risk Remittance' keywords in `pacs.008` messages.","`SELECT uetr, remittance_text FROM fact_payments WHERE REGEXP_LIKE(remittance_text, '(weapon|nuclear|iran|cuba|crypto|darknet)', 'i')`. Optimisation: Run this against the text column extracted from `<RmtInf>` in the staging layer."
Day2_ISO20022_Extraction_Gemi,"Explain how to link a `pacs.002` status report back to the original `pacs.008` in the database.","Join `stg_pacs002` to `stg_pacs008` on `stg_pacs002.OriginalMessageId` = `stg_pacs008.MessageId`. Alternatively, join on `OriginalEndToEndId` or `OriginalUETR`. Update the `fact_payment.status` column with the latest status from `pacs.002` (`ACSC`, `RJCT`)."
Day2_ISO20022_Extraction_Gemi,"Describe the extraction of 'Charge Bearer' info for a Consumer Duty report.","Extract `<ChrgBr>` from `pacs.008`. Count volume by `DEBT`, `CRED`, `SHAR`. Join with `<ChrgsInf>` to see actual amount deducted. Flag payments where `<ChrgBr>` = `DEBT` but `<ChrgsAmt>` > 0 (Mismatch/Double Charge risk)."
Day2_ISO20022_Extraction_Gemi,"How do you extract the 'Initiating Party' for a corporate payment analysis from `pain.001`?","Extract `<InitgPty><Nm>` and `<InitgPty><Id>`. This represents the Corporate Head Office. Group by `<InitgPty>` to see total volume per corporate client, regardless of which subsidiary account (`<Dbtr>`) was used."
Day2_ISO20022_Extraction_Gemi,"Write the logic to detect if a `pacs.008` is an 'Instant Payment' based on `Local Instrument` or `Service Level`.","`CASE WHEN XMLGET(xml, '.../LclInstrm/Cd') IN ('UK.FPS', 'SEPA.INST') OR XMLGET(xml, '.../SvcLvl/Cd') = 'G001' THEN 'INSTANT' ELSE 'BATCH' END`. Note: Codes vary by region (Inst = Instant)."
Day2_ISO20022_Extraction_Gemi,"Design a 'Golden Copy' architecture to preserve truncated data.","1. Ingest raw ISO XML into Data Lake (S3). 2. Generate Metadata Pointer (UETR -> S3 Path). 3. Load truncated/mapped data into Legacy SQL DB for processing. 4. For Screening/Reporting: Query the Data Lake using the Pointer to get full 140-char names, bypassing the Legacy DB entirely."
Day2_ISO20022_Extraction_Gemi,"How would you extract the 'Purpose Code' from `pacs.008` for Central Bank Reporting?","Priority Logic: 1. Check `<Purp><Cd>`. 2. If null, check `<Purp><Prtry>`. 3. If null, check `<CtgyPurp><Cd>`. 4. If null, infer from `<RmtInf>` (Risky). Map the code (e.g., 'SALA') to the Central Bank's required category (e.g., 'Income')."
Day2_ISO20022_Extraction_Gemi,"Explain how to extract `LEI` for both Debtor and Creditor from a `pacs.008`.","Path Debtor: `<Dbtr><Id><OrgId><Id><OrgId><Othr>[SchmeNm/Cd='LEI']<Id>`. Path Creditor: `<Cdtr><Id><OrgId><Id><OrgId><Othr>[SchmeNm/Cd='LEI']<Id>`. Validate the extracted ID against the GLLEI database."
Day2_ISO20022_Extraction_Gemi,"Design a check for 'Sanctions Evasion via Stripping' using `pacs.009` vs `MT202`.","In `pacs.009`: Check if `<UndrlygCstmrCdtTrf>` is missing. In `MT202`: Check if `Field 50` (Ordering Customer) and `Field 59` (Beneficiary) are missing. ISO makes this easier as the block is explicit (`COV` variant)."
Day2_ISO20022_Extraction_Gemi,"Write the logic to determine 'Payment Latency' using `CreDtTm` fields.","`latency_seconds = TIMESTAMP_DIFF('second', pacs008.CreDtTm, pacs002.CreDtTm)`. Filter for `pacs.002` where status is `ACSC` (Settled) or `ACCP` (Accepted)."
Day2_ISO20022_Extraction_Gemi,"How do you extract 'Intermediary Agents' for a correspondent banking dashboard?","Extract `<IntrmyAgt1>`, `<IntrmyAgt2>`, `<IntrmyAgt3>` BICs from `pacs.008`. Unpivot these columns into a list of 'Hops'. Count frequency of each BIC to visualise routing concentration."
Day2_ISO20022_Extraction_Gemi,"Describe the extraction of 'Postal Address' components for Sanctions Screening.","Extract `<TwnNm>`, `<Ctry>`, `<StrtNm>`, `<BldgNb>`. Concatenate them into a full string for the Screening Engine (if it takes strings) OR map to specific fields (City, Country) for the Engine (if it supports structured input). The latter reduces False Positives."
Day2_ISO20022_Extraction_Gemi,"Explain how to handle 'AnyBIC' codes during extraction for reporting.","Check if BIC is 'Generic' (non-SWIFT). If so, extract the `<Nm>` and `<PstlAdr>` associated with the Agent. Report 'Institution Name' instead of BIC. Flag data quality as 'Non-Standard BIC'."
Day2_ISO20022_Extraction_Gemi,"Design a query to count 'Payments by Channel' using `pain.001` data.","`SELECT XMLGET(xml, '.../GrpHdr/InitgPty/Nm') as Channel, COUNT(*) FROM stg_pain001 GROUP BY Channel`. Alternatively, use `<Prtry>` tags in the header if the channel injects a specific source code there."
Day2_ISO20022_Extraction_Gemi,"How would you extract 'Regulatory Reporting' specific tags (e.g., for India/UAE)?","Look for the `<RgltryRptg>` block in `pacs.008`. Extract `<Dtls><Tp>` (Code) and `<Dtls><Desc>` (Description). These are country-specific extensions; mapping requires a lookup table based on the Country Code."
Day2_ISO20022_Extraction_Gemi,"Write the logic to detect 'Invoice Numbers' in Unstructured Remittance Info.","`REGEXP_SUBSTR(remittance_text, '(INV|INVOICE|REF)[: ]?([A-Z0-9]+)')`. This attempts to pull the alphanumeric string following 'INV'. Store in `extracted_invoice_ref`."
Day2_ISO20022_Extraction_Gemi,"Explain how to map `camt.053` entries to `fact_payment_tx` for reconciliation status.","Match `camt.053` `<Ntry><NtryDtls><TxDtls><Refs><EndToEndId>` against `fact_payment_tx.end_to_end_id`. If match found and amounts agree, set `reconciled_flag = 'Y'`. Use `<ValDt>` for the actual settlement date."
Day2_ISO20022_Extraction_Gemi,"Design a 'Customer Type' classification logic based on `pacs.008` data.","If `<Dbtr><Id><OrgId>` is present -> 'Corporate'. If `<Dbtr><Id><PrvtId>` is present -> 'Retail'. If `<Dbtr>` is a BIC -> 'Bank'. Store in `customer_segment` column."
Day2_ISO20022_Extraction_Gemi,"How do you extract the 'Settlement Cycle' (Date) from a `pacs.008`?","Extract `<IntrBkSttlmDt>`. This is the Value Date. Critical for Liquidity reporting. If missing (Instant Payments often use timestamp), use `<IntrBkSttlmDt>` or derive from `<CreDtTm>`."
Day2_ISO20022_Extraction_Gemi,"Write the logic to identify 'U-Turn' payments (Country A -> Country B -> Country A).","`SELECT uetr FROM fact_payment_tx WHERE origin_country = dest_country AND intermediary_country <> origin_country`. (Requires extracting Intermediary Agent Country)."
Day2_ISO20022_Extraction_Gemi,"Explain how to handle 'Multiple Currencies' in a single `pain.001` file extraction.","The `pain.001` Group Header has no currency. Each `<PmtInf>` block or `<CdtTrfTxInf>` block has the currency. You must explode the XML at the Transaction level to capture the correct currency for each payment."
Day2_ISO20022_Extraction_Gemi,"Design an extraction for 'Returned Payments' analysis using `pacs.004`.","Extract `<OrgnlUETR>`, `<RtrRsnInf><Rsn><Cd>` (Reason Code), and `<OrgnlIntrBkSttlmAmt>`. Join to original payment on UETR. Calculate 'Return Rate' per Reason Code (e.g., 'AC01' - Account Incorrect)."
Day2_ISO20022_Extraction_Gemi,"How would you extract the 'Debtor Account' IBAN vs Proprietary ID?","`CASE WHEN xml_path('.../DbtrAcct/Id/IBAN') IS NOT NULL THEN xml_path('.../IBAN') ELSE xml_path('.../Othr/Id') END`. Store in `debtor_account_num` and `debtor_account_type` ('IBAN'/'OTHER')."
Day2_ISO20022_Extraction_Gemi,"Write logic to flag payments involving 'Tax Havens' using Country Codes.","`SELECT * FROM fact_payment_tx WHERE origin_country IN ('KY', 'VG', 'PA', ...) OR dest_country IN ('KY', 'VG', 'PA', ...)`. Maintain the list of ISO Country Codes for tax havens in a reference table."
Day2_ISO20022_Extraction_Gemi,"Explain the extraction of 'Exchange Rate Contract ID' for Treasury reporting.","Extract `<XchgRateInf><CtrctId>` from `pacs.008`. This links the individual payment to the bulk FX deal booked by Treasury. Report on 'Unlinked FX Payments' (missing Contract ID)."
Day2_ISO20022_Extraction_Gemi,"Design a check for 'Duplicate Processing' using UETR and Message ID.","`SELECT uetr, count(DISTINCT msg_id) FROM stg_pacs008 GROUP BY uetr HAVING count(*) > 1`. If a UETR appears in multiple *different* messages, it is a duplicate/replay attack or system error."
Day2_ISO20022_Extraction_Gemi,"How to extract 'Creditor Reference Information' (Structured Remittance) for auto-reconciliation?","Extract `<RmtInf><Strd><CdtrRefInf><Ref>`. This is the 'Reference' (e.g., RF Creditor Reference) the beneficiary needs to clear the invoice automatically."
Day2_ISO20022_Extraction_Gemi,"Write the logic to detect 'Split Payments' (Structuring) below a threshold.","`SELECT dbtr_nm, sum(amt) FROM fact_payment_tx WHERE amt < 10000 AND transaction_date > NOW() - INTERVAL '24 HOURS' GROUP BY dbtr_nm HAVING sum(amt) > 10000`. Detects multiple small payments summing to a large amount."
Day2_ISO20022_Extraction_Gemi,"Explain how to map the `pain.002` 'Group Status' vs 'Transaction Status'.","Extract `<OrgnlGrpInfAndSts><GrpSts>` (File Level) and `<TxInfAndSts><TxSts>` (Tx Level). If Group Status is 'PART' (Partial), you must check each Transaction Status to know which specific payments failed."
Day2_ISO20022_Extraction_Gemi,"Design an extraction for 'Payment Type' (Salary, Tax, Supplier) based on ISO codes.","Map `<Purp><Cd>` or `<CtgyPurp><Cd>` to business descriptions. `SALA` -> Salary, `TAXE` -> Tax, `SUPP` -> Supplier. If code missing, map `NULL`. Use this dimension for Portfolio Analysis."
Day2_ISO20022_Extraction_Gemi,"How would you handle 'Date of Birth' extraction for AML if formats vary?","`pacs.008` uses `YYYY-MM-DD`. Extract `<Dbtr><Id><PrvtId><DtAndPlcOfBirth><BirthDt>`. Cast to Date type. If invalid format (legacy migration error), set to NULL and flag 'Data Quality Error'."
Day2_ISO20022_Extraction_Gemi,"Write logic to compare 'Instructed Agent' vs 'Creditor Agent' to detect indirect routing.","`CASE WHEN instd_agt_bic <> cdtr_agt_bic THEN 'INDIRECT' ELSE 'DIRECT' END`. (Indirect implies the receiver bank is using a correspondent/intermediary to receive funds)."
Day2_ISO20022_Extraction_Gemi,"Explain the extraction of 'Authorisation' details from `pain.001` (if present).","Some schemas use `<Authstn>` block. Extract `<Authstn><Prtry>` to find who approved the payment at the corporate side (if data provided). Useful for fraud investigations."
Day2_ISO20022_Extraction_Gemi,"Design a 'Sanctions List' matching key using ISO fields.","Composite Key: `Name` (`<Nm>`) + `Country` (`<PstlAdr><Ctry>`) + `BIC` (`<FinInstnId>`). Generate this key from the `pacs.008` stream and query the Sanctions Engine API with it."
Day2_ISO20022_Extraction_Gemi,"How do you extract the 'Sender Reference' for client reporting?","Extract `<PmtId><EndToEndId>` (Customer Ref) and `<PmtId><InstrId>` (Bank Ref). Report both. `EndToEndId` is mandatory for the customer to recognise the transaction."
"Day2_ISO20022_Extraction_Perp","Describe how to design a staging model for pacs.008 messages that supports both AML screening and prudential reporting","Explain that each <CdtTrfTxInf> becomes one row in stg_pacs008 with columns for all key parties agents amounts references purpose codes regulatory fields and remittance alongside a link to a raw XML store Capture debtor and creditor names structured addresses organisation and private ids LEIs agent BICs and UETR plus message metadata like schema version This staging then feeds canonical payment facts and allows AML models and prudential aggregations to use the same consistent field set"
"Day2_ISO20022_Extraction_Perp","How would you map pain.001 and pacs.008 into a canonical payment_tx fact to avoid double counting and preserve lifecycle information","State that you would use UETR and EndToEndId as logical keys and create one payment_tx row per business payment Each fact row stores attributes from both messages such as channel and requested execution from pain.001 and settlement details from pacs.008 while foreign keys reference the original staging rows A status dimension or fact captures events from pain.002 and pacs.002 so lifecycle metrics can be derived without counting these as separate payments"
"Day2_ISO20022_Extraction_Perp","Explain how to extract all information needed for a cross border payments volume report from pacs.008 and pain.001 tables","Describe joining stg_pacs008 with any matching stg_pain001 on EndToEndId and or UETR then deriving debtor and creditor countries using agent BICs or party addresses For each payment determine cross border status when countries differ and aggregate volumes by corridor customer type and currency using debtor customer type from KYC tables and amounts from the settlement amount fields"
"Day2_ISO20022_Extraction_Perp","Describe an approach for preserving a golden copy of ISO 20022 message data while still supporting legacy core mappings","Explain that ingestion should first validate and store each ISO message in a raw XML table with message type direction timestamps and schema version Then parsing jobs populate staging tables and separate transformations map into legacy formats for downstream cores Any fields that cannot be mapped are still preserved in the XML so future regulatory or analytical needs can be met by re parsing without data loss"
"Day2_ISO20022_Extraction_Perp","How would you design a SQL view that flags cross border payments using only ISO agent information","Propose a view over payment_tx that uses debtor and creditor agent BICs from the canonical model applying a CASE expression comparing the two country codes from positions five and six of each BIC If they differ the view sets is_cross_border to one and also constructs a corridor field combining origin and destination country codes for downstream reporting"
"Day2_ISO20022_Extraction_Perp","Explain how to derive customer type retail or corporate for reporting using ISO 20022 data and internal reference tables","Describe that you would join debtor and creditor party ids from payment_tx to a customer dimension containing KYC information such as person or organisation flag and segment If that is missing you can infer type from ISO party identification structures where organisation ids like LEIs suggest corporates and private ids suggest individuals and store the result in debtor_customer_type and creditor_customer_type fields"
"Day2_ISO20022_Extraction_Perp","Design a mapping from ISO purpose codes into a normalised reporting dimension for central bank statistics","Explain that you would create a purpose dimension table listing all scheme specific and CBPR+ <Purp> codes with mappings to central bank purpose categories and human readable descriptions ETL logic reads the <Purp> or <CtgyPurp> codes from staging tables links them to this dimension via a key and stores the surrogate id in payment_tx so statisticians can roll up volumes by official purpose category"
"Day2_ISO20022_Extraction_Perp","How would you use camt.053 data to enrich the payment_tx fact with booking and value dates for prudential liquidity metrics","Describe joining camt.053 entry details to payment_tx on references such as EndToEndId TxId or UETR and account ids When a match is found update payment_tx with booking_date and value_date from the statement entry as well as the final booked amount This allows liquidity metrics to use actual cash movement dates rather than requested execution dates from initiation messages"
"Day2_ISO20022_Extraction_Perp","Describe a SQL approach to flag payments that involve a change of currency between customer and interbank legs","Explain that you can compare the instructed amount currency from pain.001 or customer account currency with the interbank settlement currency in pacs.008 using a CASE expression If the currencies differ set an fx_conversion_flag in payment_tx which can be used for FX reporting and AML scenarios involving conversion layers"
"Day2_ISO20022_Extraction_Perp","Explain how you would derive a corridor field for payments using country information from ISO messages","State that you would extract origin and destination country codes preferably from debtor and creditor agent BICs or else from debtor and creditor postal addresses then concatenate them in a consistent format such as origin dash destination This derived corridor field is stored in payment_tx and used in balance of payments and corridor risk reports"
"Day2_ISO20022_Extraction_Perp","Design an ingestion check that ensures each pacs.008 with CBPR+ usage has a UETR and logs exceptions","Describe applying schema or business rule validation during XML parsing such that if a pacs.008 is marked as CBPR+ but has an empty or invalid UETR the record is flagged in a quality table and optionally rejected or quarantined A nightly report of such exceptions is sent to operations and mapping logic treats them carefully to avoid broken lifecycle chains in payment_tx"
"Day2_ISO20022_Extraction_Perp","How would you structure a payment status fact using pain.002 and pacs.002 messages for operational reporting","Explain creating a payment_status fact table keyed by payment_tx_id with fields for status code reason code timestamp and source message reference Each pain.002 or pacs.002 generates a new row indicating a lifecycle event such as accepted rejected or settled which can be used to calculate rejection rates and processing times without duplicating payments in the main fact"
"Day2_ISO20022_Extraction_Perp","Describe how to handle truncation when mapping ISO remittance information into limited legacy fields","State that you should preserve full remittance content in staging and golden copy while creating a truncated version only for systems that require length limits Store both original and truncated versions in your warehouse with a flag indicating truncation so analytics and AML models use the complete text while core systems operate within their constraints"
"Day2_ISO20022_Extraction_Perp","Explain how to extract the data needed for a report on payments involving a specific correspondent bank using ISO 20022","Describe querying stg_pacs008 and stg_pacs009 for transactions where intermediary agent or instructing instructed agent BIC matches the correspondent of interest Then join to payment_tx for additional attributes such as corridor and amount and aggregate metrics like volume and value by correspondent role and currency for the report"
"Day2_ISO20022_Extraction_Perp","Design a canonical dimension for financial institutions using ISO 20022 information","Explain creating a dim_institution table keyed by BIC or internal id storing institution name country LEI group id regulator codes and risk flags During ingestion map <DbtrAgt> <CdtrAgt> and intermediary agents to this dimension so payment_tx can carry foreign keys to debtor and creditor institutions enabling reporting by bank and group"
"Day2_ISO20022_Extraction_Perp","How would you implement lineage tracking from payment_tx facts back to original ISO messages","Describe adding columns to payment_tx for raw_msg_id and msg_type that reference a central iso_message_raw table storing every XML payload plus metadata A lineage service or view can then show for any payment_tx row all underlying messages pain pacs camt enabling auditors and analysts to review original content and transformations"
"Day2_ISO20022_Extraction_Perp","Explain how to derive a sanctions screening result flag in payment_tx using a separate sanctions hits table","State that you would maintain a sanctions_hits table keyed by payment_tx_id with decision and severity fields and then in ETL or a view set sanctions_hit_flag in payment_tx to one if any associated hit has a status such as confirmed or under_investigation This allows sanctions metrics to be computed directly from the canonical fact"
"Day2_ISO20022_Extraction_Perp","Describe how to extract household inward credit statistics from pacs.008 data","Explain that you would join creditor party ids from stg_pacs008 or payment_tx to a customer dimension flagged for household or personal accounts filter to inbound transactions and apply cross border or domestic flags Then aggregate by corridor currency and possibly purpose codes to meet central bank requirements for household inflows"
"Day2_ISO20022_Extraction_Perp","How would you detect potential data quality issues where ISO messages are mapped into the wrong staging table","Describe setting validation rules that check combinations of message type usage indicators and mandatory fields for each staging table and producing exception reports where structures do not match expectations For example pacs.009 content appearing in stg_pacs008 or missing mandatory FI fields would be flagged for investigation"
"Day2_ISO20022_Extraction_Perp","Design a basic SQL query to calculate daily cross border volumes by debtor customer type from the payment_tx fact","Explain you would select booking_date debtor_customer_type and corridor aggregate count and sum of amount where is_cross_border equals one group by those fields and filter booking_date between required dates This outputs daily volumes split by customer type and corridor for regulators"
"Day2_ISO20022_Extraction_Perp","Explain how to build a reporting dimension for ISO 20022 status codes used in pain.002 and pacs.002","Describe creating a dim_status table listing each status code such as ACSP RJCT along with descriptions grouping attributes such as success or failure and whether they are final or interim The payment_status fact then references this dimension allowing clear reporting on outcomes by category"
"Day2_ISO20022_Extraction_Perp","Describe an approach to support new regulatory reporting requirements without changing core mapping repeatedly","Explain that by storing a full set of ISO fields in staging and golden copy and designing a flexible canonical model you can implement most new regulatory metrics via new views or downstream transformations without re ingesting data Only when new fields become mandatory would ingestion be extended"
"Day2_ISO20022_Extraction_Perp","How would you design a unit test strategy for ISO to canonical mapping logic","State that you would build synthetic XML messages covering edge cases like missing optional fields multi line addresses and long remittance text and define expected rows in staging and payment_tx For each transformation run automated tests comparing actual output with expected values ensuring that key fields like EndToEndId currencies and parties map correctly"
"Day2_ISO20022_Extraction_Perp","Explain how to extract corridor level rejection rates using pacs.002 and payment_tx","Describe joining pacs.002 status records to payment_tx via original references and grouping by corridor to count rejected versus total payments for each corridor This provides rejection rates that can be monitored for operational and regulatory purposes"
"Day2_ISO20022_Extraction_Perp","Design a simple ETL step to populate a currency dimension from ISO 20022 messages","Explain that you would scan distinct currency codes from amount fields in staging tables such as stg_pacs008 and insert new codes into dim_currency with descriptions from an ISO 4217 reference table This dimension can then be used by payment_tx for referential integrity and reporting"
"Day2_ISO20022_Extraction_Perp","How would you represent multi leg transactions such as cover payments in your canonical model","Describe representing each leg as a separate payment_tx row linked by a common group reference such as UETR or a chain id and marking leg roles such as underlying customer payment or cover settlement This allows reporting on both customer and FI legs while preserving their relationship"
"Day2_ISO20022_Extraction_Perp","Explain how to use ISO 20022 data to populate a legal entity exposure report for prudential purposes","Describe using LEIs organisation ids and internal mapping from payment_tx and camt.053 entries to aggregate flows and outstanding balances by legal entity then combining this with credit exposure data to build a complete exposure view required by regulators"
"Day2_ISO20022_Extraction_Perp","Describe how you would manage schema evolution when SWIFT or EPC update their ISO 20022 usage guidelines","Explain that you would track schema versions in the iso_message_raw table update parsers to support new elements and run regression tests against previous sample messages Canonical models should be extended in a backwards compatible way and migration plans for any new mandatory fields should be agreed with reporting and AML stakeholders"
"Day2_ISO20022_Extraction_Perp","How would you design an interface that exposes ISO derived metrics to data scientists without compromising regulatory integrity","State that you would provide curated analytical views of payment_tx and related dimensions with documented semantics and restrictions rather than direct access to raw staging tables Data scientists can use these views for modelling while regulatory logic remains in controlled layers any proposed changes can be tested in sandboxes before affecting official reports"
"Day2_ISO20022_Extraction_Perp","Explain how to join camt.053 entries to AML alerts for case enrichment","Describe that AML alerts referencing payment_tx_ids can be enriched by joining to payment_tx then to camt.053 entry rows using account ids and references such as EndToEndId or TxId This gives investigators actual booking dates balances and any additional narrative present on the statement"
"Day2_ISO20022_Extraction_Perp","Design a quality check to detect inconsistencies between amounts in pain.001 and pacs.008 for the same payment","Explain that you can join stg_pain001 and stg_pacs008 on EndToEndId and compare instructed and settlement amounts and currencies Flag cases where differences exceed expected FX effects or charges and log them for operations review as potential mapping or processing issues"
"Day2_ISO20022_Extraction_Perp","How would you derive an indicator for payments that might involve self funding between accounts of the same customer","Describe using internal customer ids mapped from debtor and creditor parties and setting a flag when they match or when both belong to the same group id This can be used for AML typologies involving circular flows or intra customer transfers"
"Day2_ISO20022_Extraction_Perp","Explain how ISO 20022 party structures can support demographic analysis of retail payments","State that structured party data combined with KYC attributes allows linking payments to age bands residency and income brackets for aggregated analysis while ensuring privacy This uses joins between payment_tx party ids and customer dimension fields rather than relying solely on transaction content"
"Day2_ISO20022_Extraction_Perp","Describe how you would extract data for a report on average transaction size by ISO purpose code and customer type","Explain selecting from payment_tx where purpose_code is not null grouping by purpose_code and debtor_customer_type and computing average amount in transaction currency or a converted reporting currency This gives regulators insight into how payment purposes differ across segments"
"Day2_ISO20022_Extraction_Perp","How would you design a mapping that keeps MT based core systems working while gradually moving reporting to ISO data","Describe that you would continue feeding cores with MT or proprietary formats while building new ingestion of ISO messages into staging and payment_tx facts Reports are gradually switched to use ISO based facts and any gaps are filled from legacy data only where ISO is missing ensuring that the ultimate golden source becomes ISO rather than MT"
"Day2_ISO20022_Extraction_Perp","Explain how ISO 20022 data can help satisfy transparency expectations in correspondent banking reports","Describe using pacs.008 and pacs.009 COV fields for ultimate parties agents and corridors to show regulators full payment chains volumes per correspondent banks and distribution of payments by purpose and counterparty sector This level of detail exceeds what was possible with MT and can be directly drawn from canonical payment_tx and institution dimensions"
"Day2_ISO20022_Extraction_Perp","Summarise how a well designed ISO 20022 ingestion and canonical model benefits AML sanctions and regulatory reporting","State that a robust ingestion layer captures full structured ISO content and preserves a golden copy while canonical payment facts standardise core attributes such as amounts parties purposes and corridors This shared model allows AML sanctions screening and statistical and prudential reports to use consistent high quality data reducing duplication improving transparency and making it easier to adapt to new regulatory requirements"
"Day2_RegReports_Catalogue_Perp","Design the SQL level logic and scheduling approach for a daily cross border AML transaction volume report","Explain that the report should query a canonical payment fact for the previous business day filtering on a cross border flag derived from debtor and creditor country fields and joining to customer dimensions for type and risk rating The SQL aggregates count and sum of amount by corridor customer type and risk band and runs on T plus one after ingestion reconciliation and sanctions AML DQ jobs succeed The scheduler sets dependencies on these upstream tasks and logs run results and parameter values for audit"
"Day2_RegReports_Catalogue_Perp","Describe how you would structure a star schema to support both ECB payment statistics and AML quantitative reports","Describe using a central payment_tx_fact with attributes such as instrument scheme corridor channel and customer ids plus separate aml_alert_fact and sanctions_hit_fact linked by transaction keys Shared dimensions for date customer geography instrument and institution allow ECB statistics to group by instrument corridor and sector while AML reports join alert facts to payments to show transaction volumes associated with alerts without duplicating payment storage"
"Day2_RegReports_Catalogue_Perp","Explain how to design a report catalogue table that can drive parameterised SQL for hundreds of similar returns","Describe defining columns such as report_id name frequency subject_area source_fact view_name group_by_clause where_clause aggregation_definition and output_target Values in these fields are used by a generic report runner which builds SQL statements dynamically or selects stored procedures with parameters while the scheduler uses the frequency field and a calendar to decide execution dates This approach allows new reports to be added largely through metadata rather than new code"
"Day2_RegReports_Catalogue_Perp","Design a scheduling strategy for monthly central bank reports that depend on daily payment loads and reconciliations","Explain that daily ingestion ETL and reconciliations run each business day and write success flags per date A monthly control job for each legal entity checks that all days in the target month have complete and reconciled data Only when this check passes does the scheduler trigger monthly aggregation jobs for ECB BoP or BoE statistics ensuring reports are based on a consistent and complete month"
"Day2_RegReports_Catalogue_Perp","Describe how you would implement reconciliations between payments facts and GL for reporting assurance","State that you would aggregate payment_tx_fact by GL mapping attributes such as product and currency and compare total debits and credits with movements in corresponding GL accounts for the same periods Differences beyond tolerances raise reconciliation exceptions recorded in a recon_fact table Reports cannot be finalised until exceptions are resolved or explained with sign offs documented and retained for audits"
"Day2_RegReports_Catalogue_Perp","Explain how to design data quality checks for a sanctions quantitative report that relies on screening hit facts","Explain that DQ checks should verify that the count of screened items in the screening engine matches screened_item_fact counts for the period and that every hit in sanctions_hit_fact links to a valid payment or customer record Additional checks ensure that mandatory attributes such as list_type corridor and decision are populated and that distributions are within expected ranges with anomalies raised before report generation"
"Day2_RegReports_Catalogue_Perp","Design the schema and logic for a daily scheme SLA report for an instant payments system","Describe creating a scheme_sla_fact with one row per payment including timestamps for receipt scheme acceptance and settlement plus fields for participant and outcome The daily report aggregates for business day T metrics such as percentage settled within specified thresholds average processing time and number of timeouts grouped by participant The scheduler runs the report after all T transactions are loaded and a DQ job confirms completeness of timestamp fields"
"Day2_RegReports_Catalogue_Perp","Describe how you would handle re runs and corrections for a monthly regulatory report when late adjustments arrive from the GL","Explain that the system should support rerunning ETL for the affected period in a controlled mode that reloads fact partitions from corrected sources and recomputes aggregates For the report a new version is generated with updated numbers and both the original and corrected submissions plus explanations are stored in a submission_log table with timestamps and approver information to provide a full audit trail"
"Day2_RegReports_Catalogue_Perp","Explain how you would design lineage documentation for a cross border payments report from source systems to final figures","Describe documenting each step from core banking and ISO message feeds through staging transformations into payment_tx_fact and then into report specific aggregation views using a lineage tool or metadata repository For each report field record source tables columns join conditions filters and aggregation logic and link these to job names and code repositories so that auditors can trace any figure back to raw transactions"
"Day2_RegReports_Catalogue_Perp","Design a control to ensure that a daily AML transaction volume report does not run if sanctions list updates are missing","Explain that a pre condition job checks whether the sanctions list management process has completed successfully for the reporting date and writes a control flag If the flag is absent or indicates failure the scheduler blocks AML volume and sanctions effectiveness reports for that date and sends alerts This ensures reports are not produced from partially screened data"
"Day2_RegReports_Catalogue_Perp","Describe a schema design that supports both daily card fraud statistics and annual fraud trend reports","Explain using a card_tx_fact with one row per transaction including indicators for fraud status channel MCC and SCA usage plus a fraud_case_fact for confirmed cases Dimensions such as date merchant geography channel and instrument support detailed daily aggregates while the same facts aggregated by larger time buckets and dimensions support annual trend charts without separate storage"
"Day2_RegReports_Catalogue_Perp","Explain how you would design SQL to support a weekly sanctions hit rate report segmented by corridor and channel","Describe selecting from sanctions_hit_fact joined to a screened_item_fact and payment_tx_fact filtering on booking_date in the business week and grouping by corridor and channel The query computes hits divided by total screened items for each segment and outputs hit rates and counts The scheduler parameterises the week start and end dates based on a business calendar"
"Day2_RegReports_Catalogue_Perp","Design an approach to manage a library of calculation formulas shared across multiple regulatory reports","Explain that you would store calculation definitions in a calculation_metadata table with fields for formula_name expression and applicable_reports and implement them as views or reusable SQL functions Reports reference these functions rather than embedding logic directly so changes in regulatory definitions can be made in one place and propagated to all dependent reports with proper impact analysis"
"Day2_RegReports_Catalogue_Perp","Describe how to orchestrate ETL and reports for a half yearly large exposures submission using a scheduler","Explain that daily or monthly exposure facts are updated throughout the period with reconciliations to GL At half year end a control job verifies all source data and DQ checks are complete then triggers a set of large exposure aggregation jobs which produce tables or files for submission The scheduler ensures order ingestion to DQ to aggregation to export and notifies stakeholders upon completion or failure"
"Day2_RegReports_Catalogue_Perp","Explain how you would build an internal dashboard that tracks readiness for key regulatory submissions","Describe creating a readiness_fact capturing status of key steps such as ingestion ETL reconciliations DQ checks and report generation per period The scheduler updates this fact after each job and a dashboard visualises status by report and date showing green amber red indicators so management can see which submissions are on track or delayed"
"Day2_RegReports_Catalogue_Perp","Design the data model needed to support reporting of AML alerts by typology geography and customer segment","Explain that aml_alert_fact should include alert_id customer_id typology_code linked_payment_id created_date closed_date and outcome_code joined to dim_geography via customer_id or payment corridor and dim_customer for segment The report groups by typology geography and segment and counts alerts and true positives with median investigation times"
"Day2_RegReports_Catalogue_Perp","Describe how you would implement tolerance based reconciliations between two independent payments systems feeding the same reports","Explain that you would compute aggregates such as count and sum of amount by key dimensions from each source and store them in a recon_comparison table A reconciliation job compares these figures and flags differences exceeding defined tolerances per metric and dimension Exceptions are investigated before the consolidated facts are used for reporting to ensure completeness and consistency"
"Day2_RegReports_Catalogue_Perp","Explain how you would support both business day and calendar day views within the same reporting schema","Describe using a date dimension with attributes for calendar_date business_date business_day_index and period keys Reports that require business day logic join on business_date and use business specific groupings while purely calendar based analyses use calendar_date allowing both styles from the same underlying facts"
"Day2_RegReports_Catalogue_Perp","Design a generic report export framework for delivering regulatory files in various formats","Explain that after aggregation views are built a generic export job reads report metadata specifying format such as CSV XML or XBRL delimiter layout and destination It then renders the dataset accordingly tags files with version and date and transmits them via secure channels while logging checksums and delivery confirmation for evidence"
"Day2_RegReports_Catalogue_Perp","Describe how you would integrate manual adjustments or management overlays into a regulatory report while keeping traceability","Explain that a manual_adjustment table keyed by report_id period and metric stores adjustments with reasons and approver information The final reported figure is base_value plus adjustment and reporting views display both components Reports to regulators use the adjusted value while internal dashboards show the breakdown for transparency"
"Day2_RegReports_Catalogue_Perp","Explain a strategy for migrating legacy spreadsheet based regulatory reports into a governed warehouse based solution","Describe cataloguing existing spreadsheets their inputs and logics mapping them to warehouse facts and dimensions then implementing equivalent SQL or views under version control Run old and new processes in parallel for several cycles to reconcile outputs and decommission spreadsheets once alignment is proven with governance approval"
"Day2_RegReports_Catalogue_Perp","Design how you would capture and store evidence of daily report execution for audit purposes","Explain that each report run writes a record to a run_log table with report_id period start_end_times status row_counts key totals and links to output files Sign offs are stored in a separate approval_log referencing run_ids and associated reconciliations and DQ results This provides a complete history for supervisors and auditors"
"Day2_RegReports_Catalogue_Perp","Describe how a report definition should capture regulatory references and calculation standards","Explain that the definition includes fields for regulation or guideline references such as specific articles or templates plus narrative descriptions of included and excluded items and calculation methods This metadata is stored alongside technical configuration so users and auditors understand the regulatory intent of each metric"
"Day2_RegReports_Catalogue_Perp","Explain how you would design an exception management workflow when a critical report fails DQ checks","Describe that DQ failures create exception cases in a workflow tool where data stewards review issues assign owners and document root cause and remediation Interim decisions such as using alternative data or delaying submission are recorded and only after exceptions are resolved or formally waived can the report run progress to sign off"
"Day2_RegReports_Catalogue_Perp","Design the ETL logic to support an annual report on payment fraud losses by instrument and channel","Explain that card_tx_fact and other payment facts must include flags for confirmed fraud loss amount instrument type and channel The ETL for the annual report filters confirmed fraud cases within the year groups by instrument and channel and sums loss amounts and transaction counts Optionally it also pulls totals for non fraudulent transactions to compute rates"
"Day2_RegReports_Catalogue_Perp","Describe how you would ensure that regulatory report changes are impact assessed before implementation","Explain instituting a change process where any new or revised report requirement triggers an impact analysis on affected facts dimensions ETL and downstream consumers documented in a change record Technical and business stakeholders review the analysis and test plans before development and deployment with post implementation validation comparing old and new figures"
"Day2_RegReports_Catalogue_Perp","Explain how to use a scheduler to coordinate multi entity reporting across several legal entities in a group","Describe configuring jobs to parameterise legal_entity_id and having a parent job loop through entities launching entity specific ETL and reports with dependencies Each run logs separately while group consolidations depend on completion of all relevant entity jobs enabling parallelism with clear status per entity"
"Day2_RegReports_Catalogue_Perp","Design a control framework that links regulatory reporting with model risk management for AML related metrics","Explain that metrics relying on models such as alert scores should reference model ids and versions in fact tables and report definitions Model risk governance ensures models are validated and monitored and report owners are informed of changes Aggregations should indicate when model thresholds changed to aid interpretation and audit"
"Day2_RegReports_Catalogue_Perp","Describe how you would demonstrate to supervisors that your reporting system supports full reproducibility of past submissions","Explain that you retain historical fact partitions dimensional snapshots and report code for each period and store submission metadata including run_ids and git hashes When requested you can rerun the report for a given period in a controlled replay environment and reproduce submitted figures within documented tolerances providing run logs and reconciliations as evidence"
"Day2_RegReports_Catalogue_Perp","Explain how to design a payments reporting schema that supports both gross and netted views for high value systems","Describe storing individual transactions in payment_tx_fact with fields describing whether they were processed gross or contributed to multilateral netting and maintaining a separate net_position_fact per cycle containing net pay or receive amounts Reports can then use either gross transactions or net positions depending on regulatory template requirements"
"Day2_RegReports_Catalogue_Perp","Design the logic for a weekly report showing AML alerts per thousand cross border payments by corridor","Explain that you would join aml_alert_fact to payment_tx_fact on transaction keys and count alerts per corridor and week while separately counting total cross border payments in payment_tx_fact for the same corridor and week The metric is alerts divided by total payments times one thousand computed in a weekly aggregation job driven by the business calendar"
"Day2_RegReports_Catalogue_Perp","Describe how you would manage report definitions and ETL when a regulator introduces a new corridor classification scheme","Explain that you would update the corridor dimension to include new classification codes and mapping from country pairs then adjust report definitions to group by the new attribute rather than rebuilding core facts ETL is updated to populate the new corridor attributes and regression tests confirm that existing reports behave as expected under the new scheme"
"Day2_RegReports_Catalogue_Perp","Explain how to architect reporting so that the same facts can serve both internal management dashboards and formal regulatory templates","Describe storing all core metrics in canonical facts and dimensions and building separate presentation layers one for internal dashboards and one for regulatory exports Both layers use the same validated data but can apply additional internal calculations or visualisations while regulatory templates adhere strictly to external specifications"
"Day2_RegReports_Catalogue_Perp","Design a monitoring dashboard for the scheduler itself with focus on regulatory reporting pipelines","Explain that the scheduler should feed a meta_fact capturing job run status durations and failure reasons per day and job type A dashboard shows heatmaps of success failure and duration trends by subject area and regulator highlighting bottlenecks and allowing early corrective action before deadlines are at risk"
"Day2_RegReports_Catalogue_Perp","Describe how you would integrate external economic calendars into reporting schedules for cross border statistics","Explain that a calendar dimension would include public holidays and market closures for relevant jurisdictions Reports depending on settlement activity adjust reporting windows and cut offs accordingly and the scheduler uses this calendar to avoid running or to adapt logic on days where activity is atypical"
"Day2_RegReports_Catalogue_Perp","Explain how you would model and report scheme specific cut off breaches in a CHAPS style high value payment system","Describe storing planned cut off times in a scheme_schedule dimension and actual submission and settlement times in payment_tx_fact A report identifies transactions where submission occurred after scheme cut off or settlement missed window and aggregates counts and values by participant and day enabling scheme level SLA oversight"
"Day2_RegReports_Catalogue_Perp","Summarise how a metadata and scheduler driven architecture supports scaling to thousands of regulatory reports","Explain that by centralising definitions of metrics filters groupings frequencies and dependencies in metadata tables and orchestrating ETL and report generation via a scheduler that reads this metadata the bank can add or modify large numbers of reports without duplicating code ensuring consistent logic strong controls and visible status across the entire reporting portfolio"
Day2_RegReports_Catalogue_Gemi,"Design the SQL-level logic and scheduling approach for a daily 'Cross-Border AML Transaction Volume' report.","SQL: `SELECT Sender_Country, Receiver_Country, COUNT(*) as Vol, SUM(Amount_USD) as Val FROM Fact_Payment WHERE Sender_Country <> Receiver_Country AND Date = 'Yesterday' GROUP BY 1, 2`. Scheduling: Run at 02:00 AM (after ETL). Dependency: Depends on `Payment_ETL_Success` and `Ref_Data_Load`. Output: CSV to SFTP for Compliance Team."
Day2_RegReports_Catalogue_Gemi,"Explain how you would handle 'Re-runs' in your scheduler if the Source Data (Payment Engine) sends a correction file for yesterday.","Design the ETL to be Idempotent. The Scheduler triggers a 'Correction Job'. Step 1: Delete rows in `Fact_Payment` where `Date = Yesterday` AND `Source = Payment_Engine`. Step 2: Load new file. Step 3: Trigger 'Refresh Materialized Views'. Step 4: Regenerate Yesterday's Reports and issue 'Version 2'."
Day2_RegReports_Catalogue_Gemi,"Design a schema for the 'Sanctions False Positive' MI report. What tables and fields are needed?","Fact Table: `FACT_SANCTIONS_ALERTS`. Fields: `AlertID`, `PaymentID`, `Date`, `Screening_List_ID` (Dim), `Outcome` (False Positive / True Match), `Investigation_Time_Mins`. Dimension: `DIM_SCREENING_LIST` (List Name, Version). Metric: `False_Positive_Rate = Count(False Pos) / Count(Total Alerts)`."
Day2_RegReports_Catalogue_Gemi,"Describe the dependency graph for generating the 'Monthly Prudential Liquidity' return.","1. Ingest `GL_Balances` (Task A) and `Cashflow_Forecasts` (Task B). 2. Wait for `A` and `B` to complete. 3. Run `Calc_Liquidity_Metrics` (Task C - applies LCR logic). 4. Run `Data_Quality_Check` (Task D). 5. If D passes, Run `Generate_Regulatory_XML` (Task E). 6. If D fails, Alert Ops."
Day2_RegReports_Catalogue_Gemi,"How would you calculate 'Payment System Availability' (SLA) for a monthly report using log data?","Ingest System Logs. Filter for 'Heartbeat' or 'Transaction' events. Logic: `Total_Minutes = DaysInMonth * 1440`. `Downtime_Minutes = Sum(Duration of Outage Events)`. `Availability % = (Total - Downtime) / Total * 100`. Group by System (CHAPS, FPS)."
Day2_RegReports_Catalogue_Gemi,"Design the logic to map 'Free Text' Purpose Codes from legacy systems to ISO 20022 codes for the 'Balance of Payments' report.","Create `Map_Purpose` table: `Keyword` (e.g., 'Sal', 'Payroll'), `ISO_Code` (SALA). Logic: `LEFT JOIN Fact_Payment P ON Map_Purpose M WHERE P.Narrative LIKE '%' + M.Keyword + '%'`. Prioritize exact matches. Default to 'OTHR' if no match. Report `ISO_Code`."
Day2_RegReports_Catalogue_Gemi,"Explain how to implement 'Cut-Off' logic for a report that must include payments up to 23:59:59 UTC.","Ensure all timestamps in `Fact_Payment` are normalized to UTC. Query filter: `WHERE Transaction_Timestamp >= '2023-10-01 00:00:00' AND Transaction_Timestamp <= '2023-10-01 23:59:59'`. Do not use `Date` parts alone if the source system runs on local time; convert explicitly."
Day2_RegReports_Catalogue_Gemi,"Design a 'Data Quality Gate' that stops report generation if critical fields are missing.","Create a SQL Task `Check_DQ` before the Report Task. Query: `SELECT COUNT(*) FROM Fact_Payment WHERE Country_Code IS NULL OR Amount IS NULL`. Logic: If `Count > 0`, Exit with Error Code 1 (Fail). Scheduler catches Fail and sends P1 PagerDuty alert 'DQ Breach - Report Halted'."
Day2_RegReports_Catalogue_Gemi,"How would you architect the storage for 7 years of regulatory data to balance cost and performance?","Use 'Hot/Cold' Partitioning. Current Year (Hot): Stored on High-Performance SSD / Cluster for fast dashboards. Year 2-7 (Cold): Offloaded to Object Storage (S3/Blob) in Parquet format, accessible via External Tables (e.g., Spectrum/External Table) for ad-hoc audit queries. Archive > 7 years: Glacier."
Day2_RegReports_Catalogue_Gemi,"Describe the process to generate a 'Nil Return' for a daily Sanctions report when no hits occurred.","Logic: `SELECT Count(*) FROM Fact_Sanctions WHERE Date = Today`. If `Count == 0`: Trigger `Task_Generate_Nil_XML`. This task populates the standard header but leaves the body empty (or adds `<NoActivity>True</NoActivity>` tag) and submits. Ensures regulator knows system is active."
Day2_RegReports_Catalogue_Gemi,"Design the aggregation logic for a 'High Risk Customer' monthly transaction report.","Identify High Risk Customers from `Dim_Customer` (`Risk_Rating = 'High'`). Join `Fact_Payment`. Aggregation: `SUM(Amount)`, `COUNT(Txns)` grouped by `Customer_ID`, `Counterparty_Country`. Filter: `Date BETWEEN Month_Start AND Month_End`. Flag if `SUM(Amount) > Threshold`."
Day2_RegReports_Catalogue_Gemi,"Explain how to handle 'FX Conversion' for a global report aggregating EUR, GBP, and USD into USD.","Table `Fact_FX_Rates` (Date, Currency, Rate_to_USD). Join `Fact_Payment P` to `Fact_FX_Rates R` on `P.Currency = R.Currency` AND `P.Date = R.Date`. Calc: `Amount_USD = P.Amount_Local * R.Rate_to_USD`. Handle missing weekend rates by taking `MAX(Date) <= Txn_Date` (Last known rate)."
Day2_RegReports_Catalogue_Gemi,"Design a 'Variance Check' control to detect anomalies before submission.","SQL: Calculate `Current_Month_Total`. Retrieve `Last_Month_Total`. Calc `Variance % = (Current - Last) / Last`. Logic: If `ABS(Variance) > 10%`, Trigger 'Warning' status in Scheduler. Require manual 'Approve' click to proceed to Submission task."
Day2_RegReports_Catalogue_Gemi,"How would you report on 'Payment Turnaround Time' (Latency) for CHAPS payments?","Data: `Fact_Payment` with columns `Time_Received` and `Time_Settled`. Logic: `Latency_Mins = DATEDIFF(minute, Time_Received, Time_Settled)`. Report: Bucket into bins (0-5m, 5-15m, >60m). Calculate `% in 0-5m`. Filter: `Scheme = 'CHAPS'`."
Day2_RegReports_Catalogue_Gemi,"Design a solution for 'Audit Trail' of manual adjustments to regulatory reports.","Do not edit the Fact Table. Create `Adj_Regulatory` table (`Report_ID`, `Adjustment_Amount`, `Reason`, `User`, `Timestamp`). Report Logic: `SUM(Fact.Amount) + SUM(Adj.Adjustment_Amount)`. This preserves the system truth while allowing correction, with full traceability of the delta."
Day2_RegReports_Catalogue_Gemi,"Explain how to handle 'Multi-Leg' payments (Customer -> Bank -> Clearing -> Bank -> Customer) in a 'Volume Report'.","Decide on the 'Counting Point'. Usually 'Customer-Facing Legs' (pacs.008) are counted, while 'Bank-Facing Legs' (pacs.009) are excluded to avoid double-counting the economic activity. Filter `Msg_Type = 'pacs.008'` in the report logic."
Day2_RegReports_Catalogue_Gemi,"Design a 'Duplicate Payment' check report.","Query: `SELECT Sender, Receiver, Amount, Date, COUNT(*) FROM Fact_Payment GROUP BY 1,2,3,4 HAVING COUNT(*) > 1`. This identifies potential operational errors or replay attacks. Schedule: Daily, pre-submission."
Day2_RegReports_Catalogue_Gemi,"How would you generate a 'Top 10 Counterparties' report for a specific corporate client?","Filter `Fact_Payment` by `Sender_ID`. Group by `Beneficiary_Name`. `SUM(Amount)`. `ORDER BY Sum_Amount DESC`. `LIMIT 10`. Use for 'Know Your Customer's Customer' (KYCC) analysis."
Day2_RegReports_Catalogue_Gemi,"Describe the schema for a 'SLA Performance' table.","Table: `FACT_SLA_METRICS`. Columns: `Date`, `Scheme`, `Total_Volume`, `SLA_Target_Time` (e.g., 5 mins), `Volume_Met_SLA`, `Volume_Missed_SLA`, `Percent_Met`. Populated daily by an aggregation job."
Day2_RegReports_Catalogue_Gemi,"Design the logic to identify 'Structuring' (Smurfing) for a SAR report.","Window Function: `SUM(Amount) OVER (PARTITION BY Customer ORDER BY Time RANGE BETWEEN INTERVAL '24 HOURS' PRECEDING AND CURRENT ROW)`. If `Rolling_Sum > 10,000` AND `Individual_Txn < 10,000`, Flag as 'Potential Structuring'."
Day2_RegReports_Catalogue_Gemi,"Explain how to report 'Payment Rejections' by Reason Code.","Source: `pacs.002` or `camt.054` logs. Extract `StatusReason` code (e.g., AM04 - Insufficient Funds). Aggregation: `Count(*)` Group By `ReasonCode`. Visualize in a Pareto Chart to identify top failure causes."
Day2_RegReports_Catalogue_Gemi,"Design a 'Data Lineage' metadata table to track report sources.","Table: `META_REPORT_LINEAGE`. Columns: `Report_ID`, `Report_Field`, `Source_System`, `Source_Table`, `Source_Column`, `Transformation_Rule`. Populate this during design. Use it to answer Auditor queries: 'Where does this number come from?'."
Day2_RegReports_Catalogue_Gemi,"How would you report on 'Inactive Accounts' for maintenance?","Anti-Join: `SELECT C.ID FROM Dim_Customer C LEFT JOIN Fact_Payment P ON C.ID = P.Customer_ID AND P.Date > DATEADD(year, -1, TODAY) WHERE P.ID IS NULL`. Identifies customers with zero transactions in the last year."
Day2_RegReports_Catalogue_Gemi,"Design a 'Golden Source' validation check between the Data Lake and the Warehouse.","Control Job: `SELECT SUM(Amount) FROM Lake_Raw_File` vs `SELECT SUM(Amount) FROM Warehouse_Fact_Table`. If Difference != 0, Halt Pipeline. Ensures ETL logic didn't drop rows."
Day2_RegReports_Catalogue_Gemi,"Explain how to generate a 'Sectoral Breakdown' report (e.g., Agriculture vs Tech).","Join `Fact_Payment` to `Dim_Customer`. `Dim_Customer` must have `SIC_Code` or `NACE_Code` (Industry Standard codes). Group by `SIC_Code`. Map codes to High-Level Sectors (e.g., '1000-1999' = Agriculture)."
Day2_RegReports_Catalogue_Gemi,"Design a 'Payment Flow by Hour' heatmap query.","`SELECT EXTRACT(HOUR FROM Time_Settled) as Hr, Scheme, COUNT(*) FROM Fact_Payment GROUP BY 1, 2`. Use to visualize peak loads and staffing requirements."
Day2_RegReports_Catalogue_Gemi,"How would you handle 'Schema Changes' (e.g., new column) in the Source System?","Schema Evolution: 1. Data Lake accepts new column (Schema-on-Read). 2. Update ETL Mapping to map new column to `Fact_Payment` (or generic `Additional_Data` JSON column). 3. Update Report View to expose column. 4. Backfill if necessary."
Day2_RegReports_Catalogue_Gemi,"Design a 'Customer Risk Rating' change report.","Source: `Dim_Customer_History` (SCD Type 2). Logic: `SELECT Customer_ID, Old_Rating, New_Rating, Change_Date FROM Dim_Customer_History WHERE Change_Date = 'Last Month' AND Old_Rating != New_Rating`. Tracks risk migration."
Day2_RegReports_Catalogue_Gemi,"Explain how to calculate 'Net Settlement Position' for a scheme like BACS.","`SELECT Scheme, Date, SUM(CASE WHEN Direction='IN' THEN Amount ELSE 0 END) - SUM(CASE WHEN Direction='OUT' THEN Amount ELSE 0 END) as Net_Position`. Positive = Long, Negative = Short (Need to fund)."
Day2_RegReports_Catalogue_Gemi,"Design a 'Failed Sanctions Screening' retry job.","If Screening System is down, rows stay in `Status='SCREENING_PENDING'`. Retry Job: `SELECT * FROM Queue WHERE Status='SCREENING_PENDING' AND Retry_Count < 5`. Resubmit to Screening API. Increment Retry Count. If > 5, Alert Ops."
Day2_RegReports_Catalogue_Gemi,"How would you report on 'Direct Debit Indemnity Claims'?","Filter `Fact_Payment` for `Transaction_Type = 'DD_Refund'` or specific R-Code (`MD06`). Sum Amount. Calculate `Claim Rate = Claims / Total DD Volumes`. High rate triggers risk review of the Originator."
Day2_RegReports_Catalogue_Gemi,"Design a 'VIP Customer' monitoring report.","Flag VIPs in `Dim_Customer`. Report: `SELECT * FROM Fact_Payment P JOIN Dim_Customer C ON P.Cust_ID = C.ID WHERE C.Is_VIP = 'Y' AND P.Amount > 10000`. High-touch monitoring for high-net-worth individuals."
Day2_RegReports_Catalogue_Gemi,"Explain how to map 'Country of Origin' for a payment involving an Intermediary.","Do NOT use Intermediary Country. Trace back to `Ordering Customer Country` or `Instructing Agent Country` (if customer unknown). In ISO 20022: `<Dbtr><PstlAdr><Ctry>`. If missing, fallback to `<InstgAgt>`."
Day2_RegReports_Catalogue_Gemi,"Design a 'Report Distribution' system.","Report Engine generates PDF. Metadata saved to `Report_Log` (Report_ID, Date, Path). Distribution Job checks `Subscription_Table` (Report_ID, Email). Emails the PDF (or secure link) to the subscriber. Logs 'Sent_Time'."
Day2_RegReports_Catalogue_Gemi,"How would you report 'Nostro Balances' for LCR?","Snapshot Table: `FACT_NOSTRO_BALANCE`. Columns: `Account_ID`, `Currency`, `Date`, `Balance`. LCR Logic: Filter `Date=Today`. Sum Balances. Apply 'Haircut' (Risk weighting) if required by regulation (e.g., non-convertible currency)."
Day2_RegReports_Catalogue_Gemi,"Design a 'Missing Data' exception report for Onboarding.","`SELECT Customer_ID, Name, 'Missing DOB' as Issue FROM Dim_Customer WHERE DOB IS NULL UNION ALL SELECT Customer_ID, Name, 'Missing Address' FROM Dim_Customer WHERE Address IS NULL`. Operational team uses this worklist to fix data."
Day2_RegReports_Catalogue_Gemi,"Explain how to report 'Transaction Fees' separately from Principal.","If fees are `deducted`, source `Original_Amount` and `Settled_Amount`. `Fee = Original - Settled`. If fees are `charged separately`, source the separate Fee Transaction line item. Aggregate Fees by Product/Customer."
Day2_RegReports_Catalogue_Gemi,"Design a query to find 'Dormant Accounts' suddenly becoming active (Risk Flag).","Subquery: Customers with 0 txns in last 12 months. Main Query: Join Subquery to `Fact_Payment` (Current Month). If match found, Alert 'Dormant Account Reactivation'. Potential fraud/money laundering."
Day2_RegReports_Catalogue_Gemi,"How would you report on 'Manual Payments' vs 'STP Payments'?","Filter `Fact_Payment` by `Channel` or `Entry_Mode`. `Manual` = Branch/Phone. `STP` = Electronic/File. Metric: `STP % = Count(STP) / Count(Total)`. KPI for Ops efficiency."
Day2_RegReports_Catalogue_Gemi,"Design a 'Geographic Risk Heatmap' data source.","Aggregation: `SELECT Country, SUM(Amount) as Exposure FROM Fact_Payment GROUP BY Country`. BI Tool renders this as a World Map, coloring countries by Exposure magnitude."
Day3_RegTech_Architecture_Gemi,"Design the data flow and tables to support a new 'Cross-Border Sanctions Effectiveness' report, given existing Kafka topics (Payments, Hits) and warehouse tables.","Flow: 1. Subscribe to Kafka `Payment_Topic` and `Sanctions_Hit_Topic`. 2. Stream process (Spark/Flink) to join on `PaymentID`. 3. Write enriched stream to Silver Table `LINK_PAYMENT_SANCTIONS`. 4. Batch Job (Daily) aggregates data into Gold Fact `FACT_SANCTIONS_METRICS` (Dimensions: Corridor, List_Type; Metrics: Total_Screened, Total_Hits, False_Positives). 5. BI Tool connects to Gold Fact."
Day3_RegTech_Architecture_Gemi,"Design the schema for `DIM_SANCTIONS_LIST` to handle changes in list entries over time (e.g., an entity added then removed).","Use SCD Type 2. Columns: `Sanction_Key` (Surrogate PK), `Entity_ID` (Natural Key), `Entity_Name`, `List_Source` (OFAC/UN), `Listing_Reason`, `Effective_From_Date`, `Effective_To_Date`, `Is_Active_Flag`. This allows linking a payment to the exact state of the list *at the transaction time*."
Day3_RegTech_Architecture_Gemi,"Describe how to implement a 'Reconciliation Control' between the Payment Engine (Source) and the Data Warehouse (Target).","1. Source System generates a `Control_File` at EOD containing `Row_Count` and `Sum_Amounts` per Currency. 2. Pipeline ingests Control File to `AUDIT_CONTROL` table. 3. Pipeline runs query on Warehouse `FACT_PAYMENT` for same period. 4. Comparison Logic: If `Source.Sum != Target.Sum`, Trigger P1 Alert. 5. Dashboard shows RAG status of Rec."
Day3_RegTech_Architecture_Gemi,"How would you handle 'Schema Drift' in the pipeline if the Source System adds a new column 'Risk_Score_V2' without warning?","Use a Lakehouse 'Schema Evolution' feature (e.g., Delta Lake `mergeSchema`). 1. Bronze ingestion job is configured to allow new columns. 2. New column appears in Bronze table automatically. 3. Silver job might need manual update to map it to Canonical Model, or can be dynamic. 4. Alert Data Engineers that 'New Column Detected'."
Day3_RegTech_Architecture_Gemi,"Design the lineage documentation strategy for the 'Liquidity Coverage Ratio' (LCR) report to meet BCBS 239.","Use a Metadata Tool (e.g., Solidatus). 1. Map Report Cell `HQLA_Level_1`. 2. Link to Aggregation Logic `SUM(Cash)`. 3. Link to Gold Table `FACT_BALANCE`. 4. Link to Silver Logic `Normalisation`. 5. Link to Bronze Table `RAW_GL_DUMP`. 6. Link to Source `General_Ledger`. 7. Tag Critical Data Elements (CDEs) and ownership at each hop."
Day3_RegTech_Architecture_Gemi,"Explain how to architect a 'Real-Time Fraud Dashboard' using Kafka and a serving layer.","1. Ingest Payment Events to Kafka. 2. Stream Processor (Flink) calculates rolling aggregates (e.g., 'Amount last 1 hour'). 3. Join with 'Risk Profile' from cached reference data. 4. Sink results to a Low-Latency OLAP Store (e.g., ClickHouse, Pinot, or Redis). 5. Dashboard (Grafana/Superset) queries the OLAP store with sub-second latency."
Day3_RegTech_Architecture_Gemi,"Design a 'Data Quality Firewall' for incoming ISO 20022 messages.","Implement a validation step in the Bronze-to-Silver pipeline. Rules: 1. `UETR` is present and valid UUID. 2. `Amount` > 0. 3. `Currency` is valid ISO code. Logic: Valid records $\rightarrow$ `SILVER_PAYMENTS`. Invalid records $\rightarrow$ `QUARANTINE_PAYMENTS` table + Alert. Do not stop the pipe, but isolate bad data."
Day3_RegTech_Architecture_Gemi,"How would you model the relationship between a 'Payment' and multiple 'Parties' (Sender, Receiver, Intermediary) in the Canonical Model?","Fact Table: `FACT_PAYMENT` contains `Sender_Key`, `Receiver_Key`. For complex chains (Intermediaries), use a `BRIDGE_PAYMENT_ACTORS` table: `Payment_Key`, `Party_Key`, `Role_Type` (e.g., 'Intermediary1', 'UltimateDebtor'). This allows flexible N-party querying."
Day3_RegTech_Architecture_Gemi,"Describe the strategy for partitioning the `FACT_PAYMENT` table (10 billion rows) for optimal query performance.","Partition by `Settlement_Date` (Day or Month granularity). This aligns with 95% of regulatory queries (e.g., 'Show me last month'). Within partitions, use Z-Ordering or Clustering on high-cardinality query fields like `Scheme_ID` or `Sender_Country` to further skip data."
Day3_RegTech_Architecture_Gemi,"Design a process to 'Backfill' 2 years of history into a new 'Customer Risk' dimension.","1. Retrieve 2 years of raw Customer Logs (Audit Trail) from Bronze. 2. Write a Spark job to replay the changes in order. 3. Generate SCD Type 2 history records (`Valid_From`, `Valid_To`) based on change timestamps. 4. Write to `DIM_CUSTOMER_HISTORY`. 5. Validate against a sample of past reports to ensure accuracy."
Day3_RegTech_Architecture_Gemi,"Explain how to implement 'Row-Level Security' (RLS) in the Warehouse for a global team (UK can't see US data).","1. Create `USER_ACCESS` table mapping `User` to `Region`. 2. Define Policy on `FACT_PAYMENT`: `WHERE Region_Code IN (SELECT Region FROM USER_ACCESS WHERE User = Current_User)`. 3. Database applies this predicate automatically to every query run by that user."
Day3_RegTech_Architecture_Gemi,"Design the architecture for archiving data > 7 years old while keeping it queryable for audit.","1. Define Lifecycle Policy on Cloud Storage: Move files > 7 years to 'Cold/Archive' tier (cheaper). 2. Use 'External Tables' in the Warehouse that point to the Archive path. 3. Users can query `FACT_PAYMENT_ARCHIVE` via SQL, but accept higher latency/cost. No restore required."
Day3_RegTech_Architecture_Gemi,"How would you handle 'Late Arriving Data' (e.g., a payment settles 2 days late) in a Monthly Report?","1. Use `Transaction_Date` for reporting logic, not `Ingestion_Date`. 2. If the 'January' report is run on Feb 1st, run a 'Delta Check' on Feb 5th. 3. Identify rows with `Transaction_Date = Jan` but `Ingestion_Date > Feb 1`. 4. If material, generate an 'Adjustment' file for the regulator."
Day3_RegTech_Architecture_Gemi,"Design the 'Deduplication' logic for a stream of payment status updates (`pacs.002`).","Use a windowed deduplication in the Stream Processor. Key: `Original_Msg_ID` + `Status`. Window: 1 hour (assuming duplicates arrive close together). Logic: Emit only the *first* event seen for that Key. For long-term dedup, rely on the Merge/Upsert logic into the Silver Delta/Iceberg table."
Day3_RegTech_Architecture_Gemi,"Explain how to use 'Watermarking' in a streaming pipeline for regulatory windowing.","Set a Watermark of '10 minutes'. This tells the engine: 'Don't close the 10:00-10:15 aggregation window until 10:25'. Allows late events to be included in the correct bin. Events arriving *after* the watermark are dropped or sent to a side-output for manual review."
Day3_RegTech_Architecture_Gemi,"Design a 'Metadata-Driven' ingestion framework for 50 different source files.","Create a `CONFIG_TABLE`: `Source_ID`, `File_Pattern`, `Target_Table`, `Schema_Definition`. Build a generic Spark/Python generic job. Job reads Config $\rightarrow$ Finds Files matching Pattern $\rightarrow$ Validates against Schema $\rightarrow$ Loads to Target. Adding a source = Adding a row to Config, not new code."
Day3_RegTech_Architecture_Gemi,"How would you report on 'Payment Chains' (tracing money A $\rightarrow$ B $\rightarrow$ C) using Graph technology?","1. Export `FACT_PAYMENT` (Sender, Receiver, Amount, Time) to a Graph DB (Neo4j/TigerGraph). 2. Model: Nodes = Accounts, Edges = Payments. 3. Query: Path Traversal (e.g., Cypher query) to find flow from A to C within a time window. 4. Visualize in a graph explorer tool."
Day3_RegTech_Architecture_Gemi,"Design a 'Sensitive Data Discovery' scanner for the Data Lake.","1. Scheduled Job scans random sample of Bronze files. 2. Applies Regex/NLP for PII patterns (Credit Cards, Emails). 3. If Match > Threshold, check Data Catalog tags. 4. If column not tagged 'Sensitive', Alert Security Team and auto-tag in Catalog. Prevents PII leakage."
Day3_RegTech_Architecture_Gemi,"Explain how to map legacy 'Free Text' address fields to a Structured Address model in the Silver Layer.","Use an Address Parsing/Standardization library (e.g., Libpostal, Google Address API). Attempt to extract `Street`, `City`, `Zip`. Store Parsed values in `DIM_ADDRESS`. Keep Original Text in `Raw_Address`. Assign a `Confidence_Score`. If Score < Low, flag for manual review."
Day3_RegTech_Architecture_Gemi,"Design the logic to calculate 'Daily Average Balance' from a stream of Transaction events.","1. Start with `Opening_Balance` (from yesterday). 2. Order transactions by time. 3. Calculate `Running_Balance` after each txn. 4. Weighted Average: `Sum(Balance * Duration_Held) / 24 Hours`. 5. Store in `FACT_DAILY_BALANCE`."
Day3_RegTech_Architecture_Gemi,"How would you implement 'GDPR Right to be Forgotten' in a read-only Data Lake?","'Crypto-shredding'. 1. Store PII encrypted with a specific Key per User. 2. Store Keys in a KMS. 3. When 'Forget' request arrives, delete the User's Key from KMS. 4. The PII in the lake becomes unreadable garbage. No need to rewrite Petabytes of files."
Day3_RegTech_Architecture_Gemi,"Design a 'Test Data Management' strategy for the Reporting Pipeline.","1. Production Copy is NOT allowed. 2. Use a Synthetic Data Generator based on Prod Schema stats. 3. Generate datasets with specific edge cases (e.g., 'Sanctions Hit', 'Null Country'). 4. Load into `DEV_LAKE`. 5. Run pipelines against Synthetic Data to verify logic."
Day3_RegTech_Architecture_Gemi,"Explain the 'Fan-In' architecture for ingesting data from 20 different regional payment engines.","1. Standardization: Deploy 'Adapters' near each source to convert local format to standard Avro/JSON schema. 2. Transport: All Adapters publish to a central Kafka Cluster (or regional clusters mirrored to central). 3. Ingest: Single Spark Streaming job reads from Kafka and writes to Silver Layer."
Day3_RegTech_Architecture_Gemi,"Design a 'Cost Allocation' report for Cloud Data usage.","1. Tag all resources (Compute, Storage) with `Cost_Center`. 2. Export Cloud Billing Logs to Warehouse. 3. Query: `Sum(Cost)` Group By `Tag`. 4. Allocation Logic: If a Shared Cluster is used, allocate cost based on `Job_Execution_Time` per Project."
Day3_RegTech_Architecture_Gemi,"How would you detect 'Duplicate Payments' across two different source systems (e.g., internal transfer vs SWIFT)?","1. Normalize `Amount`, `Currency`, `Value_Date`. 2. Fuzzy Match `Narrative`/`Reference`. 3. Join on `Sender` and `Receiver`. 4. Logic: If Match Score > 90% and Time Diff < 1 min, Flag 'Potential Duplicate'. 5. Human Review."
Day3_RegTech_Architecture_Gemi,"Design the schema for `DIM_CURRENCY` to support multi-currency reporting.","Columns: `Currency_Key`, `ISO_Code` (USD), `Numeric_Code` (840), `Name`, `Minor_Unit` (2 decimal places), `Is_Active`, `Country_Primary`. Essential for correctly formatting amounts (e.g., JPY has 0 decimals) in reports."
Day3_RegTech_Architecture_Gemi,"Explain how to handle 'Maintenance Windows' in the Source System for a Streaming Pipeline.","1. Source buffers events or Kafka retains logs. 2. Pipeline stays up (idle). 3. When Source returns, it floods events. 4. Pipeline must handle 'Backpressure' (scale up workers) to catch up. 5. Monitor 'Consumer Lag'. Alert if Lag doesn't decrease after X hours."
Day3_RegTech_Architecture_Gemi,"Design a 'Cross-Border' flag logic in the Silver Layer.","1. Lookup `Sender_Bank_Country` (from BIC) and `Receiver_Bank_Country`. 2. Logic: `IF Sender_Country != Receiver_Country THEN 'Y'`. 3. Edge Case: Handle 'EEA' logic (SEPA might be cross-border but treated domestically for some rules). Use a `Ref_Region_Group` table."
Day3_RegTech_Architecture_Gemi,"How would you generate a unique `Payment_ID` for a Warehouse that is robust against collisions?","Do not use DB Sequence (bottleneck). Use UUID (v4) or a Hash of (`Source_System_ID` + `Source_Unique_Ref`). Hashing ensures that if the same payment is reloaded, it generates the same ID (idempotency)."
Day3_RegTech_Architecture_Gemi,"Design a 'Version Control' strategy for DDL and SQL Logic.","1. Git Repo structure: `/ddl`, `/transformations`, `/tests`. 2. CI/CD: Commit triggers 'Linting'. Merge to Main triggers deployment to 'UAT'. Tag release triggers deployment to 'PROD'. 3. Tools: Flyway/Liquibase for DB migrations, dbt for SQL logic."
Day3_RegTech_Architecture_Gemi,"Explain how to use 'Bloom Filters' to optimize lookup of 'Sanctioned Names' in a massive dataset.","1. Create Bloom Filter of all Sanctioned Names (small memory footprint). 2. Broadcast to all worker nodes. 3. Pass: Check every transaction name against Filter. 4. If 'Maybe', perform expensive full string match. If 'No', skip. Drastically reduces compute."
Day3_RegTech_Architecture_Gemi,"Design a 'Business Glossery' to Metadata mapping.","Tool: Collibra. Concept: 'Net Liquidity'. Definition: 'Cash - Outflows'. Linkage: Mapped to Columns `FACT_BALANCE.CASH_AMT` and `FACT_FLOWS.OUTFLOW_AMT`. Transformation: Link to Calculation Logic. Owner: 'Head of Treasury'."
Day3_RegTech_Architecture_Gemi,"How would you handle 'Unstructured Data' (PDF Invoices) attached to payments?","1. Store PDF in Blob Storage. 2. Use OCR/AI Service (e.g., Textract) to extract Key-Values (Invoice #, Date, Amount). 3. Store extracted JSON in Silver Layer linked to Payment ID. 4. Enable Search on extracted text."
Day3_RegTech_Architecture_Gemi,"Design a 'Threshold Alerting' system for Transaction Monitoring.","1. Define Rules in DB (`Rule_ID`, `Threshold_Amt`, `Time_Window`, `Segment`). 2. Streaming Job joins Transaction stream with Rules. 3. Window aggregation: `Sum(Amt)` over `Time_Window`. 4. If `Sum > Threshold`, emit `Alert_Event` to Case Management System."
Day3_RegTech_Architecture_Gemi,"Explain 'Data Virtualization' vs 'Data Movement' for Ad-hoc Reporting.","Virtualization (e.g., Denodo, Starburst): Query data where it lives (Source DBs) without copying. Good for ad-hoc, low volume. Movement (ETL): Copy to Warehouse. Good for heavy, repetitive, historical reporting (Performance/Isolation). Use Virtualization for 'Data Exploration'."
Day3_RegTech_Architecture_Gemi,"Design a 'Self-Healing' pipeline pattern.","1. Job Fails. 2. Orchestrator (Airflow) catches error. 3. Retry Policy: Retry 3 times with backoff. 4. If still fail, check 'Error Class'. 5. If 'Transient' (Network), Alert Ops. If 'Data' (Schema Mismatch), route data to DLQ and mark Job 'Success with Warnings' to allow subsequent tasks to run (if safe)."
Day3_RegTech_Architecture_Gemi,"How would you model 'Exchange Rates' for historical reporting?","Table: `DIM_FX_RATE`. PK: `Currency_Pair`, `Rate_Type` (Spot/EOD), `Date`. Join `Fact_Payment` on `Currency` and `Date`. Note: Handle 'Inverse Rates' and 'Triangulation' (via USD) if direct pair rate doesn't exist."
Day3_RegTech_Architecture_Gemi,"Design a 'Data Retention' automated purge process.","1. Partition Tables by Date. 2. Scheduled Job runs daily. 3. Logic: `ALTER TABLE DROP PARTITION WHERE Date < Current_Date - 7_Years`. 4. Verify: Ensure backup/archive exists before drop. 5. Log: 'Purged [Partition] at [Time]'."
Day3_RegTech_Architecture_Gemi,"Explain how to validate 'Source to Target' row counts in a streaming pipeline.","Difficult in streams. Use 'Micro-batch' approach. 1. Source emits 'Watermark' or 'Batch End' signal with Count. 2. Sink counts records received per Watermark. 3. Compare. If mismatch, trigger 'Data Loss' alert."
Day3_RegTech_Architecture_Gemi,"Design a 'User Activity Audit' for the Data Warehouse.","1. Enable DB Audit Logging (e.g., Snowflake Access History). 2. Capture: `User`, `Query_Text`, `Tables_Accessed`, `Time`. 3. Store in `AUDIT_LOGS` table. 4. Monitor: Alert on 'Bulk Export' queries or access to 'Sensitive Tables' by unusual users."
"Day3_RegTech_Architecture_Perp","Design the data flow and tables to support a new cross border sanctions effectiveness report given existing Kafka topics and warehouse tables","Explain that sanctions screening events from Kafka topics should be landed into a raw store and parsed into a sanctions_screened_item_fact with one row per screened payment and a sanctions_hit_fact with one row per hit linked by payment_tx_id and corridor_id Payment_tx_fact already provides corridor and customer attributes The report aggregates hit counts true positives and total screened volumes by corridor and channel using these facts and shared dimensions and is scheduled daily after ingestion and DQ checks complete"
"Day3_RegTech_Architecture_Perp","Explain how you would extend the canonical model to support reporting on embedded finance partners without duplicating payment logic","Describe adding a dim_partner dimension that identifies embedded finance platforms and including a partner_id foreign key in payment_tx_fact populated during ingestion from channel metadata This allows all existing regulatory and MI reports to group or filter by partner while reusing the same payment attributes schemes and corridors without separate fact tables"
"Day3_RegTech_Architecture_Perp","Design an ingestion pattern for ISO 20022 pacs.008 messages that supports both streaming analytics and batch reporting","Propose capturing incoming pacs.008 messages from the SWIFT gateway into a Kafka topic validated against XSD then using a streaming job to persist raw XML into a landing zone and parse key fields into stg_pacs008 in a lakehouse partitioned by business_date A separate ELT process transforms staging into payment_tx_fact each night while real time dashboards use a lightweight streaming view over the staging tables"
"Day3_RegTech_Architecture_Perp","Describe how you would model and store intraday RTGS balance data for intraday liquidity monitoring","Explain that you would create a liquidity_position_fact table with one row per settlement account per observation timestamp fed by a streaming source from RTGS or internal systems The table stores available_balance utilised_intraday_credit and derived metrics and is partitioned by date and account enabling intraday profiles to be reconstructed and aggregated into regulatory liquidity reports"
"Day3_RegTech_Architecture_Perp","Explain how you would implement data quality checks for KYC reference data used in payments reporting","Describe loading KYC snapshots into a stg_customer table then running DQ rules that verify mandatory fields such as residency sector and risk rating are present for active customers plus uniqueness of identifiers and valid country codes Failures are logged to a dq_result_fact and serious breaches such as missing residency on high value customers block downstream fact loads until remediated"
"Day3_RegTech_Architecture_Perp","Design a control that ensures every AML alert in aml_alert_fact is linked to an existing customer and at least one payment","Explain that aml_alert_fact should have foreign keys to dim_customer and optionally to payment_tx_fact A nightly integrity job checks for orphan alerts by left joining alerts to these tables and counting rows with missing matches Any such alerts are logged as DQ exceptions for investigation and either fixed by correcting keys or tagged as non transaction based alerts with appropriate flags"
"Day3_RegTech_Architecture_Perp","Describe how you would architect the flow from card authorisation systems into the regulatory warehouse","Explain that authorisation events from card switches are streamed into Kafka topics then landed into stg_card_auth with one row per authorisation including cardholder merchant MCC and channel A subsequent transformation enriches these with customer and merchant dimensions and loads them into card_tx_fact aligned with payment_tx_fact so fraud AML and scheme reports can treat cards consistently with other instruments"
"Day3_RegTech_Architecture_Perp","Explain how you would design lineage documentation for a BoP cross border payments metric such as total outward credits by corridor","State that for this metric you would record that it derives from payment_tx_fact.amount_settlement_ccy filtered where cross_border_flag equals one and role is outward and grouped by corridor_id then map corridor_id to dim_corridor origin and destination countries The lineage traces back to stg_pacs008 amounts and UETR plus ISO country fields with documentation stored in a catalogue that links the regulatory template cell to each underlying field"
"Day3_RegTech_Architecture_Perp","Design an approach to handle late arriving payment corrections while maintaining consistent historical reporting","Explain that corrections are ingested as adjustment events referencing original transaction ids and stored in an adjustments table A periodic reconciliation job applies these adjustments by updating or inserting new versions of rows in payment_tx_fact using effective from dates Reports for past periods either use the latest corrected snapshot or maintain both original and restated figures with correction logs depending on regulatory guidance"
"Day3_RegTech_Architecture_Perp","Describe how you would model scheme specific payment statuses for use in both SLA and regulatory reports","Explain that you would create a status_fact or a status bridge table keyed by payment_tx_id and status_event_id with fields for status_code scheme_event_type and timestamp Status codes are normalised in a dim_status dimension per scheme This allows SLA reports to compute durations between specific status events while regulatory reports can classify outcomes such as accepted rejected or pending using the same source"
"Day3_RegTech_Architecture_Perp","Explain how you would implement a CDC based ingestion from a core banking database into dim_account","Describe enabling change data capture on the core account tables and streaming changes into Kafka or a log based tool then applying them to stg_account with operation types Inserts updates and deletes are processed into dim_account using SCD logic so that historical versions are retained when attributes like product or branch change ensuring regulatory reports can reconstruct exposures accurately over time"
"Day3_RegTech_Architecture_Perp","Design the canonical tables needed to report card fraud losses by customer segment and region","Explain that card_tx_fact should include fields for fraud_flag fraud_loss_amount customer_id and geography plus instrument and channel keys Dim_customer provides segment attributes such as retail or SME and dim_geography holds country and region For reporting you aggregate fraud_loss_amount and transaction counts by customer segment and region for the desired period"
"Day3_RegTech_Architecture_Perp","Describe how you would structure an Airflow DAG for daily payments regulatory pipelines from ingestion to report export","Explain that the DAG would start with parallel ingestion tasks for payments ISO KYC AML and GL followed by DQ and reconciliation tasks that gate downstream flows After successful checks transformation tasks load canonical facts and dimensions then mart build tasks prepare subject specific summary tables Finally export tasks generate regulatory files or views Each task uses the business_date parameter and dependencies ensure correct ordering and failure alerting"
"Day3_RegTech_Architecture_Perp","Explain how you would enable data scientists to build AML models using canonical data without compromising regulatory stability","Describe creating separate analytical schemas or workspaces containing read only views of canonical facts and dimensions masked where necessary Data scientists build models there and any features needed for production are promoted into a governed feature store Changes to canonical models still follow change control and report logic remains separate from experimentation"
"Day3_RegTech_Architecture_Perp","Design a table structure that allows a payment to be associated with multiple parties including UBOs and intermediaries","Explain that you would create a payment_party_bridge table keyed by payment_tx_id and party_role with a foreign key to dim_party which unifies customers institutions and UBOs Roles such as debtor creditor ultimate_debtor ultimate_creditor and owner allow flexible reporting of flows by any party type without changing the payment fact schema"
"Day3_RegTech_Architecture_Perp","Describe how you would capture and expose list version information for sanctions hits in the canonical model","Explain that sanctions_hit_fact would include list_type and list_version fields populated from the screening engine along with hit timestamps This allows reports to filter or group hits by sanctions list release and demonstrate that screening used up to date lists when assessing past flows"
"Day3_RegTech_Architecture_Perp","Explain how you would design an ETL job to populate dim_purpose from ISO 20022 fields","Describe parsing <Purp> and <CtgyPurp> codes from staging tables such as stg_pacs008 and stg_pain001 collecting distinct code values then joining them to an external reference mapping supplied by schemes or regulators The ETL inserts or updates rows in dim_purpose with code descriptions and regulatory categories so payment_tx_fact can link to purpose_id values consistently"
"Day3_RegTech_Architecture_Perp","Design a reconciliation report that compares daily payment volumes against Nostro account movements from camt.053","Explain that the report would aggregate payment_tx_fact by Nostro account id and day summing inflows and outflows and compare these with camt.053 entry sums for the same accounts and dates Differences beyond tolerances are listed with breakdowns by corridor and instrument for investigation and reconciliation results are stored for audit"
"Day3_RegTech_Architecture_Perp","Describe how you would model an AML risk rating overlay for customers in the canonical warehouse","Explain that dim_customer would store a current base_risk_rating plus historical versions while a separate risk_assessment_fact could track periodic and trigger based assessments with scores and dates Payment_tx_fact joins to dim_customer to use the risk rating valid at transaction time while AML reports use risk_assessment_fact to show trends and links between rating changes and alerts"
"Day3_RegTech_Architecture_Perp","Explain how to design a robust interface from the sanctions engine to the warehouse that minimises coupling","Describe exposing the sanctions engine output as a versioned JSON schema pushed to Kafka or a file store with fields such as hit_id payment_reference list_type match_score and decision The warehouse ingestion process parses this into staging and canonical sanctions facts and hides internal engine specifics so that engine changes only require schema mapping updates not changes in all reports"
"Day3_RegTech_Architecture_Perp","Design the process for onboarding a new real time payments engine into the regulatory data architecture","Explain that initial steps include analysing message formats and lifecycle events mapping them to canonical payment attributes and building ingestion into raw and staging layers For real time needs events are streamed into Kafka topics while nightly transforms load payment_tx_fact and derived mart tables DQ and reconciliations are extended to include the new engine and reports are validated in parallel runs before full cutover"
"Day3_RegTech_Architecture_Perp","Describe how you would provide lineage views to regulators for a specific regulatory return such as an ECB payment statistic","Explain that for each figure in the return you would map the template cell to a warehouse view and underlying facts and dimensions in the data catalogue The lineage diagrams show source systems and message types used transformations applied and DQ checks executed For demonstrations you can drill down to example transactions from the report to their originating ISO messages and core records"
"Day3_RegTech_Architecture_Perp","Explain how you would design a multi region architecture where payments data is processed locally but key regulatory reports are consolidated globally","Describe deploying regional lakehouse and warehouse instances that ingest and model local payments and reference data using a common canonical schema Periodically aggregated metrics and selected de identified facts are replicated to a central warehouse for group reporting while detailed PII remains regionally confined Federation or export jobs ensure global reports use consistent dimensions while respecting data residency"
"Day3_RegTech_Architecture_Perp","Design a DQ dashboard for payments regulatory data that operations teams can use daily","Explain that DQ results from dq_result_fact are surfaced in a dashboard showing rule breaches by table severity and business date with trend lines and drill down to offending records Filters allow teams to view issues for specific source systems corridors or entities and links to run logs help correlate DQ problems with ETL job incidents"
"Day3_RegTech_Architecture_Perp","Describe how you would architect storage for ISO 20022 XML so it remains queryable but does not slow down standard analytics","Explain that you would store raw XML in a dedicated column or table in the raw zone and optionally in a secondary column family of staging tables while canonical facts contain only parsed fields For occasional deep dives you expose views that join facts to raw payloads but most analytics use slimmer tables avoiding performance impact"
"Day3_RegTech_Architecture_Perp","Explain how you would design cut off aware scheduling for T plus one regulatory reports across multiple time zones","Describe defining business_day and cut_off_time attributes in dim_date per region and configuring the scheduler so ingestion jobs for each region complete before its cut off A global control job then verifies that all necessary regions have reached cut off for the target date before triggering consolidated reports using a common business_date parameter"
"Day3_RegTech_Architecture_Perp","Design an approach to integrate FX rate data into the canonical model for cross currency reporting","Explain that you would maintain an fx_rate_fact table with rates by currency pair and date time from a trusted source Payment_tx_fact includes transaction and settlement currencies and joins to fx_rate_fact to compute amounts in a reporting currency for BoP or risk reports while preserving original currencies"
"Day3_RegTech_Architecture_Perp","Describe how you would configure lineage and controls to satisfy an internal model validation team for AML related data","Explain that you would document all fields used in AML models and reports including their source systems transformations and DQ rules in the data catalogue Model validation can trace feature values for sample alerts back to original messages and see evidence of DQ and reconciliations passed before data entered the model"
"Day3_RegTech_Architecture_Perp","Explain how you would implement a reference data service for corridor and high risk country lists consumed by both ETL and real time services","Describe creating a centrally managed reference database or API that stores corridor definitions high risk country flags and effective dates ETL pipelines and streaming services call this service or cache snapshots to derive corridor_id and risk flags consistently ensuring that any change in definitions is reflected across batch and real time logic"
"Day3_RegTech_Architecture_Perp","Design a reporting view that combines payment volumes and AML alerts for a corridor risk dashboard","Explain that the view would join payment_tx_fact and aml_alert_fact on payment_tx_id grouping by corridor_id and time bucket It returns total payments volumes number of alerts number of true positives and averaged model scores per corridor enabling management to compare risk intensity with business activity"
"Day3_RegTech_Architecture_Perp","Describe how you would structure a hazard free rollback strategy for a major ETL change in the regulatory warehouse","Explain that before deploying the change you would snapshot affected tables or maintain old and new versions side by side During rollout both pipelines may run in parallel feeding comparison reports If issues are detected you can switch report views back to old tables without data loss while investigating and correcting the new ETL"
"Day3_RegTech_Architecture_Perp","Explain how you would support pKYC use cases within the same architecture used for payments reporting","Describe that streaming events on payments alerts ownership changes and external signals feed a risk_profile service backed by canonical facts and KYC data Dim_customer stores current and historical risk ratings while a risk_event_fact captures trigger events AML and KYC teams access dashboards built on these tables and the same canonical payments data used for regulatory reporting"
"Day3_RegTech_Architecture_Perp","Design a small set of base views that can support most payments related regulatory reports without bespoke SQL per return","Explain that base views might include payments_by_corridor_day payments_by_instrument_and_channel aml_alerts_by_typology sanctions_hits_by_corridor and liquidity_by_account_interval Each view aggregates canonical facts along common dimensions and regulators specific templates are built as thin layers of filters and pivots on top of these views rather than entirely new queries"
"Day3_RegTech_Architecture_Perp","Describe how you would instrument ETL jobs to provide metrics needed for operational MI on the data platform itself","Explain that each job logs start and end times rows read and written DQ failures and error counts into an etl_run_fact table With a small set of dimensions for job type system and subject area you can build MI on pipeline health such as average runtimes failure rates and volumes processed which supports capacity planning and governance"
"Day3_RegTech_Architecture_Perp","Explain how a well designed canonical model simplifies the addition of a new regulatory report on payment flows to a specific high risk region","Describe that because payment_tx_fact already carries corridor_id customer type and risk flags with conformed dimensions the new report can be implemented as a query filtering on corridors involving the high risk region and aggregating existing measures No new ETL or bespoke tables are required beyond possibly adding a region attribute to the corridor dimension"
"Day3_RegTech_Architecture_Perp","Summarise how the combination of streaming ingestion canonical modelling lineage and layered controls delivers a G SIB grade payments regulatory architecture","State that streaming ingestion provides timely capture of payments and risk events while canonical modelling ensures consistent representation across thousands of reports Field level lineage and version controlled logic allow regulators to trace figures back to source messages and layered DQ and reconciliations ensure numerical integrity Together these aspects enable scalable reliable and transparent regulatory reporting for a complex global payments business"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to compute daily CHAPS volumes by corridor and customer type from fact_payment and dim_customer ensuring only settled payments are included","Use a grouped aggregate joining the debtor customer dimension for type and filtering on scheme and status for example SELECT fp.payment_date fp.corridor dc.customer_type COUNT(*) AS tx_count SUM(fp.amount) AS total_amount FROM fact_payment fp JOIN dim_customer dc ON fp.debtor_cust_key=dc.customer_key WHERE fp.scheme_code='CHAPS' AND fp.status='SETTLED' GROUP BY fp.payment_date fp.corridor dc.customer_type ORDER BY fp.payment_date fp.corridor dc.customer_type"
"Day3_SQL_Scenarios_RegReports_Perp","Outline a SQL query that produces base data for a cross border flows above threshold report including debtor residency and corridor","Select from fact_payment joined to dim_customer for debtor attributes filter by cross_border_flag and amount threshold and project relevant columns for example SELECT fp.payment_date fp.corridor fp.currency_code fp.amount dc.residency_country AS debtor_residency FROM fact_payment fp JOIN dim_customer dc ON fp.debtor_cust_key=dc.customer_key WHERE fp.status='SETTLED' AND fp.cross_border_flag='Y' AND fp.amount>=:threshold AND fp.payment_date BETWEEN :start_dt AND :end_dt"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to compute STP rates per scheme per day using payment payment_failure and payment_repair tables","Build a base set marking each settled payment as STP when there is no matching failure or repair then aggregate per scheme date for example WITH base AS (SELECT p.payment_id p.scheme_code CAST(p.settled_ts AS DATE) AS settled_date CASE WHEN f.payment_id IS NULL AND r.payment_id IS NULL THEN 1 ELSE 0 END AS is_stp FROM payment p LEFT JOIN payment_failure f ON p.payment_id=f.payment_id LEFT JOIN payment_repair r ON p.payment_id=r.payment_id WHERE p.status='SETTLED') SELECT settled_date scheme_code COUNT(*) AS total_count SUM(is_stp) AS stp_count 100.0*SUM(is_stp)/COUNT(*) AS stp_rate_pct FROM base GROUP BY settled_date scheme_code"
"Day3_SQL_Scenarios_RegReports_Perp","Show how to find the top three corridors by total SEPA volume for each currency in a given month using window functions","First aggregate to month currency corridor then apply ROW_NUMBER over a partition by month and currency ordering by total amount for example WITH monthly AS (SELECT DATE_TRUNC('month',payment_date) AS month_start currency_code corridor SUM(amount) AS total_amount FROM fact_payment WHERE scheme_code='SEPA' AND status='SETTLED' AND payment_date BETWEEN :start_dt AND :end_dt GROUP BY DATE_TRUNC('month',payment_date) currency_code corridor) SELECT month_start currency_code corridor total_amount FROM (SELECT monthly.* ROW_NUMBER() OVER(PARTITION BY month_start currency_code ORDER BY total_amount DESC) AS rn FROM monthly) x WHERE rn<=3 ORDER BY month_start currency_code rn"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to generate a daily sanctions hit count by list_type and corridor for use in MI","Join the sanctions hit fact to payment facts on key fields and group by date list and corridor for example SELECT sh.decision_date fp.corridor sh.list_type COUNT(*) AS hit_count FROM fact_sanctions_hit sh JOIN fact_payment fp ON sh.payment_key=fp.payment_key WHERE sh.decision_date BETWEEN :start_dt AND :end_dt GROUP BY sh.decision_date fp.corridor sh.list_type ORDER BY sh.decision_date fp.corridor sh.list_type"
"Day3_SQL_Scenarios_RegReports_Perp","Explain and write SQL to compute daily FPS payment counts and amounts for retail customers only","You must restrict payments to scheme FPS status SETTLED and debtor customer_type RETAIL by joining dim_customer then grouping per day for example SELECT fp.payment_date COUNT(*) AS tx_count SUM(fp.amount) AS total_amount FROM fact_payment fp JOIN dim_customer dc ON fp.debtor_cust_key=dc.customer_key WHERE fp.scheme_code='FPS' AND fp.status='SETTLED' AND dc.customer_type='RETAIL' GROUP BY fp.payment_date ORDER BY fp.payment_date"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to identify the top five failure codes for SEPA transactions over the last 30 days","Filter payment or failure tables to SEPA in the last 30 days then group by failure_code and order by count descending with a limit for example SELECT pf.failure_code COUNT(*) AS failure_count FROM payment_failure pf JOIN payment p ON pf.payment_id=p.payment_id WHERE p.scheme_code='SEPA' AND pf.created_ts>=CURRENT_DATE-30 GROUP BY pf.failure_code ORDER BY failure_count DESC FETCH FIRST 5 ROWS ONLY"
"Day3_SQL_Scenarios_RegReports_Perp","Describe SQL to compute a running month to date cross border amount per corridor using window functions","First aggregate to daily corridor totals then apply a window SUM over days within each corridor for example SELECT payment_date corridor SUM(amount) AS day_amount SUM(SUM(amount)) OVER(PARTITION BY corridor ORDER BY payment_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS mtd_amount FROM fact_payment WHERE cross_border_flag='Y' AND status='SETTLED' AND payment_date BETWEEN :start_dt AND :end_dt GROUP BY payment_date corridor"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to produce a base dataset for an AML corridor report that counts alerts and payments per corridor per day without double counting payments","Build two aggregates one on payments one on alerts then join them on date and corridor for example WITH pay AS (SELECT payment_date corridor COUNT(*) AS payment_count FROM fact_payment WHERE status='SETTLED' GROUP BY payment_date corridor) alerts AS (SELECT a.alert_date fp.corridor COUNT(DISTINCT a.alert_id) AS alert_count COUNT(DISTINCT a.payment_key) AS payment_with_alerts FROM aml_alert_fact a JOIN fact_payment fp ON a.payment_key=fp.payment_key GROUP BY a.alert_date fp.corridor) SELECT p.payment_date p.corridor p.payment_count COALESCE(a.alert_count 0) AS alert_count COALESCE(a.payment_with_alerts 0) AS payment_with_alerts FROM pay p LEFT JOIN alerts a ON p.payment_date=a.alert_date AND p.corridor=a.corridor"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how you would write SQL to find corridors where sanctions hits exceed a given ratio of payments in the last month","Compute total payments and total confirmed hits per corridor using separate aggregates and then join them dividing hits by payments for example WITH pay AS (SELECT corridor COUNT(*) AS payment_count FROM fact_payment WHERE payment_date>=CURRENT_DATE-30 AND status='SETTLED' GROUP BY corridor) hits AS (SELECT fp.corridor COUNT(*) AS hit_count FROM fact_sanctions_hit sh JOIN fact_payment fp ON sh.payment_key=fp.payment_key WHERE sh.decision='CONFIRMED' AND sh.decision_date>=CURRENT_DATE-30 GROUP BY fp.corridor) SELECT p.corridor payment_count hit_count 1.0*hit_count/payment_count AS hit_ratio FROM pay p LEFT JOIN hits h ON p.corridor=h.corridor WHERE hit_count IS NOT NULL AND 1.0*hit_count/payment_count>:threshold"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to compute daily CHAPS volumes split by whether debtor and creditor are in the same country using CASE expressions","Use CASE to classify domestic versus cross border flows then group by date and classification for example SELECT payment_date CASE WHEN debtor_country=creditor_country THEN 'DOMESTIC' ELSE 'CROSS_BORDER' END AS flow_type COUNT(*) AS tx_count SUM(amount) AS total_amount FROM payment WHERE scheme_code='CHAPS' AND status='SETTLED' GROUP BY payment_date CASE WHEN debtor_country=creditor_country THEN 'DOMESTIC' ELSE 'CROSS_BORDER' END ORDER BY payment_date flow_type"
"Day3_SQL_Scenarios_RegReports_Perp","Describe SQL to list payments that have a sanctions hit but no corresponding entry in a sanctions review table","Use a LEFT JOIN from hits to reviews and filter for NULL review keys for example SELECT sh.hit_id sh.payment_id FROM sanctions_hit sh LEFT JOIN sanctions_review sr ON sh.hit_id=sr.hit_id WHERE sr.hit_id IS NULL filters hits lacking review records"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to compute average time from payment creation to settlement in hours for FPS payments","Subtract timestamps and average the interval for example SELECT AVG(EXTRACT(EPOCH FROM (settled_ts-created_ts)))/3600 AS avg_hours FROM payment WHERE scheme_code='FPS' AND status='SETTLED' AND settled_ts IS NOT NULL AND created_ts IS NOT NULL"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how to compute the 95th percentile settlement time for CHAPS payments using a window function","Order settled CHAPS payments by settlement duration and take the value at the appropriate percentile for example SELECT duration_hours FROM (SELECT EXTRACT(EPOCH FROM (settled_ts-created_ts))/3600 AS duration_hours PERCENTILE_CONT(0.95) WITHIN GROUP(ORDER BY EXTRACT(EPOCH FROM (settled_ts-created_ts))/3600) OVER() AS p95 FROM payment WHERE scheme_code='CHAPS' AND status='SETTLED') t WHERE duration_hours=p95"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to count number of payments and total amount per customer risk_rating using dim_customer and fact_payment","Join dim_customer to fact_payment on debtor_cust_key and group by risk_rating for example SELECT dc.risk_rating COUNT(*) AS tx_count SUM(fp.amount) AS total_amount FROM fact_payment fp JOIN dim_customer dc ON fp.debtor_cust_key=dc.customer_key WHERE fp.status='SETTLED' GROUP BY dc.risk_rating ORDER BY dc.risk_rating"
"Day3_SQL_Scenarios_RegReports_Perp","Describe SQL to fetch last three reg_report_run records for a given report_code ordered by period_end_date","Select from reg_report_run with filter on report_code order by period_end_date descending and limit rows for example SELECT * FROM reg_report_run WHERE report_code=:code ORDER BY period_end_date DESC FETCH FIRST 3 ROWS ONLY"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how to write SQL that returns payment_ids present in GL movements but missing from fact_payment for reconciliation","Use a LEFT JOIN from GL to payments and filter NULL payment keys for example SELECT g.payment_id g.gl_tx_id FROM gl_movement g LEFT JOIN fact_payment fp ON g.payment_id=fp.payment_id WHERE fp.payment_key IS NULL highlights GL entries without matching payments"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to produce a daily count of AML alerts by typology and corridor","Join aml_alert_fact to fact_payment for corridor and group by alert_date typology and corridor for example SELECT a.alert_date a.typology_code fp.corridor COUNT(*) AS alert_count FROM aml_alert_fact a JOIN fact_payment fp ON a.payment_key=fp.payment_key WHERE a.alert_date BETWEEN :start_dt AND :end_dt GROUP BY a.alert_date a.typology_code fp.corridor ORDER BY a.alert_date a.typology_code fp.corridor"
"Day3_SQL_Scenarios_RegReports_Perp","Describe how to ensure a JOIN between payments and customers does not double count when a customer has multiple dimension rows history","Use the correct SCD key by linking fact_payment to dim_customer via a surrogate customer_key that already represents the correct historical version instead of joining on natural customer_id which would produce multiple matches"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL that lists corridors with total FPS amount below a threshold using HAVING","Group by corridor and filter on SUM(amount) for example SELECT corridor SUM(amount) AS total_amount FROM fact_payment WHERE scheme_code='FPS' AND status='SETTLED' GROUP BY corridor HAVING SUM(amount)<:threshold ORDER BY total_amount"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how to compute the ratio of confirmed sanctions hits to all screened payments per scheme using two aggregates","Aggregate payments per scheme and hits per scheme separately then join and divide for example WITH pay AS (SELECT scheme_code COUNT(*) AS payment_count FROM fact_payment GROUP BY scheme_code) hits AS (SELECT fp.scheme_code COUNT(*) AS hit_count FROM fact_sanctions_hit sh JOIN fact_payment fp ON sh.payment_key=fp.payment_key WHERE sh.decision='CONFIRMED' GROUP BY fp.scheme_code) SELECT p.scheme_code payment_count COALESCE(h.hit_count 0) AS hit_count COALESCE(h.hit_count 0)/NULLIF(p.payment_count 0.0) AS hit_ratio FROM pay p LEFT JOIN hits h ON p.scheme_code=h.scheme_code"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to find customers whose total outbound cross border volume last month exceeds a specified limit","Sum cross border settled payments by debtor customer then filter with HAVING for example SELECT dc.customer_id dc.legal_name SUM(fp.amount) AS total_outbound FROM fact_payment fp JOIN dim_customer dc ON fp.debtor_cust_key=dc.customer_key WHERE fp.cross_border_flag='Y' AND fp.status='SETTLED' AND fp.payment_date BETWEEN :start_dt AND :end_dt GROUP BY dc.customer_id dc.legal_name HAVING SUM(fp.amount) > :limit"
"Day3_SQL_Scenarios_RegReports_Perp","Describe SQL to compute daily card transaction counts by MCC and SCA usage flag","Use card_tx_fact grouped by date MCC and sca_flag for example SELECT tx_date mcc_code sca_flag COUNT(*) AS tx_count FROM card_tx_fact WHERE tx_date BETWEEN :start_dt AND :end_dt GROUP BY tx_date mcc_code sca_flag ORDER BY tx_date mcc_code sca_flag"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how a window function ROW_NUMBER can help de duplicate overlapping payment feeds","You can partition by a business key such as external_reference and order by a quality indicator then keep only rows with ROW_NUMBER()=1 thus choosing a single canonical record for each business transaction"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to calculate daily average transaction size per scheme using GROUP BY","Group by scheme_code and date with AVG(amount) for example SELECT payment_date scheme_code AVG(amount) AS avg_amount FROM fact_payment WHERE status='SETTLED' GROUP BY payment_date scheme_code ORDER BY payment_date scheme_code"
"Day3_SQL_Scenarios_RegReports_Perp","Describe an example where using a cursor for regulatory SQL might be justified despite set based preference","A cursor may be justified for a one off data repair or migration where each affected payment requires conditional procedural updates or external API calls that are difficult to model in pure SQL while core reporting aggregates remain set based"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how to write SQL to identify payments not yet linked to any sanctions screening result","Use a LEFT JOIN from payments to sanctions_hit_fact on payment_key and filter for NULL hit keys for example SELECT fp.payment_id FROM fact_payment fp LEFT JOIN fact_sanctions_hit sh ON fp.payment_key=sh.payment_key WHERE sh.hit_key IS NULL AND fp.payment_date BETWEEN :start_dt AND :end_dt"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL that returns for each day and scheme the percentage of payments that generated at least one AML alert","First count total payments per day scheme then count distinct payment_keys with alerts and divide for example WITH pay AS (SELECT payment_date scheme_code COUNT(*) AS total_payments FROM fact_payment WHERE status='SETTLED' GROUP BY payment_date scheme_code) alert_pay AS (SELECT fp.payment_date fp.scheme_code COUNT(DISTINCT fp.payment_key) AS alerted_payments FROM aml_alert_fact a JOIN fact_payment fp ON a.payment_key=fp.payment_key GROUP BY fp.payment_date fp.scheme_code) SELECT p.payment_date p.scheme_code p.total_payments COALESCE(a.alerted_payments 0) AS alerted_payments COALESCE(a.alerted_payments 0)/NULLIF(p.total_payments 0.0) AS alert_ratio FROM pay p LEFT JOIN alert_pay a ON p.payment_date=a.payment_date AND p.scheme_code=a.scheme_code"
"Day3_SQL_Scenarios_RegReports_Perp","Describe how you would modify a corridor report SQL to show both per corridor totals and a grand total row using GROUP BY ROLLUP","Use GROUP BY ROLLUP(corridor) and interpret NULL corridor in the result as the grand total row for example SELECT corridor SUM(amount) AS total_amount FROM fact_payment WHERE status='SETTLED' GROUP BY ROLLUP(corridor) ORDER BY corridor NULLS LAST"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL to generate a base dataset for a monthly ECB payment instrument report from fact_payment and dim_instrument","Join fact_payment to dim_instrument group by month and instrument category and aggregate counts and values for example SELECT DATE_TRUNC('month',payment_date) AS month_start di.instrument_category COUNT(*) AS tx_count SUM(fp.amount) AS total_amount FROM fact_payment fp JOIN dim_instrument di ON fp.instrument_id=di.instrument_id WHERE fp.payment_date BETWEEN :start_dt AND :end_dt AND fp.status='SETTLED' GROUP BY DATE_TRUNC('month',payment_date) di.instrument_category ORDER BY month_start di.instrument_category"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how you would write SQL to compute a per customer rolling 30 day outbound volume using window functions","Pre aggregate payments per customer and date then apply a window SUM with a frame of 30 days for example SELECT payment_date debtor_cust_key SUM(amount) AS day_amount SUM(SUM(amount)) OVER(PARTITION BY debtor_cust_key ORDER BY payment_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS rolling_30d_amount FROM fact_payment WHERE status='SETTLED' GROUP BY payment_date debtor_cust_key"
"Day3_SQL_Scenarios_RegReports_Perp","Write SQL that lists for each report_code the last successful run_ts from reg_report_run","Group by report_code and take MAX(run_ts) where run_status='SUCCESS' for example SELECT report_code MAX(run_ts) AS last_success_ts FROM reg_report_run WHERE run_status='SUCCESS' GROUP BY report_code ORDER BY report_code"
"Day3_SQL_Scenarios_RegReports_Perp","Describe how you would write SQL to check that sum of payment amounts by GL mapped product matches GL movement sums for a day","Aggregate fact_payment by gl_product_code and date and aggregate gl_movement by same keys then join the two aggregates and compare sums reporting differences greater than a tolerance for example using HAVING ABS(p.total_amount-g.total_amount)>:tolerance to highlight mismatches"
"Day3_SQL_Scenarios_RegReports_Perp","Explain how to parameterise a corridor based AML report SQL by business date for use in a scheduler","Add a bind parameter such as :biz_date and use it in WHERE clauses for payment_date or alert_date Then reference the parameter in the scheduler configuration so the same SQL is reused with different dates without hard coded literals improving maintainability and rerun capability"
"Day3_SQL_Scenarios_RegReports_Perp","Summarise how you would answer an interviewer asking why set based SQL is preferred for payments regulatory metrics","State that set based SQL expresses aggregations over entire data sets which lets the database engine use indexes parallelism and optimised execution plans providing predictable performance and simpler code while cursor based row by row logic is slower harder to maintain and more error prone especially on G SIB scale volumes"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to calculate the Daily 'Failure Rate' (Failed / Total) per Scheme from `FACT_PAYMENT`, handling division by zero.","`SELECT Scheme, Date, SUM(CASE WHEN Status='Failed' THEN 1 ELSE 0 END) * 100.0 / NULLIF(COUNT(*), 0) as Fail_Rate FROM FACT_PAYMENT GROUP BY Scheme, Date`"
Day3_SQL_Scenarios_RegReports_Gemi,"Design a table structure for `FACT_SANCTIONS_HIT` to link Payments to Watchlist Hits.","`CREATE TABLE FACT_SANCTIONS_HIT (HitID INT PK, PaymentID INT FK, ListID INT FK, HitScore DECIMAL(5,2), IsFalsePositive BOOLEAN, ReviewerID INT, DecisionDate DATETIME)`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find 'Orphaned' Payments (Payments in `TXN_DETAILS` that do not have a header record in `TXN_HEADER`).","`SELECT d.PaymentID FROM TXN_DETAILS d LEFT JOIN TXN_HEADER h ON d.PaymentID = h.PaymentID WHERE h.PaymentID IS NULL`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to identify the 'Top 3 High-Risk Corridors' by volume for last month.","`SELECT TOP 3 OriginCountry, DestCountry, SUM(Amount) as Vol FROM FACT_PAYMENT WHERE Date BETWEEN '2023-01-01' AND '2023-01-31' GROUP BY OriginCountry, DestCountry ORDER BY Vol DESC`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query using a Window Function to calculate the '7-Day Moving Average' of payment volume.","`SELECT Date, Volume, AVG(Volume) OVER (ORDER BY Date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as Moving_Avg FROM Daily_Stats`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find duplicate UETRs in the `STAGING_PAYMENTS` table and return the count of duplicates.","`SELECT UETR, COUNT(*) FROM STAGING_PAYMENTS GROUP BY UETR HAVING COUNT(*) > 1`"
Day3_SQL_Scenarios_RegReports_Gemi,"Design the SQL logic to classify payments as 'High', 'Medium', or 'Low' value based on thresholds (e.g., >1M, >10k).","`SELECT PaymentID, CASE WHEN Amount > 1000000 THEN 'High' WHEN Amount > 10000 THEN 'Medium' ELSE 'Low' END as Value_Category FROM Payments`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to delete all payments from `STAGING` that are older than 7 days.","`DELETE FROM STAGING_PAYMENTS WHERE IngestDate < DATEADD(day, -7, GETDATE())`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query using `EXISTS` to find all Customers who have *never* had a Sanctions Hit.","`SELECT * FROM Customers c WHERE NOT EXISTS (SELECT 1 FROM Sanctions_Hits h WHERE h.CustomerID = c.CustomerID)`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to calculate the total volume of 'Cross-Border' payments (where Sender Country != Receiver Country).","`SELECT SUM(Amount) FROM Payments WHERE SenderCountry <> ReceiverCountry`"
Day3_SQL_Scenarios_RegReports_Gemi,"How would you optimize a query filtering by `TransactionDate` on a table with 100 Million rows?","Ensure there is a Clustered Index or Non-Clustered Index on the `TransactionDate` column to allow an Index Seek instead of a Table Scan."
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to concatenate 'Street', 'City', and 'Zip' into a single address string, handling NULLs gracefully.","`SELECT CONCAT(COALESCE(Street, ''), ', ', COALESCE(City, ''), ', ', COALESCE(Zip, '')) FROM Addresses`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find the 'First' and 'Last' payment date for each customer.","`SELECT CustomerID, MIN(Date) as First_Payment, MAX(Date) as Last_Payment FROM Payments GROUP BY CustomerID`"
Day3_SQL_Scenarios_RegReports_Gemi,"Design a query to list all payments that were 'Rejected' and the reason, joining `Payments` and `StatusCodes` tables.","`SELECT p.ID, p.Amount, s.ReasonDescription FROM Payments p JOIN StatusCodes s ON p.ReasonCode = s.Code WHERE p.Status = 'Rejected'`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to calculate the percentage of payments that are 'CHAPS' vs 'FPS'.","`SELECT Scheme, COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as Percentage FROM Payments GROUP BY Scheme`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to update the `RiskScore` of a customer to 100 if they reside in 'High Risk Country List'.","`UPDATE Customers SET RiskScore = 100 WHERE Country IN (SELECT CountryCode FROM HighRiskCountries)`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find customers with duplicate email addresses.","`SELECT Email, COUNT(*) FROM Customers GROUP BY Email HAVING COUNT(*) > 1`"
Day3_SQL_Scenarios_RegReports_Gemi,"Design a `MERGE` statement to Upsert (Insert/Update) payments into a Target table from Staging.","`MERGE Target AS T USING Staging AS S ON T.ID = S.ID WHEN MATCHED THEN UPDATE SET T.Amt = S.Amt WHEN NOT MATCHED THEN INSERT (ID, Amt) VALUES (S.ID, S.Amt);`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find the 'Median' payment amount (using `PERCENTILE_CONT`).","`SELECT DISTINCT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY Amount) OVER () FROM Payments`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to extract the Year and Month from a Date column for grouping.","`SELECT EXTRACT(YEAR FROM Date) as Yr, EXTRACT(MONTH FROM Date) as Mth, COUNT(*) FROM Payments GROUP BY 1, 2` (Syntax varies by DB)"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query using `CROSS APPLY` (or `LATERAL`) to parse a JSON column `Details` for specific attributes.","`SELECT p.ID, j.Attribute FROM Payments p CROSS APPLY OPENJSON(p.Details) WITH (Attribute varchar(50) '$.attr') j`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find 'Gaps' in sequential Invoice Numbers.","`SELECT T1.ID + 1 FROM Invoices T1 WHERE NOT EXISTS (SELECT 1 FROM Invoices T2 WHERE T2.ID = T1.ID + 1)`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to calculate 'Year-Over-Year' growth in volume.","`SELECT Year, Vol, (Vol - LAG(Vol) OVER (ORDER BY Year)) / LAG(Vol) OVER (ORDER BY Year) as Growth FROM Yearly_Stats`"
Day3_SQL_Scenarios_RegReports_Gemi,"Design a `VIEW` that exposes only 'Safe' columns (hiding PII like Names) to analysts.","`CREATE VIEW Safe_Payments AS SELECT PaymentID, Amount, Currency, Date, Country FROM Payments` (Omitting Name/Address)"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find 'Circular Payments' (A pays B, B pays A) on the same day.","`SELECT P1.Sender, P1.Receiver FROM Payments P1 JOIN Payments P2 ON P1.Sender = P2.Receiver AND P1.Receiver = P2.Sender AND P1.Date = P2.Date`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to strip non-numeric characters from a phone number field.","`SELECT TRANSLATE(Phone, '0123456789' || Phone, '0123456789')` (Oracle) or using Regex Replace functions."
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to get the 'Latest Status' for each Payment from a History table.","`SELECT PaymentID, Status FROM (SELECT *, ROW_NUMBER() OVER (PARTITION BY PaymentID ORDER BY Timestamp DESC) as rn FROM StatusHistory) WHERE rn = 1`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to check if a Table Partition exists before dropping it.","`IF EXISTS (SELECT * FROM sys.partitions...) DROP...` (SQL Server specific logic)"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to generate a series of dates (Calendar) for a report, filling missing days with 0 volume.","`SELECT c.Date, COALESCE(SUM(p.Amt), 0) FROM Calendar c LEFT JOIN Payments p ON c.Date = p.Date GROUP BY c.Date`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to pivot 'Payment Types' (Credit, Debit) into columns.","`SELECT Customer, SUM(CASE WHEN Type='Credit' THEN Amt END) as Credits, SUM(CASE WHEN Type='Debit' THEN Amt END) as Debits FROM Payments GROUP BY Customer`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find the length of the longest name in the Customer table.","`SELECT MAX(LEN(Name)) FROM Customers`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to fetch the first 3 characters of a BIC code (Bank Code).","`SELECT SUBSTRING(BIC, 1, 3) FROM Banks`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to calculate the number of days between 'Initiation' and 'Settlement'.","`SELECT DATEDIFF(day, InitiationDate, SettlementDate) FROM Payments`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to randomize the order of rows (for sampling).","`SELECT * FROM Payments ORDER BY NEWID()` (SQL Server) or `RAND()`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to count how many distinct currencies each customer has used.","`SELECT CustomerID, COUNT(DISTINCT Currency) FROM Payments GROUP BY CustomerID`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to list duplicate rows based on all columns.","`SELECT * FROM Payments GROUP BY All_Columns HAVING COUNT(*) > 1`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to replace a substring (e.g., change 'http' to 'https') in a URL field.","`UPDATE Config SET URL = REPLACE(URL, 'http:', 'https:')`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to find values that match a pattern (e.g., starts with 'A' and ends with 'Z').","`SELECT * FROM Items WHERE Code LIKE 'A%Z'`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a query to get the current database user and time.","`SELECT SYSTEM_USER, GETDATE()`"
Day3_SQL_Scenarios_RegReports_Gemi,"Write a transaction block to transfer funds (Debit A, Credit B) ensuring atomicity.","`BEGIN TRANSACTION; UPDATE Accts SET Bal = Bal - 10 WHERE ID=A; UPDATE Accts SET Bal = Bal + 10 WHERE ID=B; COMMIT;`"
"DayX_ISO20022_MX_Schema_Perp","Describe how to derive a cross border flag and corridor from pacs.008 fields and how this differs from deriving it from pain.001","Use debtor and creditor locations from pacs.008 first taking Dbtr/PstlAdr/Ctry and Cdtr/PstlAdr/Ctry or if absent deriving countries from DbtrAgt and CdtrAgt BICFI country codes Then set corridor as debtor_country hyphen creditor_country and cross_border_flag as Y when the two differ Pain.001 derivation is similar but uses Dbtr and Cdtr parties and DbtrAgt and CdtrAgt without the benefit of interbank routing information so pacs.008 is generally preferred as the authoritative view for corridor reporting"
"DayX_ISO20022_MX_Schema_Perp","Outline how you would design stg_pain001_tx and stg_pacs008_tx to preserve all information needed to rebuild debtor and creditor party hierarchies","Design both staging tables with explicit columns for each party role including name country address lines organisation and private identifiers account ids and agent BICs For example include dbtr_name dbtr_country dbtr_addr_lines dbtr_org_id dbtr_prvt_id dbtr_acct_id dbtr_agt_bic and similarly for creditor and ultimate parties Supplement these with JSON or child tables for all identification schemes and structured postal addresses ensuring that the combination of debtor ultimate debtor creditor ultimate creditor and their agents can be reconstructed for downstream AML and regulatory analytics"
"DayX_ISO20022_MX_Schema_Perp","Explain how to map instructed and settlement amounts from pain.001 and pacs.008 into a canonical payment_tx fact","Use pain.001 CdtTrfTxInf/Amt/InstdAmt and currency as a source for amount_tx_ccy and ccy_tx and pacs.008 CdtTrfTxInf/InstdAmt as a higher priority if present For settlement values use pacs.008 CdtTrfTxInf/IntrBkSttlmAmt and its currency attribute to populate amount_sttlm_ccy and ccy_sttlm If only domestic ACH exists without pacs.008 you may set settlement equal to transaction amount but in all cross border cases pacs.008 should drive settlement metrics"
"DayX_ISO20022_MX_Schema_Perp","Design staging and canonical mappings so that ultimate debtor and ultimate creditor are available even when only some messages carry them","Include explicit ultmt_dbtr_name and ultmt_dbtr_country columns in stg_pain001_tx and stg_pacs008_tx and corresponding ultimate creditor fields During canonical ETL set payment_tx.ultmt_debtor_name and country from pacs.008 where present then fall back to pain.001 while leaving them null when neither message contains this information For pacs.009 COV also map underlying ultimate parties using separate und_ prefixed staging columns applying the same precedence routine and document that where ultimate data is missing you revert to account debtor and creditor for reporting"
"DayX_ISO20022_MX_Schema_Perp","Describe how to extract all information needed for an ECB style cross border flow report from pacs.008","For each pacs.008 transaction extract debtor and creditor residence or country from Dbtr and Cdtr postal addresses or their agent BICs to derive corridor and cross border flags Use IntrBkSttlmAmt and currency as the settlement amount and CdtTrfTxInf/Purp and CtgyPurp codes for economic purpose Join debtor and creditor parties to a customer or counterparty master to obtain sector and residency attributes Optionally read RgltryRptg codes where national guidelines require further BoP details then aggregate by corridor sector purpose and instrument according to ECB templates"
"DayX_ISO20022_MX_Schema_Perp","Explain how to map ISO agent and party data into separate institution and customer dimensions for reporting","During ETL create dim_institution for financial institutions keyed by BIC or LEI populated from DbtrAgt CdtrAgt and intermediary agent structures and any KYC FI master data Create dim_customer for non FI debtors creditors and their ultimate parties keyed by internal customer ids mapped from Dbtr Cdtr UltmtDbtr and UltmtCdtr For each payment fact resolve agent fields to dim_institution keys and party fields to dim_customer keys ensuring that corridors and correspondent analyses use institution data while customer residency and sector reports use customer data"
"DayX_ISO20022_MX_Schema_Perp","Design a relational structure to store multiple RgltryRptg entries per transaction while keeping payment_tx slim","Create a child table such as payment_reg_rptg with columns payment_tx_id sequence_no regulatory_code amount currency and narrative text populated from the RgltryRptg blocks of pain.001 pacs.008 or pacs.009 This table holds one row per regulatory reporting detail while the payment_tx fact stores only a primary regulatory_code or a flag if needed Reports that require detailed codes join to payment_reg_rptg while others remain on the slim payment_tx table"
"DayX_ISO20022_MX_Schema_Perp","Describe how to implement field level lineage for settlement amount from ISO XML to payment_tx.amount_sttlm_ccy","Document that settlement amount originates from pacs.008 at XML path FIToFICstmrCdtTrf CdtTrfTxInf IntrBkSttlmAmt value and its Ccy attribute Parsed values land in stg_pacs008_tx.intrbk_sttlm_amt and intrbk_sttlm_ccy During canonical ETL these staging columns are transformed into payment_tx.amount_sttlm_ccy and payment_tx.ccy_sttlm with clear mapping rules recorded in metadata so that auditors can follow the value from the final report back to the original XML node"
"DayX_ISO20022_MX_Schema_Perp","Explain how you would handle an ISO upgrade where a previously optional field in pain.001 becomes mandatory for a domestic scheme","First update schema validation and DQ rules so that messages for the affected scheme must contain the field failing ingestion if absent for new data but not for historical messages Extend staging to capture the field if not already present and adjust canonical ETL to populate corresponding attributes Default existing report behaviour by leaving canonical columns unchanged and document that nulls in historical data reflect the period before the mandatory requirement Without changing report SQL you rely on the new data populating existing columns from the upgrade date onward"
"DayX_ISO20022_MX_Schema_Perp","Design a naming convention for staging columns that reflects ISO paths while remaining SQL friendly","Adopt a pattern combining role and element names in lower snake case for example dbtr_name dbtr_country dbtr_acct_id dbtr_agt_bic cdtr_name ultmt_dbtr_name intrbk_sttlm_amt and purp_cd Use consistent prefixes for underlying legs in pacs.009 such as und_dbtr_name and und_instd_amt and avoid XML special characters by replacing them with underscores A short mapping document then maps each column name back to its precise ISO XPath"
"DayX_ISO20022_MX_Schema_Perp","Describe how to ensure that one canonical payment_tx row can be traced back to the exact ISO messages that produced it","Include foreign keys from payment_tx to stg_pain001_tx stg_pacs008_tx and stg_pacs009_tx storing their primary keys together with the uetr and end_to_end_id values Each staging transaction record carries iso_message_raw_id linking to the raw XML With this design you can trace from the payment_tx row to all contributing ISO transactions and from there to the raw message for full content and schema version"
"DayX_ISO20022_MX_Schema_Perp","Explain how to map pain.001 and pacs.008 to determine debtor_customer_type in payment_tx","Use Dbtr party identification structures from pain.001 and pacs.008 where organisation identifiers such as OrgId or LEI indicate a corporate and private identifiers indicate a retail Bring these through into staging as dbtr_org_id and dbtr_prvt_id and join to a KYC master to classify the customer into RETAIL CORP or FI Canonical ETL then populates payment_tx.debtor_customer_type based on the resolved KYC segment rather than only on ISO flags"
"DayX_ISO20022_MX_Schema_Perp","Design the rules for choosing debtor and creditor country for payment_tx when both address countries and agent BIC countries are present","Define debtor_country as the party postal address country when populated and valid otherwise derive it from DbtrAgt BICFI country Similarly creditor_country uses Cdtr postal address first then CdtrAgt BICFI This precedence prioritises customer residency data but ensures corridor derivation is still possible when addresses are missing Document the logic and retain original BIC based countries for audit"
"DayX_ISO20022_MX_Schema_Perp","Describe how to set the sanctions_screening_required_flag in payment_tx using ISO fields and risk references","Use debtor and creditor countries and agents from ISO to derive the corridor and compare with high risk country and corridor lists maintained in reference tables If either party or agent involves a sanctioned or high risk country or if transaction amount and scheme meet a screening policy threshold set sanctions_screening_required_flag to Y otherwise N This flag is driven by business rules applied to canonical ISO sourced attributes rather than a direct ISO field"
"DayX_ISO20022_MX_Schema_Perp","Explain how to flatten multiple CdtTrfTxInf elements from pain.001 into relational tables without losing relationships","Create a stg_pain001_msg table with a primary key pain001_msg_id then a stg_pain001_tx table with a foreign key pain001_msg_id and one row per CdtTrfTxInf element Within each transaction include PmtInfId and InstrId or a transaction sequence number to preserve the original ordering and batch grouping Ensuring that the combination of pain001_msg_id and InstrId or sequence number is unique maintains the one to many relationship"
"DayX_ISO20022_MX_Schema_Perp","Design the lineage for corridor from ISO message through staging to canonical attribute including fallback logic","Document that corridor uses debtor_country and creditor_country derived from Dbtr and Cdtr postal addresses in pain.001 or pacs.008 where available falling back to debtor and creditor agent BIC country codes if addresses are missing These ISO values are stored in stg_pain001_tx.dbtr_country and cdtr_country and stg_pacs008_tx.dbtr_country and cdtr_country ETL then computes payment_tx.debtor_country and creditor_country using precedence across pacs and pain followed by corridor as debtor hyphen creditor and cross_border_flag based on equality of these countries"
"DayX_ISO20022_MX_Schema_Perp","Describe how to model pacs.009 CORE and COV so that customer details are only present when truly available","Store common interbank fields such as IntrBkSttlmAmt agents and TxId in stg_pacs009_tx for all messages and add separate columns or a child table prefixed und_ to hold underlying customer information present only in COV variants Set cov_indicator on the header and populate underlying columns only when UndrlygCstmrCdtTrf is present Canonical ETL then enriches payment_tx with customer level data only for COV while keeping CORE as FI only flows"
"DayX_ISO20022_MX_Schema_Perp","Explain how new CBPR+ usage guidelines might change DQ rules without changing the canonical schema","CBPR+ can make certain fields effectively mandatory for cross border messages such as UETR ultimate parties or structured addresses while the canonical schema already has columns for them Adjust DQ rules and ingestion validation to treat these as mandatory when the business_msg_type and scheme indicate CBPR+ but keep the canonical schema unchanged and allow nulls for legacy or non CBPR+ flows preserving backwards compatibility"
"DayX_ISO20022_MX_Schema_Perp","Design how to handle an additional regulatory code introduced in RgltryRptg without breaking existing reports","Extend RgltryRptg parsing in staging to capture the new code into rgltryrptg_json or a child table and optionally add a new canonical attribute or mapping in dim_purpose or a dedicated regulatory dimension Existing reports continue using the same canonical fields while new reports can join to the enriched structures Because you have not removed or changed existing columns all current logic remains valid"
"DayX_ISO20022_MX_Schema_Perp","Explain how to ensure that changing the mapping of purpose codes from ISO to internal categories is auditable for regulators","Implement a dim_purpose table that stores every ISO Purp and CtgyPurp code with mapping to internal and regulatory categories and version its records with validity dates When mapping changes insert new rows instead of overwriting and reference dim_purpose from payment_tx using surrogate keys so historical transactions continue to reference the old classification while new ones use the updated mapping"
"DayX_ISO20022_MX_Schema_Perp","Describe how to create a canonical payment_tx record from pain.001 only when no pacs.008 is available","Use stg_pain001_tx as the primary source taking EndToEndId debtor creditor and ultimate parties amounts and purpose codes and populate payment_tx with these while leaving settlement amount and UETR null Mark source_pref as PAIN001_ONLY and derive corridor from debtor and creditor countries using Debtor and Creditor parties or agents This ensures domestic or ACH payments can still be reported robustly even without an interbank pacs leg"
"DayX_ISO20022_MX_Schema_Perp","Write a mapping narrative for how UETR is populated in payment_tx when both pacs.008 and pacs.009 exist for the same payment","State that when pacs.008 is present payment_tx.uetr is populated from FIToFICstmrCdtTrf GrpHdr UETR or from the transaction level element if the implementation uses it If only pacs.009 exists the uetr is taken from FICdtTrf GrpHdr UETR If both exist and differ a DQ rule flags the inconsistency and a precedence rule such as preferring pacs.008 is applied with the original values retained in staging for audit"
"DayX_ISO20022_MX_Schema_Perp","Explain how to add a new ISO field into canonical JSON extensions for experimentation without affecting regulated metrics","Add a json_extensions column to payment_tx where less critical fields from ISO messages are stored as key value pairs For a new ISO field parse it from staging into this JSON payload and make it available to analytics and model teams while keeping all regulated metrics and reports based solely on stable first class columns This allows experimentation and gradual adoption without changing regulatory report definitions"
"DayX_ISO20022_MX_Schema_Perp","Describe how to model charges information from pacs.008 so fees can be analysed without polluting core payment_tx","Create a payment_charges_fact table keyed by payment_tx_id containing one row per ChrgsInf element from pacs.008 with fields for charges amount currency agent and type Do not add charge breakdowns directly into payment_tx beyond an optional total charge amount For reporting on fees join payment_tx to payment_charges_fact while leaving core payment volume and value metrics unaffected"
"DayX_ISO20022_MX_Schema_Perp","Explain how to design DQ checks that validate message level control sums for pain.001 and pacs.008","Implement checks that for each pain.001 or pacs.008 message compare GrpHdr NbOfTxs to the count of CdtTrfTxInf rows parsed and GrpHdr CtrlSum or TtlIntrBkSttlmAmt to the sum of individual InstdAmt or IntrBkSttlmAmt amounts Differences beyond tolerance are logged as DQ exceptions and the affected messages or batches are flagged for investigation before canonical ETL proceeds"
"DayX_ISO20022_MX_Schema_Perp","Describe how to store ISO schema version and business message type so you can support multiple versions over time","Include columns such as schema_version and business_msg_type on iso_message_raw and propagate them to message level staging tables like stg_pain001_msg and stg_pacs008_msg Use schema_version to resolve which XSD and market practice apply when parsing and adjust ETL branches accordingly Canonical fields remain version agnostic while lineage documentation records which schema version was used for each transaction"
"DayX_ISO20022_MX_Schema_Perp","Explain how you would model multi leg cover payments across pacs.008 and pacs.009 in the canonical layer","Create payment_tx as the logical customer payment anchored by EndToEndId and UETR and represent each FI leg pacs.008 and pacs.009 as separate records in a payment_leg_fact referencing payment_tx Each leg records its own IntrBkSttlmAmt agents and settlement dates Reports using customer flows aggregate at payment_tx level while liquidity and correspondent reports operate at leg level preserving the full cover payment structure"
"DayX_ISO20022_MX_Schema_Perp","Describe a migration strategy to add new pacs.008 fields related to AML risk into your staging and canonical models without breaking existing ETL","Extend stg_pacs008_tx with new nullable columns for the AML related fields and adjust parsing code to populate them when present keeping default nulls for older messages Then add corresponding nullable columns or JSON entries in payment_tx and populate them in a new ETL step Existing ETL and reports can continue running unchanged while new AML use cases start consuming the extra attributes"
"DayX_ISO20022_MX_Schema_Perp","Explain how to document end to end lineage for economic purpose from ISO Purp to a central bank reporting column","Record in a lineage tool that the central bank report column EconomicPurpose is sourced from payment_tx.purpose_code which in turn comes from stg_pacs008_tx.purp_cd or stg_pain001_tx.purp_cd depending on precedence each of which is parsed from XML path CdtTrfTxInf Purp Cd in the respective messages Also document any mapping from raw Purp code to regulatory categories in dim_purpose including version histories"
"DayX_ISO20022_MX_Schema_Perp","Describe how to use ISO 20022 party identification structures to support large exposure reporting in the canonical model","Parse organisation identifiers and LEIs from Dbtr Cdtr and ultimate party identification structures into staging then link them to dim_customer and dim_institution where group structures and exposure hierarchies are maintained In payment_tx store references to these dimension keys so that large exposure reports can group flows by group id or LEI consistent with other exposure templates even though the original information came through ISO messages"
"DayX_ISO20022_MX_Schema_Perp","Summarise how your canonical model reconciles differences between pain.001 and pacs.008 for the same concept like debtor name","The canonical model defines a single debtor_name attribute and fills it using a precedence rule typically taking pacs.008 Dbtr/Nm as the executed view and falling back to pain.001 Dbtr/Nm when pacs.008 is absent Staging tables retain both values so discrepancies can be analysed and reconciliation reports can show when the executing bank or correspondent amended or enriched the original customer supplied data"
DayX_ISO20022_MX_Schema_Gemi,"Describe how to derive a 'Cross-Border Flag' and 'Corridor' from `pacs.008` fields, and how that differs from `pain.001` derivation.","In `pacs.008`: Compare `<InstgAgt>` Country (Sender Bank) vs `<InstdAgt>` Country (Receiver Bank) for Bank Corridor. For Customer Corridor, compare `<Dbtr/PstlAdr/Ctry>` vs `<Cdtr/PstlAdr/Ctry>`. In `pain.001`: You only have the Customer perspective (`<Dbtr>` vs `<Cdtr>`); you rely on the IBAN country code in `<DbtrAcct>` and `<CdtrAcct>` if addresses are missing."
DayX_ISO20022_MX_Schema_Gemi,"Outline how you would design `stg_pain001` and `stg_pacs008` tables to preserve all information needed to rebuild party hierarchies.","Design: Tables include columns for `dbtr_nm`, `dbtr_id`, `dbtr_acct`, `dbtr_agt_bic` AND `ultmt_dbtr_nm`, `ultmt_dbtr_id`. Crucially, include `initg_pty_nm` (pain.001 only). Use a JSON/XML column `raw_party_block` to store the full nested structure (Addresses, IDs) for re-parsing if schema evolves."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to extract all information needed for an ECB cross-border flow report from `pacs.008`, including purpose, counterpart sector, and amount.","Extract `<IntrBkSttlmAmt>` (Amount & Currency). Extract `<Purp><Cd>` (Purpose). For Counterpart Sector: Map `<Cdtr><Id><OrgId>` to 'Non-Financial Corp' or 'Financial' based on LEI/BIC lookup; map `<PrvtId>` to 'Household'. Use `<Cdtr/PstlAdr/Ctry>` for Counterpart Country."
DayX_ISO20022_MX_Schema_Gemi,"Describe the mapping logic to populate a canonical `payment_tx.sanctions_screening_text` field from a `pacs.008`.","Concatenate: `<Dbtr/Nm>`, `<Dbtr/PstlAdr>` (all lines), `<UltmtDbtr/Nm>`, `<UltmtDbtr/PstlAdr>`, `<Cdtr/Nm>`, `<Cdtr/PstlAdr>`, `<UltmtCdtr/Nm>`, `<UltmtCdtr/PstlAdr>`, and `<RmtInf/Ustrd>` (or structured ref). This ensures all relevant text is screened."
DayX_ISO20022_MX_Schema_Gemi,"How would you handle the schema evolution of `pacs.008` where an optional field like `<UETR>` became mandatory in a later version?","In the Staging table, the `uetr` column remains nullable (to support historic data). In the Canonical transformation (ELT), apply logic: `COALESCE(xml_uetr, 'UNKNOWN')` or flag 'Data Quality Error' for new records missing it. Do not alter the historic data."
DayX_ISO20022_MX_Schema_Gemi,"Design the SQL table structure for flattening the `<RmtInf>` (Remittance Information) block which allows multiple occurrences.","Table `stg_pacs008_remittance`. Columns: `remittance_id` (PK), `msg_uetr` (FK to parent), `seq_num` (1, 2, 3...), `unstructured_text`, `structured_ref`, `ref_type`. This allows 1-to-Many join with the main transaction table."
DayX_ISO20022_MX_Schema_Gemi,"Explain the data lineage for the 'Settlement Amount' from XML to a Regulatory Report.","Lineage: `pacs.008 XML (/CdtTrfTxInf/IntrBkSttlmAmt)` -> `stg_pacs008.interbank_amt` (Decimal) -> `fact_payment.settlement_amount_base_ccy` (Converted to reporting ccy if needed) -> `view_reg_report.total_settlement_amt`."
DayX_ISO20022_MX_Schema_Gemi,"How do you identify 'Cover Payments' in your data model given that `pacs.009` can be CORE or COV?","Check for the presence of the `<UndrlygCstmrCdtTrf>` block in the `pacs.009` XML. If present (NOT NULL), flag `payment_type = 'COV'` in the canonical model. If absent, `payment_type = 'CORE'`."
DayX_ISO20022_MX_Schema_Gemi,"Describe how to map the `pain.001` `<PmtInf>` (Group) level data to the `pacs.008` `<CdtTrfTxInf>` (Transaction) level data in a canonical model.","`pain.001` has 1 `<PmtInf>` : N `<CdtTrfTxInf>`. The `<PmtInf>` contains the Debtor and Execution Date. During ingestion/mapping, 'denormalize' (explode) the `<PmtInf>` data down to each transaction row, so every canonical `payment_tx` row has its own copy of Debtor and Date."
DayX_ISO20022_MX_Schema_Gemi,"Explain the strategy for preserving 'Truncated Data' when an ISO 20022 message flows into a legacy system and back out.","Strategy: 'Golden Copy'. Ingest raw ISO XML into a Data Lake/Staging table (`iso_message_raw_id`). Generate a metadata pointer (UETR). Pass the truncated data to legacy. When the legacy system outputs a result, rejoin with the Golden Copy using UETR to restore the full 140-char data for reporting."
DayX_ISO20022_MX_Schema_Gemi,"How do you derive the 'Debtor Customer Type' (Corporate vs Retail) from `pacs.008` fields?","Check `<Dbtr><Id>`. If `<OrgId>` is present (contains BIC/LEI), set Type = 'CORPORATE'. If `<PrvtId>` is present (contains Date of Birth/Passport), set Type = 'RETAIL'. If both missing, default to 'UNKNOWN'."
DayX_ISO20022_MX_Schema_Gemi,"Write the logic to extract 'Regulatory Reporting Codes' (e.g., for UAE) from the `<RgltryRptg>` block.","The block contains `<Dtls><Tp>` (Type) and `<Dtls><Cd>` (Code). Logic: `SELECT code FROM stg_pacs008_regulatory WHERE type = 'BOP' (Balance of Payments) AND uetr = :current_uetr`. Handle multiple codes if applicable."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to determine the 'Settlement Date' for a payment, distinguishing between the requested date and the actual interbank value date.","`Requested_Date` comes from `pain.001` `<ReqdExctnDt>`. `Actual_Settlement_Date` comes from `pacs.008` `<IntrBkSttlmDt>`. In the canonical model, store both: `requested_date` and `value_date` to analyze delays."
DayX_ISO20022_MX_Schema_Gemi,"Design a check to validate that the 'Instructed Amount' matches the 'Settlement Amount' if the currencies are the same.","SQL Check: `WHERE instd_ccy = intrbk_ccy AND instd_amt <> intrbk_amt`. If this returns rows, it implies hidden fees were deducted or a data error occurred. Flag as 'Amount Mismatch'."
DayX_ISO20022_MX_Schema_Gemi,"How do you handle 'Structured Address' vs 'Unstructured Address' in the canonical `debtor_address` field?","Create separate columns: `addr_line1`, `addr_line2` (for unstructured/legacy) AND `street_name`, `building_nb`, `post_code`, `town_name` (for structured). Populate the relevant columns based on which XML tags are present. Do not force structured data into a single string blob if possible."
DayX_ISO20022_MX_Schema_Gemi,"Describe the mapping of 'Charge Bearer' codes to a numeric representation for fee analysis.","Map `<ChrgBr>`: `DEBT` -> Sender Pays All, `CRED` -> Receiver Pays All, `SHAR` -> Shared. In the Fact table, you might derive flags: `is_sender_charged` (True for DEBT/SHAR), `is_receiver_charged` (True for CRED/SHAR)."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to extract the 'Intermediary Chain' for a correspondent banking report.","Extract `<IntrmyAgt1>`, `<IntrmyAgt2>`, `<IntrmyAgt3>` from `pacs.008`. In the canonical model, you can store these as `intermed_1_bic`, `intermed_2_bic` etc., or pivot them into a `payment_route` table sequence."
DayX_ISO20022_MX_Schema_Gemi,"How do you handle 'AnyBIC' codes in the `creditor_agent_bic` canonical column?","If `<FinInstnId><BICFI>` is present, use it. If `<Othr><Id>` is present (AnyBIC), map it to a generic placeholder 'NONREF' or store the proprietary ID in a separate `creditor_agent_proprietary_id` column to avoid polluting the BIC column."
DayX_ISO20022_MX_Schema_Gemi,"Design the logic to link a `pacs.002` (Status Report) to the original `pacs.008` in the database.","Join `stg_pacs002` on `original_msg_id` = `stg_pacs008.msg_id` AND `original_tx_id` = `stg_pacs008.instr_id`. Ideally use `UETR` if available in the `pacs.002` (it should be). Update the `payment_tx.status` column with the code from `pacs.002` (`<TxSts>`)."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to determine if a payment is 'SWIFT gpi' based on `pacs.008` data.","Check `<PmtTpInf><SvcLvl><Cd>`. If it equals 'G001', set canonical flag `is_gpi = TRUE`. This enables specific SLA reporting logic."
DayX_ISO20022_MX_Schema_Gemi,"Describe the extraction of 'Tax Information' if present in the message.","Look for `<Tax>` block within `<RmtInf>` (rare) or specific local extensions. Map `<Tax><TtlTaxAmt>` to `tax_amount` and `<Tax><Tp>` to `tax_type` in a specialized `tax_payment_details` table."
DayX_ISO20022_MX_Schema_Gemi,"How would you model the 'Payment Return' (`pacs.004`) in the canonical fact table?","Insert a NEW row in `payment_tx` with `transaction_type = 'RETURN'`. Link it to the original payment via `parent_payment_id` (derived from `<OrgnlUETR>`). The amount should be negative or flagged as a reversal direction."
DayX_ISO20022_MX_Schema_Gemi,"Write the logic to detect 'Stripping' (missing cover data) in `stg_pacs009`.","`SELECT * FROM stg_pacs009 WHERE settlement_method = 'COV' AND (underlying_debtor_name IS NULL OR underlying_creditor_name IS NULL)`. This indicates the required nested block is empty."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to extract the 'Service Level' (e.g., SEPA) vs 'Local Instrument' (e.g., CORE/B2B).","Map `<SvcLvl><Cd>` (e.g., 'SEPA') to `scheme_code`. Map `<LclInstrm><Cd>` (e.g., 'CORE') to `product_code`. Combined, they define the exact clearing rulebook applied."
DayX_ISO20022_MX_Schema_Gemi,"Design a mechanism to handle 'Long Tail' ISO fields that are rarely populated (e.g., `<Prtry>` tags).","Instead of creating columns for every possible proprietary tag, store them in a JSONB column `additional_properties` in the canonical table. Key = Tag Name, Value = Content. Index this column for ad-hoc search."
DayX_ISO20022_MX_Schema_Gemi,"How do you ensure 'Idempotency' when processing the same `pacs.008` file twice?","Use `<MsgId>` from `<GrpHdr>` as a unique constraint in the `stg_file_load` table. Check if `<MsgId>` exists before processing. For transactions, use `<UETR>` as the unique key to prevent duplicate ledger postings."
DayX_ISO20022_MX_Schema_Gemi,"Describe how to extract the 'Exchange Rate' used in a cross-currency payment.","Extract `<XchgRate>` from `<XchgRateInf>`. Store in `fx_rate_applied`. Also extract `<UnitCcy>` and `<RtdCcy>` to understand the direction (e.g., 1 EUR = 1.1 USD)."
DayX_ISO20022_MX_Schema_Gemi,"Explain the mapping of 'Creditor Reference' (Structured Remittance) for reconciliation.","Map `<RmtInf><Strd><CdtrRefInf><Ref>` to `creditor_reference`. Map `<RmtInf><Strd><CdtrRefInf><Tp><CdOrPrtry><Cd>` (e.g., 'SCOR') to `reference_type`. This is vital for AR reconciliation."
DayX_ISO20022_MX_Schema_Gemi,"How do you derive the 'Originating Bank Country' if the Instructing Agent is missing?","Fallback logic: 1. Check `<InstgAgt>` BIC. 2. If missing, check `<DbtrAgt>` BIC. 3. If missing, check IBAN prefix of `<DbtrAcct>`. 4. If all fail, mark as 'UNKNOWN'."
DayX_ISO20022_MX_Schema_Gemi,"Design a 'Data Quality' check for the `pacs.008` staging table.","Check: `uetr IS NOT NULL`. Check: `interbank_amt > 0`. Check: `dbtr_nm IS NOT NULL`. Check: `cdtr_nm IS NOT NULL`. Check: `interbank_ccy` is valid ISO code. Fail the batch or flag rows if thresholds exceeded."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to map the 'Debtor Identification' when it is a Passport Number vs a Tax ID.","Check `<Dbtr><Id><PrvtId><Othr><SchmeNm><Cd>`. If 'CUST', map `<Id>` to `customer_id`. If 'TXID', map to `tax_id`. If 'CCPT', map to `passport_number`. Use the Scheme Name to direct the ID to the correct column."
DayX_ISO20022_MX_Schema_Gemi,"Write the logic to flag 'On-Us' payments (Internal Book Transfer).","`IF stg_pacs008.dbtr_agt_bic == stg_pacs008.cdtr_agt_bic THEN is_on_us = 'Y' ELSE is_on_us = 'N'`. (Checks if Sender Bank = Receiver Bank)."
DayX_ISO20022_MX_Schema_Gemi,"How do you handle 'Time Zones' when extracting `<CreDtTm>`?","ISO 20022 uses UTC (Zulu). Store in database as `TIMESTAMP WITH TIME ZONE` (UTC). Convert to Local Market Time (e.g., CET for SEPA, EST for Fedwire) only at the report presentation layer."
DayX_ISO20022_MX_Schema_Gemi,"Describe the extraction of 'Batch Booking' indicator from `pain.001`.","Extract `<BtchBookg>` boolean from `<PmtInf>`. Store as `is_batch_booking` flag. If True, the settlement to the Debtor account is one debit for the sum of transactions; if False, individual debits."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to support 'Multi-Currency' reporting in the canonical model.","Store `amount` and `currency` columns. Add a `amount_usd_equiv` column computed using the daily spot rate table joined on `value_date`. This allows aggregation across currencies."
DayX_ISO20022_MX_Schema_Gemi,"How do you extract the 'Cheque Number' from a `pain.001` if the payment method is Cheque?","If `<PmtMtd>CHK`, look for `<ChqInstr><ChqNb>`. Map this to `instrument_reference_number` in the canonical table."
DayX_ISO20022_MX_Schema_Gemi,"Design the logic to identify 'High Value Payments' for a specific report.","`IF interbank_amt > threshold_amt (e.g., 1M EUR) THEN is_high_value = 'Y'`. Ensure threshold is applied to the converted USD equivalent amount to be consistent across currencies."
DayX_ISO20022_MX_Schema_Gemi,"Explain how to extract the 'Clearing System' member ID if BIC is not used.","Look at `<FinInstnId><ClrSysMmbId><MmbId>`. Map this to `clearing_member_id` and `<ClrSysId><Cd>` (e.g., USABA) to `clearing_system_type`."
DayX_ISO20022_MX_Schema_Gemi,"How do you map the 'Debtor Agent' if it is a Branch of a major bank?","The BIC (`<BICFI>`) often identifies the branch (last 3 chars != XXX). Map the full BIC to `debtor_agent_bic`. Use reference data to derive the 'Head Office' BIC if needed for risk aggregation."
DayX_ISO20022_MX_Schema_Gemi,"Describe the 'Golden Copy' re-parsing process if a new regulatory requirement emerges.","1. Identify affected UETRs/Dates. 2. Retrieve `xml_blob` from `stg_pacs008` (or Data Lake). 3. Apply NEW XSLT/Parser logic to extract the previously ignored field (e.g., `<LclInstrm>`). 4. Update the Canonical Table columns for those records."
"DayX_Kafka_Core_Perp","Design a topic and key strategy for a real-time payments pipeline, ensuring that all events for a given account maintain order while still supporting horizontal scale.","Create a main topic such as `payments.prod.events.v1` with a partition count sized for expected throughput and growth (e.g., 48+). Use the account ID (or a hashed form of it) as the message key so that all events for a given account (PaymentCreated, PaymentAuthorized, PaymentSettled) land in the same partition, preserving per-account ordering. Across accounts, events distribute evenly across partitions, allowing horizontal scale by adding more consumer instances. Use a well-defined schema in Schema Registry and keep event types in an `event_type` field to let consumers filter and process appropriately."
"DayX_Kafka_Core_Perp","Explain how you would configure producers and consumers to achieve effectively exactly-once processing for a regulatory reporting pipeline that writes to both Kafka and a database.","Enable idempotent and transactional producers (`enable.idempotence=true`, set `transactional.id` per producer instance). Use read-process-write topology where the consumer is part of a transactional client that consumes from an input topic, processes, writes results to an output topic and the external database inside the same transaction, and then commits consumer offsets as part of the transaction. On the consumer side, disable auto-commit and let the transaction coordinator handle offset commits. Configure `acks=all` and tune `min.insync.replicas` for safety. The database writes must be included in the transaction using Kafka Streams or a transactional outbox pattern so that either all side effects (Kafka + DB) succeed or all are rolled back, yielding exactly-once processing from the pipeline’s perspective."
"DayX_Kafka_Core_Perp","Outline how you would set up monitoring and alerting for a Kafka-based AML screening system so that lag or broker issues do not cause missed regulatory deadlines.","Define key metrics: consumer lag per partition on AML topics, processing throughput, error rate, DLT volume, under-replicated partitions, offline partitions, ISR changes, broker disk usage, and request latency. Instrument consumers with Prometheus or native metrics and feed them into a monitoring stack (e.g., Prometheus/Grafana, CloudWatch, or equivalent). Set alert thresholds for lag based on acceptable processing delay (e.g., warning at 1 minute, critical at 5 minutes), critical alerts for any under-replicated/offline partitions, and rising DLT or error counts. Integrate alerting into on-call tooling so SREs and AML ops are notified quickly. Dashboards should show end-to-end lag from production to AML decision topics, with drill-down by partition and consumer instance."
"DayX_Kafka_Core_Perp","Describe how you would use Schema Registry and compatibility rules to evolve payment event schemas safely without breaking downstream consumers.","Register payment schemas in Schema Registry under consistent subject naming (e.g., `payments.events-value`). Start with `BACKWARD` or `BACKWARD_TRANSITIVE` compatibility so new schemas must be backward compatible with all previous versions. When adding regulatory or AML-related fields (e.g., purpose code, ultimate debtor), add them as optional fields with sensible defaults so existing consumers can ignore them. Coordinate with downstream teams by documenting changes and versioning event semantics in release notes. Validate new producer deployments by running schema compatibility checks in CI/CD pipelines. Only consider breaking changes if absolutely necessary, in which case introduce a new topic or subject version (`payments.events.v2`) and run both versions in parallel for a migration period."
"DayX_Kafka_Core_Perp","Design a basic retry and dead-letter topic strategy for a payment enrichment consumer that occasionally encounters transient and permanent failures.","Implement a main input topic (e.g., `payments.enriched.in`) and a DLT (e.g., `payments.enriched.dlq`) plus optional time-based retry topics (e.g., `payments.enriched.retry.5s`, `...retry.1m`). On failure, the consumer classifies errors: transient (e.g., downstream service unavailable) vs permanent (e.g., invalid schema, bad data). For transient errors, publish the record to a retry topic with increased backoff encoded via headers, and have a separate consumer read from retry topics after delays. For permanent errors or after exhausting retry attempts, send the record to the DLT with error metadata in headers (reason, stack trace ID, timestamp). Monitor DLT volume and create dashboards and processes for AML ops or support teams to inspect and remediate DLQ messages."
"DayX_Kafka_Core_Perp","Explain how you would choose the number of partitions for a new high-throughput 'payments.events' topic in a bank.","Estimate target peak throughput (messages/sec and MB/sec), expected consumer parallelism, and growth over the next 1–2 years. Size partitions so that each partition can handle a safe fraction of the peak throughput given broker resources (CPU, disk, network). For example, if a conservative rule-of-thumb is X MB/s per partition and you expect 200 MB/s, you might start with 64 or 96 partitions. Consider future scaling (ability to double consumers), cluster limits, and management overhead. Document the capacity assumptions and avoid using auto-created topics with default partition counts for critical flows."
"DayX_Kafka_Core_Perp","Describe how Kafka consumer groups enable horizontal scaling and failover for an AML scoring service.","Run multiple instances of the AML scoring service in a single consumer group subscribed to the AML input topic. The group coordinator assigns partitions so each partition is owned by exactly one instance, allowing parallel processing across instances. If traffic grows, add more instances to the group, and Kafka will rebalance partitions to spread load. If an instance fails, the coordinator reassigns its partitions to healthy instances, providing automatic failover without manual intervention. This model supports both elasticity and resilience for the AML service."
"DayX_Kafka_Core_Perp","How would you design topic naming conventions in a large bank to support governance and discovery?","Define a consistent naming pattern such as `domain.subdomain.env.usecase.version`, for example `payments.core.prod.events.v1` or `risk.aml.nonprod.alerts.v1`. Include domain and subdomain to show business ownership, environment for isolation, use case (events, commands, snapshots), and an explicit semantic version. Document the convention, enforce it via automation (topic creation pipelines), and integrate it into catalogs and ACL management so teams can easily discover topics and understand their purpose and sensitivity."
"DayX_Kafka_Core_Perp","Explain how you would secure a Kafka cluster that carries payment and PII data using authentication and authorization.","Enable TLS for all client-broker and broker-broker communication. Use strong authentication mechanisms such as mTLS with client certificates, or SASL mechanisms integrated with corporate identity providers (Kerberos, OAUTH/OIDC). Define ACLs that grant least-privilege access: producers can only write to allowed topics, consumers can only read necessary topics and use allowed consumer groups, and admin actions are restricted to platform teams. Integrate ACL management with RBAC and change control processes. Audit all access using broker logs and integrate them into SIEM tooling."
"DayX_Kafka_Core_Perp","Outline a simple high-level DR design using Kafka for a critical payment stream.","Deploy two Kafka clusters in separate regions or data centres (primary and DR). Use MirrorMaker or cluster linking to replicate critical topics (e.g., `payments.events`) from primary to DR, preserving keys and ordering within partitions. Ensure security and ACLs are mirrored. In normal operation, producers and consumers use the primary cluster; DR cluster passively receives mirrored data. Regularly test DR by failing over selected consumers or running read-only checks from DR. Define clear RPO/RTO targets and ensure mirror lag and retention are sufficient to meet them. Document cutover procedures, DNS or config switch mechanisms, and roll-back steps."
"DayX_Kafka_Core_Perp","Describe how you would use Kafka headers in a payments platform for observability and tenancy.","Include correlation IDs, trace IDs, and tenant or business-unit identifiers in record headers (e.g., `x-correlation-id`, `x-trace-id`, `x-tenant-id`). These headers allow tracing requests end-to-end across microservices and topics without changing payload schemas. Logging frameworks can pull header values into structured logs, and monitoring systems can aggregate metrics by correlation or tenant. Headers can also carry technical metadata like schema version hints or retry counters without altering business fields."
"DayX_Kafka_Core_Perp","Explain a pattern to prevent a single poison message from blocking progress in a partition for a regulatory consumer.","Implement bounded retries and DLT handling. When a message fails, the consumer retries processing a small fixed number of times, potentially with backoff, while tracking attempts with a header or local state. If the message still fails, the consumer produces it to a DLT with error details and then commits the offset on the main topic, allowing the consumer to continue to the next record. This prevents infinite retry loops on a single poisonous message while preserving it for later investigation and manual correction."
"DayX_Kafka_Core_Perp","How would you use partitions and consumer groups to separate high-value and low-value payment processing?","Create separate topics (e.g., `payments.highvalue.events` and `payments.lowvalue.events`) or use a single topic with headers and have different consumer groups subscribe selectively. Use more partitions and stricter durability settings for high-value payments. Run separate consumer groups for high-value flows with more resources and tighter SLAs, and perhaps simpler or lower-priority consumers for low-value flows. This separation prevents low-value traffic spikes from delaying high-value regulatory or settlement-critical flows."
"DayX_Kafka_Core_Perp","Describe how you would design a Kafka Streams-based application to maintain a real-time view of customer balances from payment events.","Ingest payment events from a topic keyed by account ID. Use Kafka Streams to define a `KStream` of payments and group by account key, then aggregate to a `KTable` that maintains running balances (credit minus debit). Materialize the `KTable` to a local state store and optionally expose it via an interactive query API or stream it out to a `balances.snapshot` topic. Make sure to handle out-of-order events with appropriate windowing or sequence numbers if needed, and configure exactly-once semantics so updates to the balance topic and state store are atomic."
"DayX_Kafka_Core_Perp","Explain how you would manage consumer offsets to support safe reprocessing for a reporting use case.","Disable auto-commit and manually commit offsets only after successful processing and durable writes to the reporting store. To reprocess, run a separate consumer group with a different group ID, or reset offsets for the existing group using admin tooling to a desired point in time. Store checkpoints of last processed offset and related business watermark in metadata so you can reason about what has been included in which report. Ensure offset resets are change-controlled and auditable in a regulated context."
"DayX_Kafka_Core_Perp","Design a Kafka-based ingestion pattern from core banking systems using CDC for a cloud-native reporting platform.","Deploy CDC connectors (e.g., via Kafka Connect) to capture changes from core banking databases and publish them as change events (insert/update/delete) to domain-specific topics (`core.accounts.cdc`, `core.payments.cdc`). Use keys based on primary keys (account ID, transaction ID). Downstream, stream processors normalize and enrich events, then write to curated topics and data lake sinks. Ensure schemas in Schema Registry reflect CDC payloads and include metadata like operation type and source timestamps. Configure retention and compaction suitably for CDC streams, and secure connectors with strong authentication and ACLs."
"DayX_Kafka_Core_Perp","Outline steps to migrate a Kafka cluster from ZooKeeper-based metadata management to KRaft with minimal downtime.","Plan a migration using vendor or community guidance: deploy a new KRaft-enabled cluster or perform in-place upgrade if supported. Export current metadata (topics, ACLs, quotas). Test migration in nonprod first. For a new cluster approach, set up the KRaft cluster, replicate topics using cluster linking, and gradually cut over producers and consumers by updating bootstrap servers, monitoring lag and errors. Ensure all security configs and ACLs are replicated. Schedule a maintenance window for final cutover, and keep rollback procedures ready. For in-place, follow rolling upgrade steps and controller quorum migration, ensuring compatibility and backups."
"DayX_Kafka_Core_Perp","Explain how you would design access control for multiple teams using a shared Kafka cluster.","Create separate technical principals (service accounts) per application or team, not per individual. Use ACLs to grant minimal permissions: producers get `WRITE` on specific topics; consumers get `READ` on required topics and `READ` on relevant consumer groups; admins get cluster-level rights. Group topics by domain and sensitivity and align ACLs accordingly. Tie service accounts to CI/CD and secret management, not hardcoded credentials. Regularly review ACLs, remove unused ones, and integrate with identity governance for joiner/mover/leaver processes."
"DayX_Kafka_Core_Perp","Describe how you would implement throttling or backpressure handling for a consumer overwhelmed by bursty traffic.","Batch records from `poll` and process them in bounded batches; configure `max.poll.records` to control batch size. If processing is slower than intake, consider pausing partitions (`pause`/`resume`) to exert backpressure. Use rate limiting in the application layer or autoscale consumer instances to keep up. Implement backpressure-aware downstream interactions (e.g., circuit breakers, queues). Monitor lag and trigger alerts so manual or automated scaling decisions can be made before SLAs are violated."
"DayX_Kafka_Core_Perp","How would you design Kafka topics and consumers so that AML and regulatory reporting systems can independently evolve without breaking each other?","Use independent consumer groups (e.g., `aml-engine` and `reg-reporting`) subscribed to shared well-governed topics like `payments.events.v1`. Ensure schemas are managed in Schema Registry with backward compatibility, and avoid overloading the topic with consumer-specific fields. If AML or reporting need specialized enriched views, create separate derived topics (e.g., `payments.events.aml-enriched`) so changes can be introduced there without affecting other consumers. Communicate schema changes and deprecations via governance forums and catalogs."
"DayX_Kafka_Core_Perp","Explain how you would test failure scenarios (broker loss, consumer crashes) for a critical Kafka-based payment flow.","Set up a realistic test environment mirroring production settings. Use tooling to kill or stop brokers and observe leader elections, ISR changes, and whether producers/consumers recover. Simulate consumer crashes and restarts, verifying that offset management leads to expected replays or no loss. Inject network partitions if possible. Measure how quickly the system recovers and whether alerts fire as expected. Capture metrics (lag, error rates) and logs to validate that failure modes are handled within agreed RTO/RPO and that no payments are lost or processed twice without idempotent sinks."
"DayX_Kafka_Core_Perp","Describe a pattern for integrating Kafka with an external relational database while preserving exactly-once semantics for writes to the DB.","Use a transactional outbox pattern: application writes business changes and an outbox record into the database within a single DB transaction. A separate outbox reader (e.g., Debezium or custom process) reads committed outbox rows and publishes them to Kafka, marking rows as processed idempotently. Alternatively, use Kafka Streams or a transactional producer that writes to Kafka and uses a transactional sink connector that participates in exactly-once processing. Avoid direct dual-writes from application to DB and Kafka without coordination because they can drift under failures."
"DayX_Kafka_Core_Perp","How would you use Kafka to separate online payment processing from heavy analytics workloads?","Publish all payment events to Kafka topics once from online systems. Run separate consumer groups for online processing (e.g., AML, posting) tuned for low latency, and other consumer groups for analytics and reporting (feeding data lakes, warehouses). Use different consumption patterns and SLAs per group. Analytics can read from Kafka at their own pace, possibly via connectors, without loading core systems. This decoupling allows online services to remain responsive even when analytics are heavy or temporarily backlogged."
"DayX_Kafka_Core_Perp","Explain how you would set up alerts using consumer lag to protect intraday liquidity reporting SLAs.","Identify topics and consumer groups feeding liquidity reporting. Use metrics for current and max lag per partition. Define SLOs (e.g., max acceptable end-to-end delay of 1 minute). Configure alerts so that when lag exceeds a warning threshold (e.g., 30 seconds) for a sustained period, a warning is raised; above a critical threshold (e.g., 2–5 minutes), a critical alert is sent to on-call SRE and business operations. Correlate lag with throughput and error metrics in dashboards to support triage, and document runbooks for remediation steps."
"DayX_Kafka_Core_Perp","Design a simple pattern to roll out a new version of a payment event schema without breaking existing consumers.","Introduce new optional fields in the schema and register the updated version under the same subject with backward compatibility enforced. Update producers to populate the new fields while keeping old ones intact. Coordinate with key consumers so they validate against or read the new fields when ready. If truly breaking changes are required, create a new topic (e.g., `payments.events.v2`) and run both topics in parallel, gradually migrating consumers to v2. Announce deprecation timelines and, after migration, retire v1 under change control."
"DayX_Kafka_Core_Perp","Explain how you would use Kafka Connect in a regulated bank to move data in and out of Kafka safely.","Deploy Kafka Connect in distributed mode with secure configuration (TLS, authentication). Use vetted source connectors (e.g., JDBC, CDC) to ingest from databases and file systems, and sink connectors (e.g., S3, JDBC) to deliver to data lakes and warehouses. Configure connectors with least-privilege DB and topic access. Store connector configs in version control, and manage them via controlled pipelines. Monitor connector health, task failures, and error logs; route connector errors and DLQ records to central logging. Ensure configs, especially credentials, are stored securely using secrets management."
"DayX_Kafka_Core_Perp","Describe how you would structure topics so that payment events can be replayed for specific date ranges during an audit.","Partition topics based on time-sensitive keys (e.g., account or transaction ID) but use time-based retention and well-defined naming. To support date-range replays, either use timestamp-based filtering with client-side logic (reading from `earliest` and filtering by event time) or maintain auxiliary index metadata (e.g., offsets by date). Alternatively, write an additional time-partitioned sink (e.g., S3 buckets per day) via connectors for efficient date-based access. Document procedures and provide internal tooling to generate replay consumer configs for given date intervals."
"DayX_Kafka_Core_Perp","How would you handle multi-region Kafka design for an always-on global payments platform with strict consistency needs within a region but eventual consistency across regions?","Deploy separate regional Kafka clusters, each authoritative for payments initiated in that region. Within each region, use strong durability settings and local Consumer groups. Use cluster linking/MirrorMaker to replicate key topics asynchronously between regions for global visibility and DR. Design applications so cross-region consumers tolerate replication lag and do not assume global ordering. Avoid active-active writes for the same key across regions unless a conflict-resolution strategy exists. Clearly define which region is source of truth per corridor or account, and ensure governance matches legal/residency requirements."
"DayX_Kafka_Core_Perp","Explain how you would implement multi-tenancy in Kafka for different business units while preventing data leakage.","Adopt topic prefix patterns per business unit (e.g., `retail.*`, `corporate.*`) and manage ACLs accordingly. Use separate service accounts per BU or application, granting them access only to their topics and authorized shared topics. Implement network isolation where appropriate (e.g., separate VPCs, private endpoints). Enforce naming and tagging that indicate sensitivity and ownership. Monitor topic access logs and integrate them into a SIEM. For highly sensitive workloads, consider separate clusters or namespaces per BU in addition to ACLs."
"DayX_Kafka_Core_Perp","Design a way to capture end-to-end lineage for a payment event as it flows through multiple Kafka topics and into downstream stores.","Assign a globally unique business ID to each payment and carry it as the message key or in a dedicated field, plus correlation IDs and headers. Maintain a lineage catalog that records which topics, processors, and sink systems touch events identified by that ID. Processing applications should log structured events including input topic, offset, output topic, and transformation metadata. Use this information to build lineage graphs in a metadata system, mapping from original `payments.events` through enrichment and aggregation topics into data warehouses and regulatory tables."
"DayX_Kafka_Core_Perp","Explain how you would use Kafka for blue/green deployments of a downstream consumer service.","Run two consumer groups, `service-blue` and `service-green`, subscribing to the same topics. Initially, only blue is active. During deployment, start green with the same configuration but different group ID, and let it read from the same point (or a controlled offset) while monitoring its behaviour and outputs. Once validated, route traffic (e.g., via downstream endpoints) to green and gradually drain or stop blue. This pattern allows safe validation and rollback for critical consumers without disrupting the main event stream."
"DayX_Kafka_Core_Perp","Describe how you would handle large messages (e.g., documents) in Kafka without overloading the cluster.","Avoid sending large binary payloads directly. Instead, store large objects (documents, images) in an object store, and place references (URIs, IDs, hashes) plus metadata into Kafka messages. For cases where messages must be larger, tune `max.message.bytes` and producer `max.request.size`, but recognize the trade-offs on throughput, memory, and disk. Consider splitting payloads into chunks and re-assembling downstream if necessary. Always assess whether Kafka is the right medium for large content vs metadata-only events."
"DayX_Kafka_Core_Perp","Explain how you would introduce Kafka into a legacy batch-based reporting environment gradually.","Start by replicating key batch outputs or database tables into Kafka via CDC or scheduled ingestion into new topics. Let new consumers, such as near real-time dashboards or incremental reporting jobs, consume from Kafka while legacy batch remains unchanged. Over time, move existing batch jobs to read from Kafka-backed data stores instead of directly from operational systems. Gradually reduce direct batch extractions from core systems, using Kafka as the central integration bus. Ensure parallel run and reconciliation during the transition to confirm that Kafka-based pipelines are producing equivalent results."
"DayX_Kafka_Core_Perp","Design a governance process for new Kafka topics requested by teams in a G-SIB.","Require teams to submit topic requests including business purpose, data classification, retention requirements, estimated volume, owning domain, and consumers. A review board (platform + security + data governance) validates naming, retention, ACLs, and schema design, ensuring alignment with standards. Topics are created via automated infrastructure-as-code pipelines, not ad hoc scripts, and registered in a central catalog with ownership and sensitivity tags. Periodically review topic usage, retention, and access to clean up unused topics and adjust controls."
"DayX_Kafka_Core_Perp","Describe how you would implement observability for a payment enrichment Kafka Streams application.","Instrument the application with metrics for input/output throughput, processing latency, end-to-end lag, error counts, and state store sizes. Expose metrics via Prometheus or cloud-native monitoring. Log structured events including keys, correlation IDs, and error details with appropriate PII redaction. Propagate trace IDs via headers and integrate with distributed tracing. Create dashboards for throughput, lag, and error rates with drill-down by partition and instance. Set alerts on lag, error spikes, and abnormal throughput drops to catch issues before SLAs are missed."
"DayX_Kafka_Core_Perp","Explain a strategy for handling late and out-of-order events in a Kafka-based payment analytics pipeline.","Model event-time vs processing-time and design aggregates using windowing that tolerates some lateness (e.g., tumbling or hopping windows with allowed lateness in Kafka Streams/Flink). Use event timestamps from payloads, not ingestion time. Configure grace periods so late events can still update aggregates up to a limit, and send very late events to a separate correction topic or store for manual adjustment. Document the business rules for how late updates affect reports and ensure regulators are comfortable with the chosen windows and correction mechanisms."
